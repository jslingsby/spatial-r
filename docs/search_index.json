[["index.html", "A Minimal Introduction to GIS (in R) 1 Overview 1.1 General 1.2 Using this resource 1.3 Module and Project details", " A Minimal Introduction to GIS (in R) Jasper Slingsby 2024-02-21 1 Overview This is a minimal introduction to GIS and handling spatial data in R compiled for the Biological Sciences BSc(Honours) class at the University of Cape Town. 1.1 General The goal is to give you a very brief introduction to Geographic Information Systems (GIS) in general and some familiarity with handling spatial data in R. GIS is a field of research that many people dedicate their entire lives to, yet we only have a week, so this really is a minimalist introduction. I’ll focus on giving you a broad overview and some idea of how to teach yourself (using R). The core outcomes I hope you’ll come away with: Some familiarity with GIS and what it can help you achieve Some familiarity with GIS jargon and technical terms Highlight some of the common problems and pitfalls when using GIS Some familiarity with handling spatial data in R Some hints and resources to help you teach yourself R Some idea of how to help yourself or find help when you inevitably come unstuck… These course notes borrow or paraphrase extensively from Adam Wilson’s GEO 511 Spatial Data Science course, Manny Gimond’s Intro to GIS &amp; Spatial Analysis and the 2020 series of GIS Lecture Lunches by Thomas Slingsby and Nicholas Lindenberg from UCT Library’s GIS Support Unit. Other very valuable resources include: Lovelace et al’s online book Geocomputation with R Ryan Garnett’s cheatsheet for library(sf) Pebesma and Bivand’s Spatial Data Science All code, images, etc can be found here. I have only used images etc that were made available online under a non-restrictive license (Creative Commons, etc) and have attributed my sources. Content without attribution is my own and shared under the license below. If there is any content you find concerning with regard to licensing, or that you find offensive, please contact me. Any feedback, positive or negative, is welcome! This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. 1.2 Using this resource I’ve included quite a bit of demonstration R code in Chapters 7 and 8. To run the code you will need: to have working R and RStudio installations - this tutorial may help if needed to install the following R packages (you can copy and run the code): required: install.packages(c(\"tidyverse\", \"sp\", \"raster\", \"sf\", \"lwgeom\", \"terra\", \"stars\", \"exactextractr\")) optional, but handy for some visualizations: install.packages(c(\"cowplot\", \"hrbrthemes\", \"knitr\", \"leaflet\", \"htmltools\", \"rosm\", \"ggspatial\", \"rnaturalearth\", \"mapview\", \"tmap\")) to download the datasets discussed in section 7.1 1.3 Module and Project details Lectures Lectures will be held in the mornings between 10:00 to 12:00 Monday to Thursday in BIOLT1. For the Honours students, Thursday afternoon, 2-4PM, will be lightning talk presentation day (venue TBD). For the Conservation Biology MSc course, we’ll do lightning talk presentations on Friday. Projects Afternoons are self-study time where you will incrementally develop your own individual GIS project in R and RMarkdown or Quarto. For the Honours students, the project will count 70% of your mark for the module and will be due on Friday the 1st March. You will need to submit a .Rmd or .Qmd file and stitched HTML notebook. You’re welcome to do this in a Git Repository (nudge nudge), now that we’ve completed the Reproducible Research module. For the Conservation Biology students, the project will count 50% (10% lightning talk, 40% project). You will need to submit a .Rmd or .Qmd file and stitched HTML notebook, but I don’t expect you to use GitHub. The focus of the assignment is essentially setting up a GIS workflow and will be assessed on whether you’ve absorbed the content of the lectures. For the Honours students, the topic and datasets used will be up to you. Pro tip: Use this as an opportunity to get a kick start on your Honours projects. If your Honours project doesn’t require GIS (which is unlikely) then either help a buddy or let your curiosity roam wild! This is a teaching exercise, so it doesn’t even have to be based on biological data, but it would help you to explore some of the data sources suggested below. For the Conservation Biology students, the project needs to be based on one of the themes that we are reading about for week two. See separate email and Amathuba announcement. The project objectives, broken down as daily goals to help you pace yourselves this week: Day 1: Define a question that requires GIS, the kinds of data you’ll need to address the question, and describe in words what you think you would need to do with the data to get there. Find and describe the datasets you’ll need for your analysis (type, source, how created, etc). Day 2: Describe the GIS workflow you think you’ll need to perform your analysis (in words and/or a figure). Reconsider and refine your datasets. Day 3: Translate your workflow into the R functions you think you’ll need to use and begin coding and running the GIS workflow in R. Day 4: Keep working on the code. Develop and present your 1 slide lightning talk - examples from the Honours class of 2021 are here and 2022 are here. Until the 1st March (Hons students): Iterate over the previous steps until done! I will be available 2-3PM on Wednesday afternoon and 10-11AM on Friday morning as a “help desk” to assist you refine your projects and troubleshoot issues. Lightning talks: you’ll do lightning presentations (30% of your mark for the module) on your GIS projects (1 slide, 2 minutes presentation, 1 minute questions). Don’t worry, you won’t lose marks if your projects are not yet complete! We just want to know what you’re doing your project on, what datasets you’re using, and what you plan to do with them. Please read the instructions in the Google slide deck and add your slide. For the Honours students, this will be on Thursday afternoon (2PM, 22nd February). For the Conservation Biology students, this will be on Friday (23rd February). Some sources of local data to help you get started. Feel free to look for others! If you find good ones, let me know and I’ll add them. SANBI’s “Biodiversity GIS” - https://bgis.sanbi.org/ SANBI’s Botanical Database of South Africa (BODATSA) - http://newposa.sanbi.org/ SAEON’s Data Catalogue - http://catalogue.saeon.ac.za/ City of Cape Town’s Open Data Portal - https://odp.capetown.gov.za/ iNaturalist - https://www.inaturalist.org/ (accessible from R - see section 7.7) Google Datasets Search - https://datasetsearch.research.google.com/ The STAC Index - mostly for access to Cloud-optimized geotiffs (COGs) Make sure to check the data use policies and make sure you have permission use the datasets!!! "],["intro.html", "2 Why care about GIS?", " 2 Why care about GIS? How lost would you be without Google Maps? Figure 2.1: Screenshot of Google Maps for Cape Town. If we search “Geographic Information Systems OR GIS” in Web of Science, it is clear that the number of papers using GIS has exploded over time! Figure 2.2: The number of papers on Web of Science when searching Geographic Information Systems OR GIS. GIS is especially important in environment and life sciences! Figure 2.3: The number of papers on Web of Science by theme when searching Geographic Information Systems OR GIS. We use it for things like mapping ecosystems and biomes. Figure 2.4: The historical extent of the biomes of South Africa from Mucina, Rutherford, and Others (2006). Or the loss of ecosystems and biomes. Figure 2.5: The remaining extent of the biomes of South Africa from Skowno, Jewitt, and Slingsby (2021). Although the outcomes of GIS analyses are not always maps, e.g. this table from Skowno, Jewitt, and Slingsby (2021). But it can also be interactive! Figure 2.6: The Global Forest Watch app. References "],["gis-basics.html", "3 GIS basics 3.1 What is GIS? 3.2 How do we do GIS? 3.3 How to get help?", " 3 GIS basics Here we cover the basics of GIS… 3.1 What is GIS? A Geographic Information System is a framework used to create, manage, visualize and analyze data in a spatial context. Most datasets can be assigned a spatial location, either on the earth’s surface or within some constrained area like a sports field, a vegetation survey plot, or even a drawer in your kitchen. While almost any dataset has a spatial context, and can thus be represented in a GIS, the real question is whether it needs to be analyzed in a GIS environment? The answer to this question depends on the purpose of the analysis. Typically, one would use GIS if you were using the spatial information associated with the data to: access data elements (e.g. select data by spatial location or coverage), perform analytical operations (e.g. spatially overlay and merge two datasets to produce a new dataset), or render visualizations (e.g. generate a map). 3.1.1 An example workflow Identifying the list of South Africa’s threatened ecosystems is largely based on analyses done in GIS, using the map of the historical extent of South Africa’s vegetation types and the National Land Cover data (which, incidentally, were largely created using GIS…). Figure 3.1: South Africa’s list of threatened ecosystems depends heavily on GIS… Here’s a simplified breakdown of the steps in the GIS workflow… Figure 3.2: A simplified workflow for assessing threat to South Africa’s ecosystems, highlighting GIS operations (numbered and in italics). There are many many operations and functions within each of the three major operation types (access, analyze, map). There are also many support functions and operations that are common across the operation types. We’ll delve into these later. 3.2 How do we do GIS? There are a large number of software packages for doing GIS. Some are free (often open source), while others require you to pay for a license (i.e. are proprietary). They also vary in what they do (or the combination of things they do). For example, some of the best known desktop GIS applications, which is what your average GIS user is interested in, include: ArcGIS - a proprietary product that works on Windows only (or online). UCT has a site license, which provides a set number of user licenses for UCT postgrads and staff (see section 3.3 for details). Note that ArcGIS is an ESRI product and comes with (or can be upgraded to include) a suite of other (commercial) software components in addition to the desktop application QGIS (“Quantum GIS”) - free and open source software (FOSS) for Windows, Mac or Linux Google Earth - free (for now), but not open source, for Windows, Mac, Linux, or online Figure 3.3: Screenshot of the QGIS Desktop 3.14 graphical user interface (GUI). Desktop GIS applications are typically graphical user interfaces (GUIs) that call on various other geospatial libraries to do the actual data processing (think of these as sets of functions or tools). Some examples of FOSS geospatial libraries are GDAL and GEOS, which are designed to deal with different data models and/or file types. Another one you’ll encounter quite a bit is PROJ, which deals with coordinate reference systems. We can also do GIS by calling the geospatial libraries and other GIS software types directly with various programming languages. While we can use many (or just about any) coding language, there are a few for which the functions and syntax are better developed for the user, including: R Python JavaScript A major advantage of using these general purpose programming languages for your GIS work is that it allows you take advantage of their functions for statistical analyses and presentation etc all in one environment and/or work flow. This also makes it much easier to make your research reproducible. Note that the geospatial libraries and the other GIS softwares that we call are coded in a variety of other languages, such as Java, C/C++, C# or SQL, but these languages are typically less user friendly and/or more difficult to learn. Other “GIS software types” include: spatial databases such as PostGIS, which is free and open source software (FOSS) for Windows, Mac or Linux and great for storing and querying large and complex datasets web mapping such as MapServer or GeoServer I’m sure there are others I haven’t thought of… 3.3 How to get help? UCT has a GIS Support Unit to assist UCT staff and postgraduate researchers with their GIS and spatial data needs. Their primary goal is to help users develop their GIS skills in order to perform sound data capture, geospatial analysis and map production. They can help you with: Troubleshooting, Project Planning, Analysis, Cartographic Design and Data Handling. GIS Training Applying for an ESRI (ArcMAP) Software license. Note that they predominantly work with ESRI and QGIS and don’t provide support for R, etc. That said, many GIS work flow issues are common across platforms, and the support unit really know what they’re doing. They’re also a good source of data if you’re struggling to find what you need, but please do your homework before asking them for data! Lastly… I am not a help desk!!! The goal for this module is to teach you how to help yourself. I’m available to help in the afternoon office hours during this module, but unfortunately, while I would love to, I do not have the time to help you all with your GIS or R issues for the rest of your careers… You can find the answer to any issue you are encountering in an online forum like GIS Stack Exchange. I typically just type my questions (or copy and past error messages) directly into Google. The trick is working out what to ask, and sometimes you need to reword your question a couple of times to find the answer you need. If you’re really getting nowhere, you can even post your question on a forum, although it is unlikely that you’ll ever need to do this… "],["gis-data-models-and-file-formats.html", "4 GIS data models and file formats 4.1 Data models 4.2 Attribute data 4.3 File formats", " 4 GIS data models and file formats 4.1 Data models GIS data typically come in two data model types vector or raster. 4.1.1 Vector data The three basic vector data types are points, lines (also sometimes referred to as polylines or linestrings) and polygons. While they are treated as different data types, you can also consider them to be a nested hierarchy. For example, to make a line you need two or more points, while a polygon requires three or more lines. Figure 4.1: The hierarchical construction of vector data types. From this we can observe the different properties of the data types: a point is a location in space defined by a set of coordinates based on a coordinate reference system (more about these later) a line is two or more points with straight lines connecting them, where each line has a length a polygon is a set of points connected by lines that form a closed shape, which has an area Note that these “data types” are also commonly called feature classes, geometric primitives or geometries. Later we’ll see that you get more complicated “types”, but these are generally combinations of the above: multipoint, multilinestring, multipolygon, geometry collection, etc and are largely just different data classes designed to help with handling data than unique geometries. Vector data models are obviously the best way to represent points and lines. Polygons are usually the best way to represent discrete (categorical) data, especially where they may have complex boundaries. For example: Figure 4.2: Vector (polygon) representation of discrete data; the vegetation types of the Cape Peninsula. Vector data models are less good for representing continuous data (e.g. elevation, see surface temperature, etc). See further down. 4.1.2 Raster data Raster data are essentially data stored in a regular grid of pixels (or cells). Digital images like jpeg or png files are essentially rasters without spatial information. The value of each pixel is a number representing a measured value (e.g. continuous data such as sea surface temperature) or a category (e.g. discrete data such as land cover class). All pixels have a value, even if the value is “No Data”. Figure 4.3: Raster representation of continuous data; a digital elevation model of the Cape Peninsula. Rasters are particularly useful for representing continuous data. If this was a vector plot of the raw data, each pixel would have to be its own polygon and the legend would have a separate entry for each unique value, &gt;60 000 entries!!! That said, you can quite effectively represent continuous values visually with a vector data model if you bin the continuous data (from the raster) into classes, such as one can do with a filled contour plot (see below). This is not ideal for analyses though, as the binning results in data loss. You’ll find that you often need to convert data between vector and raster models for various reasons, and that this usually means some tough decisions need to be made about what is acceptable data loss. We’ll cover that later. Figure 4.4: Vector representation of continuous data; a filled contour plot of a digital elevation model of the Cape Peninsula using 100m contours. Conversely, rasters are usually not that good at representing categorical data. Note that most raster file formats (and GIS software) can only store numeric data, so this plot misleadingly represents the vegetation types as continuous data. You can label and represent categorical data in rasters in R, but this is usually more effort than its worth and is almost always less effective than using a vector format… A common exception is land use and land cover (LULC) maps, where remotely sensed satellite imagery (raster data) are classified into predefined classes (e.g. agriculture, rock, grassland, etc) based on various criteria or algorithms. Even then, these are difficult to interpret visually with static maps and are best visualized as interactive maps so you can make sense of them by zooming in and panning around. Figure 4.5: Raster representation the discrete data; the vegetation types of the Cape Peninsula. 4.2 Attribute data Attributes are what we know about the objects represented in a layer in addition to their geometry - i.e. each spatial object usually has additional information associated with it. These data are usually stored in an associated Attribute Table. Here are the first few entries of the attribute table for our Cape Peninsula vegetation vector layer: AREA_HCTR PRMT_MTR veg type Subtype Community geometry 66 6.774255 1596.83494 Beach - FalseBay BEACH Need to Find Out POLYGON ((-46636.54 -380320… 67 14.151168 3886.68578 Beach - FalseBay BEACH Need to Find Out POLYGON ((-47220.45 -380302… 68 8.575597 2154.00714 Beach - FalseBay BEACH Need to Find Out POLYGON ((-48967.57 -380253… 69 0.000001 23.25575 Beach - FalseBay BEACH Need to Find Out POLYGON ((-49355.61 -380223… 70 5.333203 3589.09436 Beach - FalseBay BEACH Need to Find Out POLYGON ((-50008.26 -380132… 71 24.448116 7378.70451 Beach - FalseBay BEACH Need to Find Out POLYGON ((-52927.7 -3800156… Note that vector data generally have attribute tables, but they are rare for raster layers, because most raster file formats can store just one attribute per cell (e.g. elevation) and can’t have associated attribute tables. A handy feature of most GIS systems is that they can treat attribute tables like relational database table structures. Additional information can be joined onto your spatial data by joining two tables with a common key field, as one does when joining two tables of non-spatial data. In GIS, this is called an “Attribute Join”, because you have joined the tables by attribute and haven’t used spatial information (also sometimes called a “non-spatial join”). We’ll learn about “spatial joins” later… WARNING! The values in attribute tables are typically static and are not recalculated every time you alter the feature of interest. For example, you can crop the Cape Peninsula vegetation layer, but the values in the AREA_HCTR (area) and PRMT_MTR (perimeter) columns of the attribute table will not change, even if the polygons in question are now smaller! 4.3 File formats Linked to data models, and attributes, is file formats. Generally, there are separate file formats for vector vs raster data. Usually, we even have separate files for the different types of vectors (points, lines, polygons, etc), but this is changing as new “database” formats evolve. There is a huge variety of GIS file formats, which have proliferated as different software packages have developed their own set of “native” formats. Each of these have different properties in terms of the data they store, whether they can include attribute data, file size and compression, and of course how they actually store (and retrieve) the data. Many of these, like the ESRI formats, are proprietary (i.e. not open source). If you’ve done any GIS before, you’ll be familiar with ESRI shapefiles, which usually include a group of 3 or more files with the same name, but a different file extension. Each file stores different information. The most common ones are: .shp = the main feature geometry .shx = an index file, used for searching etc .dbf = stores the attribute information .prj = stores the coordinate reference system etc = there are many other optional files that may be present depending on the data stored Shapefiles are by far the most common format for vector data. For raster data, the most common format is probably GeoTIFF (.tif) or ASCII (.asc). You can view the lists of most of the file types supported by R (or rather the GDAL software that underlies most of R’s spatial data capabilities) by running the code sf::st_drivers() which gives this output: name long_name write copy is_raster is_vector vsi ESRIC ESRIC Esri Compact Cache FALSE FALSE TRUE TRUE TRUE PCIDSK PCIDSK PCIDSK Database File TRUE FALSE TRUE TRUE TRUE netCDF netCDF Network Common Data Format TRUE TRUE TRUE TRUE FALSE PDS4 PDS4 NASA Planetary Data System 4 TRUE TRUE TRUE TRUE TRUE VICAR VICAR MIPL VICAR file TRUE TRUE TRUE TRUE TRUE JP2OpenJPEG JP2OpenJPEG JPEG-2000 driver based on OpenJPEG library FALSE TRUE TRUE TRUE TRUE PDF PDF Geospatial PDF TRUE TRUE TRUE TRUE FALSE MBTiles MBTiles MBTiles TRUE TRUE TRUE TRUE TRUE BAG BAG Bathymetry Attributed Grid TRUE TRUE TRUE TRUE TRUE EEDA EEDA Earth Engine Data API FALSE FALSE FALSE TRUE FALSE OGCAPI OGCAPI OGCAPI FALSE FALSE TRUE TRUE TRUE ESRI Shapefile ESRI Shapefile ESRI Shapefile TRUE FALSE FALSE TRUE TRUE MapInfo File MapInfo File MapInfo File TRUE FALSE FALSE TRUE TRUE UK .NTF UK .NTF UK .NTF FALSE FALSE FALSE TRUE TRUE LVBAG LVBAG Kadaster LV BAG Extract 2.0 FALSE FALSE FALSE TRUE TRUE OGR_SDTS OGR_SDTS SDTS FALSE FALSE FALSE TRUE TRUE S57 S57 IHO S-57 (ENC) TRUE FALSE FALSE TRUE TRUE DGN DGN Microstation DGN TRUE FALSE FALSE TRUE TRUE OGR_VRT OGR_VRT VRT - Virtual Datasource FALSE FALSE FALSE TRUE TRUE Memory Memory Memory TRUE FALSE FALSE TRUE FALSE CSV CSV Comma Separated Value (.csv) TRUE FALSE FALSE TRUE TRUE GML GML Geography Markup Language (GML) TRUE FALSE FALSE TRUE TRUE GPX GPX GPX TRUE FALSE FALSE TRUE TRUE KML KML Keyhole Markup Language (KML) TRUE FALSE FALSE TRUE TRUE GeoJSON GeoJSON GeoJSON TRUE FALSE FALSE TRUE TRUE GeoJSONSeq GeoJSONSeq GeoJSON Sequence TRUE FALSE FALSE TRUE TRUE ESRIJSON ESRIJSON ESRIJSON FALSE FALSE FALSE TRUE TRUE TopoJSON TopoJSON TopoJSON FALSE FALSE FALSE TRUE TRUE OGR_GMT OGR_GMT GMT ASCII Vectors (.gmt) TRUE FALSE FALSE TRUE TRUE GPKG GPKG GeoPackage TRUE TRUE TRUE TRUE TRUE SQLite SQLite SQLite / Spatialite TRUE FALSE FALSE TRUE TRUE ODBC ODBC FALSE FALSE FALSE TRUE FALSE WAsP WAsP WAsP .map format TRUE FALSE FALSE TRUE TRUE PGeo PGeo ESRI Personal GeoDatabase FALSE FALSE FALSE TRUE FALSE MSSQLSpatial MSSQLSpatial Microsoft SQL Server Spatial Database TRUE FALSE FALSE TRUE FALSE PostgreSQL PostgreSQL PostgreSQL/PostGIS TRUE FALSE FALSE TRUE FALSE OpenFileGDB OpenFileGDB ESRI FileGDB FALSE FALSE FALSE TRUE TRUE DXF DXF AutoCAD DXF TRUE FALSE FALSE TRUE TRUE CAD CAD AutoCAD Driver FALSE FALSE TRUE TRUE TRUE FlatGeobuf FlatGeobuf FlatGeobuf TRUE FALSE FALSE TRUE TRUE Geoconcept Geoconcept Geoconcept TRUE FALSE FALSE TRUE TRUE GeoRSS GeoRSS GeoRSS TRUE FALSE FALSE TRUE TRUE VFK VFK Czech Cadastral Exchange Data Format FALSE FALSE FALSE TRUE FALSE PGDUMP PGDUMP PostgreSQL SQL dump TRUE FALSE FALSE TRUE TRUE OSM OSM OpenStreetMap XML and PBF FALSE FALSE FALSE TRUE TRUE GPSBabel GPSBabel GPSBabel TRUE FALSE FALSE TRUE FALSE OGR_PDS OGR_PDS Planetary Data Systems TABLE FALSE FALSE FALSE TRUE TRUE WFS WFS OGC WFS (Web Feature Service) FALSE FALSE FALSE TRUE TRUE OAPIF OAPIF OGC API - Features FALSE FALSE FALSE TRUE FALSE EDIGEO EDIGEO French EDIGEO exchange format FALSE FALSE FALSE TRUE TRUE SVG SVG Scalable Vector Graphics FALSE FALSE FALSE TRUE TRUE Idrisi Idrisi Idrisi Vector (.vct) FALSE FALSE FALSE TRUE TRUE XLS XLS MS Excel format FALSE FALSE FALSE TRUE FALSE ODS ODS Open Document/ LibreOffice / OpenOffice Spreadsheet TRUE FALSE FALSE TRUE TRUE XLSX XLSX MS Office Open XML spreadsheet TRUE FALSE FALSE TRUE TRUE Elasticsearch Elasticsearch Elastic Search TRUE FALSE FALSE TRUE FALSE Carto Carto Carto TRUE FALSE FALSE TRUE FALSE AmigoCloud AmigoCloud AmigoCloud TRUE FALSE FALSE TRUE FALSE SXF SXF Storage and eXchange Format FALSE FALSE FALSE TRUE TRUE Selafin Selafin Selafin TRUE FALSE FALSE TRUE TRUE JML JML OpenJUMP JML TRUE FALSE FALSE TRUE TRUE PLSCENES PLSCENES Planet Labs Scenes API FALSE FALSE TRUE TRUE FALSE CSW CSW OGC CSW (Catalog Service for the Web) FALSE FALSE FALSE TRUE FALSE VDV VDV VDV-451/VDV-452/INTREST Data Format TRUE FALSE FALSE TRUE TRUE MVT MVT Mapbox Vector Tiles TRUE FALSE FALSE TRUE TRUE NGW NGW NextGIS Web TRUE TRUE TRUE TRUE FALSE MapML MapML MapML TRUE FALSE FALSE TRUE TRUE TIGER TIGER U.S. Census TIGER/Line FALSE FALSE FALSE TRUE TRUE AVCBin AVCBin Arc/Info Binary Coverage FALSE FALSE FALSE TRUE TRUE AVCE00 AVCE00 Arc/Info E00 (ASCII) Coverage FALSE FALSE FALSE TRUE TRUE HTTP HTTP HTTP Fetching Wrapper FALSE FALSE TRUE TRUE FALSE Note that you can specify the what = argument in the function to \"vector\" or \"raster\" if you want only the drivers specific to each. In short, there’s lots!!! But note that there are others that are not supported in R. Perhaps the most common unsupported ones you’ll encounter are the ESRI geodatabases (.gdb and .mdb), which are designed for ArcGIS and are super efficient (in ArcGIS), but ESRI haven’t released the drivers, so they don’t work (or at least not properly) for most other GIS software… Note that there has been a big push to develop a standardized set of open source, efficient and interoperable file formats. Some examples to watch: GeoPackage - SQLite database containers for storing vector, raster and attribute data in a compact and transferable format. GeoJSON - a geographic version of JSON (JavaScript Object Notation) for vector data, very commonly used for web apps etc. Cloud-optimized GeoTIFF - as the name suggests; a GeoTIFF-based format for optimally hosting and allowing querying and downloading of raster data on the cloud… Simple Features - an open, efficient and interoperable standard for vector data. "],["some-important-concepts-and-pitfalls.html", "5 Some important concepts and pitfalls 5.1 Scale 5.2 Coordinate Reference Systems (CRS)", " 5 Some important concepts and pitfalls 5.1 Scale All maps have a scale. Scale is the ratio between the size of the representation of an object and its size in reality. E.g. objects on a 1:50,000 scale map are drawn at 1/50,000 their size, so 1cm on the map represents a distance of 500m in reality (i.e. \\(1*50,000 = 50,000cm = 500m\\)). Figure 5.1: The Cape Peninsula (3418AB &amp; AD) from South Africa’s 1:50,000 topographic map series. The printed scale for these maps is 1:50,000, but what is it on your screen? GIS is usually scaleless (or at least flexible in scale); we can “zoom in” as much as we want to, and perform operations at just about any scale we want to, but should we? There are lots of issues we need to consider! Representation (i.e. mapping)… There are 2 issues here: Firstly, a 1mm thick line on a 1:50,000 scale map would be 50m wide in reality. Conversely, a 5m wide road would be 1/10mm on the map. Would the map be readable? Sometimes we break the rules of scale to make maps readable. Bear this in mind! Secondly, scale and our desired representation affect how we capture data. For example, a road is typically best represented as a line at 1:5,000 scale or smaller (note that scale is a ratio, so “small scale” = large extent or area!). At 1:1,000 scale a 5m wide road would be 5mm across on the map, so one might capture it as a polygon to represent its area. You should always consider the purpose for which (and scale at which) the data were captured before using them for a new application! This affects both the appropriateness of the data type and the accuracy and precision of the data… Figure 5.2: Zoomed in on Chapman’s peak on the the Cape Peninsula 1:50,000 topographic map (3418AB &amp; AD). At this scale the road (in red) is probably better mapped as an area than a line? Accuracy of location versus scale of data capture. We should always check the scale at which the data were captured to make sure it is accurate enough for the scale of the analysis we are doing. For example, the various vegetation units in the National Vegetation Map of South Africa were mapped at a range of scales, some as small as 1:250,000. At this scale 1mm = 250m, so a minor digitization error is a huge difference on the ground! If you need your analysis to be accurate to &lt;10m then you’d probably need data mapped at a scale larger than 1:10,000. Precision - Can mean two things: The unit or number of decimal places to which the attribute has been measured (and can be stored) The spread of repeat measurements (typically in field data collection). A big spread means the measurements weren’t very precise… A quick aside on the difference between accuracy and precision! Figure 5.3: The difference between accuracy and precision, where the true value is the origin (0,0). A last word on Scale… For vector data, we typically refer to scale when describing a data set. For raster data, we typically refer to pixel resolution (or sample interval). For example, a 30m digital elevation model is made up of pixels 30m across. For remotely sensed imagery (i.e. from drone, plane or satellite), one often uses the term “ground sample distance”. BEWARE!!! If you convert between raster and vector data formats (e.g. by “rasterizing” a vector layer, or “binning” a raster into polygons), it will affect all three of precision, accuracy and representation, so you need to give careful thought to whether what you are doing is appropriate for the analysis you are doing! 5.2 Coordinate Reference Systems (CRS) Coordinate Reference Systems (CRS) provide a framework for defining real-world locations. There are many different CRSs, with different properties. They can be a minefield, and I don’t have the time to cover them in any detail. I provide some of the basics here, and list some of the golden rules (mostly from the GIS Support Unit) below. 5.2.1 Geographic (or “unprojected”) Coordinate Systems The most common coordinate system is latitude/longitude, also known as geographic, lat/long or sometimes WGS84. There are many ways to record geographic coordinates: Degrees, Minutes &amp; Seconds: S33°26’46”,E18°10’23’’ Degrees and decimal minutes: S33°26.7666667’,E18°10.3833333’ (out to get you!) Decimal Degrees: -33.4461111,18.17305556 Most GIS prefer decimal degrees… The problem with doing analyses using geographic CRS is that lat/long coordinates are actually angular measurements on a 3D sphere (Geodesic) and degrees differ in their actual ground distance depending on where you are on the planet. They also differ in the N-S vs W-E plane! Figure 5.4: Map highlighting that a degree is larger at the equator than at the poles. Image source: https://annakrystalli.me/intro-r-gis/gis.html This means that Euclidean measurement calculations are not appropriate for calculating areas and distances on data in the geographic CRS. 5.2.2 Projected Coordinate Systems To perform linear measurements from a 3D shape using Euclidean methods, you need to squash that shape into a 2D plane. This squashing is called a projection… Figure 5.5: How many different ways could you flatten a naartjie peel? There are 4 properties that get distorted, you can pick which one gets preserved the best by a projection type: Shape - you want a Conformal projection Area - Equal-Area Distance - Equidistant Direction - Azimuthal (see here for a nice description and illustrations) Some “general purpose” projections, like Transverse Mercator (TM), try to compromise and minimize distortion in all properties, but can’t preserve any perfectly. Their distortions also tend to get worse the larger the spatial extent being analyzed. Projections get tuned to best fit an area through the use of projection parameters. For example, Transverse Mercator, which is used by the 1:50,000 map series and by Municipalities like Cape Town, uses a narrow projection window of 2 degree-wide bands. As a result, our map series projection parameters are set by moving the central meridian (or tangent) line of longitude every odd degree across the country. Cape Town is close to 19°E, so our version is colloquially called ‘Lo19’. For Durban you’d use ‘Lo31’. Universal Transverse Mercator (UTM) is similar, but uses 6 degree-wide bands so that it can be applied across larger extents. 5.2.3 Projection codes The type of CRS is usually (but not always) stored in the metadata of your file (or dataset, if it is comprised of multiple files like an ESRI shapefile). There are various formats for this, the most commonly used in R being known as EPSG, PROJ4 or WKT codes or strings (be warned, there are many more…). To assign a CRS or reproject your data in R you need to know the appropriate code in the format required by the function. Fortunately, there is a huge online library of the codes at https://spatialreference.org/. I also provide some suggested projections, based on the properties you’d like to preserve, and their codes here. Note that for UTM and TM you may need to adjust the PROJ4 strings for your area - read the comments. Why do projections matter? Figure 5.6: Africa, visualized with different coordinate reference systems. All four maps are different, even if the differences may be subtle! 5.2.4 “On the fly” vs manual projection Note that some GIS tools can perform “on the fly” (re)projection of data. For example, by default ArcGIS sets the CRS for a project from the first dataset imported. When you want to visualize the data, it will reproject all other datasets to the set CRS so that it can visualize it properly. Similarly, ArcGIS and other software can project data in a geographic CRS to a projected CRS on the fly when asked to perform Euclidean measurement calculations. On the fly projection can clearly be very useful, but it can also be misleading if you don’t know what its doing. This is especially problematic if it uses the wrong projection for the property you want to preserve! You should always check the default settings for the software you’re using, and check the set CRS(s) and individual dataset CRS(s) to make sure you are working in a suitable CRS for the operations you want to perform!!! 5.2.5 The golden rules… If things don’t line up, its probably a CRS issue. You need to know what CRS your dataset is in. This is essential, because you need to define your projections to be able to compare datasets. If they are not the same, you will need to reproject one to align with the other*. If your datasets are not in the same CRS, most GIS software will give you warning or error messages, but not always! Note that not all file formats store the CRS “metadata”, so check and store it yourself if needed! You need to make sure you use a CRS that best preserves the properties you are interested in (area, distance, direction, shape). More on this in section 5.2.2. If your areas and distances are stupidly small (0.001 etc) your data are probably in Geographic (i.e. degrees and not a unit of distance like metres). Always interrogate and “common-sense-check” your results!!! *Defining Projection is not the same as reprojecting! Think in terms of languages, “I have text in Japanese and want it in English”. Defining is saying what it IS (“This text is in Japanese” - defining Japanese as English gives you garbage). Reprojecting is what you want it to be (i.e. translate Japanese to English). Two other issues to look out for: The official South African CRS is waiting to get you. If you see Gauss Conform run screaming, its a left handed CRS based on Southings and Westings (i.e. completely inverted…). Simple, yes? Datums… These are essentially models of the shape of the surface of the planet. Most South African datasets (since 1999 at least) use the Hartebeesthoek 94 datum, which is our local “bespoke” solution. It’s pretty much the same as the WGS84 datum (a good global datum), and the difference is negligible for most ecological analyses. Our former local “bespoke” datum (the Cape or Clarke1880 datum), which was often used for data before 1999, is out to get you. These datasets will never line up perfectly with modern data sets when reprojected in a normal GIS and will usually be a couple of tens of metres off… Some other important pitfalls to avoid are best covered in this chapter in Manny Gimond’s Intro to GIS and Spatial Analysis. "],["r-as-a-gis.html", "6 R as a GIS 6.1 Overview 6.2 Some key R packages", " 6 R as a GIS 6.1 Overview Points, lines, polygons and rasters - R can handle them all and more! R is a free software environment for statistical computing and graphics, but its abilities have been extended into many realms through the &gt;20,000 (!) contributed extension packages (also called libraries). The list of packages can be bewildering, but fortunately some great folks have taken the time to sift through and make some sense of them for different focal topics and created Task Views. For GIS there are two Task Views of interest: Spatial - maintained by Roger Bivand and Jakub Nowosad, and SpatioTemporal - maintained by Edzer Pebesma and Roger Bivand They overlap somewhat, but the latter specifically focuses on data where both location and time of observation are registered, and relevant for the analysis of the data. Each has an overview page listing packages and highlighting their respective strengths, weaknesses etc., e.g. Figure 6.1: Screenshot of the “Spatial” Task View at https://cran.r-project.org/ The Spatial Task View focuses on “Analysis of Spatial Data”, with sections on: Classes for spatial data and metadata Reading and writing spatial data Handling spatial data Visualizing spatial data Analyzing spatial data Task Views also allow easy download and installation of all packages in a Task View using library(ctv) (which you can install with install.packages(\"ctv\")). In this case the code you’d need to install the Task View is ctv::install.views(\"Spatial\", coreOnly = TRUE). But beware! If you leave out the coreOnly = TRUE it can take a while to download and install!!! It may take a while even then… You don’t need the whole Task View for my tutorials, so don’t bother downloading it if you’re just working through these. Figure 6.2: Screenshot of the Task View landing page at https://cran.r-project.org/ 6.2 Some key R packages We don’t have time to go through all packages or provide a full history, but here are some notes in brief. 6.2.1 For vector data (although some of these packages can handle rasters too) The leading package was sp. It is still active and useful, but is superseded by a newer package sf, which is a modern implementation and standardization of parts of sp. It is highly recommended that you use sf over the older packages as they will not be maintained in the long term, largely because they rely on other packages that are no longer maintained because their creators have retired. sf stands for “Simple Features for R”, in compliance with the OGC Simple Feature standard. It is highly efficient, and comes with the advantage that it uses Tidyverse principles and coding styles, e.g. allowing use of the pipe operator (%&gt;%) and the direct application of library(dplyr) data manipulation and library(ggplot2) visualization functions. I will use sf for the most part in the demonstration material. Unfortunately, not all operations are available in sf yet and I may still have to use sp at times, especially when performing operations using both vector and raster data. Here’s a quick list of the functions available in sf: library(sf) methods(class = &#39;sf&#39;) ## [1] [ [[&lt;- [&lt;- ## [4] $&lt;- aggregate anti_join ## [7] arrange as.data.frame cbind ## [10] coerce crs dbDataType ## [13] dbWriteTable distance distinct ## [16] dplyr_reconstruct drop_na duplicated ## [19] extent extract filter ## [22] full_join gather group_by ## [25] group_split identify initialize ## [28] inner_join left_join lines ## [31] mask merge mutate ## [34] nest pivot_longer pivot_wider ## [37] plot print raster ## [40] rasterize rbind rename_with ## [43] rename right_join rowwise ## [46] sample_frac sample_n select ## [49] semi_join separate_rows separate ## [52] show slice slotsFromS3 ## [55] spread st_agr st_agr&lt;- ## [58] st_area st_as_s2 st_as_sf ## [61] st_as_sfc st_bbox st_boundary ## [64] st_break_antimeridian st_buffer st_cast ## [67] st_centroid st_collection_extract st_concave_hull ## [70] st_convex_hull st_coordinates st_crop ## [73] st_crs st_crs&lt;- st_difference ## [76] st_drop_geometry st_filter st_geometry ## [79] st_geometry&lt;- st_inscribed_circle st_interpolate_aw ## [82] st_intersection st_intersects st_is_valid ## [85] st_is st_join st_line_merge ## [88] st_m_range st_make_valid st_minimum_rotated_rectangle ## [91] st_nearest_points st_node st_normalize ## [94] st_point_on_surface st_polygonize st_precision ## [97] st_reverse st_sample st_segmentize ## [100] st_set_precision st_shift_longitude st_simplify ## [103] st_snap st_sym_difference st_transform ## [106] st_triangulate_constrained st_triangulate st_union ## [109] st_voronoi st_wrap_dateline st_write ## [112] st_z_range st_zm summarise ## [115] transform transmute ungroup ## [118] unite unnest ## see &#39;?methods&#39; for accessing help and source code This doesn’t tell you how to use them though. To get help with a function in R just type “?” followed by the function name, e.g. ?st_read, and it’ll take you to the help page. Of course, you don’t want to have to read every help page to find the function you want! Fortunately, here’s a “cheat sheet” that allows you to find the function you want relatively quickly (once you’re familiar with the syntax etc): Figure 6.3: An R cheat sheet for library(sf) by Ryan Garnett (page 1). Figure 6.4: An R cheat sheet for library(sf) by Ryan Garnett (page 2). 6.2.2 For raster data By far the best package has been raster, maintained by Robert Hijmans (of WorldClim fame), and can do just about anything with rasters and interfaces with sp very nicely. Unfortunately, both raster and sp are being phased out as explained above. raster is currently being superseded by a new package called terra, also being developed by Hijmans. “terra is very similar to the raster package; but terra is simpler, better, and faster” - Roger Bivand I first developed this module using raster, because terra was still largely in development, so I have kept the demonstrations of raster for those who depend on them for now, but highly recommend you start using terra, because I’ll likely remove the raster material next year. It’s worth noting that raster and terra can handle vector data directly too, and that sometimes their integration with sf is a little clunky, but it does seem to get better all the time. Unfortunately, there’s no cheat sheet for terra or raster, but there’s a lot of documentation and tutorials here. terra is also able to handle spatiotemporal arrays (raster and vector data cubes). Think of these as time-series of GIS data, like satellite archives, etc. Other powerful packages to watch in the spatiotemporal space are stars and gdalcubes. "],["rdemo.html", "7 Vector GIS operations in R 7.1 Case study and demo datasets 7.2 Reading and writing 7.3 Basic plotting 7.4 Cropping 7.5 Select and subset by attribute 7.6 Combine classes and dissolve by attribute 7.7 Calling iNaturalist locality (point) data from R 7.8 Converting a dataframe into a spatial object 7.9 Adding basemaps to plots 7.10 Interactive maps with leaflet and mapview 7.11 Reprojecting 7.12 Intersecting points and polygons 7.13 Colour or label points 7.14 Buffering 7.15 Within distance and intersect", " 7 Vector GIS operations in R 7.1 Case study and demo datasets Ok, for demonstrating some of the many GIS operations R can perform we will be using data from one of my favourite study areas, the Cape Peninsula. The datasets we will use, some of their properties and where to source them are tabled below. You can also download them as one (40MB) .zip file here, because the City of Cape Town updates the files from time to time and small changes in format, naming etc break the code to come. Please do not use the version from the .zip file in any real analyses etc, because I can make no guarantees about their accuracy etc. It is best to use the latest version from the links in the table. Name Data source Data model Geometry type File format Localities iNaturalist Vector Point Data frame from API Watercourses City of Cape Town Vector Line ESRI shapefile Vegetation Types (historical) City of Cape Town Vector Polygon ESRI shapefile Vegetation Types (remnants) City of Cape Town Vector Polygon ESRI shapefile Elevation City of Cape Town Raster Raster MapServer If you’d like to follow along and run the analyses that follow, please download the datasets. There’s no need to download the iNaturalist data as we’ll download it directly from R. For installing R and the required packages see section 1.2. 7.2 Reading and writing sf has a one-size-fits-all approach in that most functions can be applied to most different data types (point, line, polygon, etc) or, in the case of reading and writing, file formats. To read data the function you want is st_read(). You’ll note that most of the sf functions begin with “st_” - this stands for “spatial and temporal” and is the same in some other GIS like PostGIS. Let’s try to read in some data with st_read(): NOTE: if you’re trying any of the read/write code at home, you’ll need to set the file path to where you put the data and want the outputs on your local machine. You can also use ?setwd to simplify this. If you are on Windows, make sure to change the backslashes to either double backslashes or forward slashes “/”. library(sf) veg &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous.shp&quot;) ## Reading layer `Vegetation_Indigenous&#39; from data source ## `/Users/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1325 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63972.95 ymin: -3803535 xmax: 430.8125 ymax: -3705149 ## Projected CRS: WGS_1984_Transverse_Mercator This has successfully read in the data and given us a summary of some of its properties. Note the “Projected CRS” WGS_1984_Transverse_Mercator, so it is Transverse Mercator (TM), using the WGS84 datum, but it hasn’t told us what line of longitude it’s centred on, which is an essential feature of any TM projection. The first thing you should do when interrogating any spatial data is to check the coordinate reference system (CRS). In sf, you do this with the function st_crs, like so: st_crs(veg) ## Coordinate Reference System: ## User input: WGS_1984_Transverse_Mercator ## wkt: ## PROJCRS[&quot;WGS_1984_Transverse_Mercator&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;Hartebeesthoek94&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6148]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;unnamed&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,19, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,1, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] This shows us the CRS as a WKT string and looks very complicated… Essentially there are three components to it: BASEGEOGCRS - the geographic (or unprojected) CRS CONVERSION - the projection, which includes a lot of information, but essentially tells us it’s Transverse Mercator, and the \"Longitude of natural origin\",19 indicates that it is centred on the 19 degree line of longitude (i.e. we’re dealing with Transverse Mercator Lo19) CS - the cartesian axes, showing that we’re dealing with axes oriented to North and East and units of metres TM Lo19 is a good projection for most calculations at this scale (and on this line of longitude). If you’re using Transverse Mercator, always make sure it is set for your closest “odd” line of longitude (i.e. Lo19, Lo21, Lo23)! More on working with coordinate reference systems in see section 7.11. Let’s have a closer look at the data: class(veg) ## [1] &quot;sf&quot; &quot;data.frame&quot; It is an object of two different “classes”, a data.frame, which is an R object class you should be familiar with, and class sf, which is the native class for the sf library. The nice thing about being both classes is it means you can apply the functions built for either class, such as head, a commonly used function for looking at the first few rows of a dataframe. head(veg) ## Simple feature collection with 6 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -51324.95 ymin: -3732075 xmax: -35653.98 ymax: -3718136 ## Projected CRS: WGS_1984_Transverse_Mercator ## AREA_HCTR PRMT_MTR National_ Subtype Community ## 1 1807.5183616 24763.8073 Atlantis Sand Fynbos on marine-derived acid sands Need to Find Out ## 2 2.1437754 609.5892 Atlantis Sand Fynbos on marine-derived acid sands Need to Find Out ## 3 0.2134855 185.5566 Atlantis Sand Fynbos on marine-derived acid sands Need to Find Out ## 4 2.8602421 652.1671 Atlantis Sand Fynbos on marine-derived acid sands Need to Find Out ## 5 0.5468058 336.8006 Atlantis Sand Fynbos on marine-derived acid sands Need to Find Out ## 6 0.4172046 259.7772 Atlantis Sand Fynbos on marine-derived acid sands Need to Find Out ## geometry ## 1 POLYGON ((-48203.88 -372294... ## 2 POLYGON ((-36676.72 -371974... ## 3 POLYGON ((-35891.46 -371837... ## 4 POLYGON ((-35750.07 -371847... ## 5 POLYGON ((-35823.89 -371817... ## 6 POLYGON ((-35929.18 -371824... Note there are 5 attribute columns (the attribute table as you would see in most GIS software) and a 6th geometry column. All sf objects have a geometry column. This is where it stores the geometry - i.e. the point, line, polygon etc - associated with each row of attribute data. To write data with sf you use st_write(), like so: st_write(veg, &quot;data/cape_peninsula/veg/Vegetation_Indigenous_duplicate.shp&quot;, append = FALSE) ## Writing layer `Vegetation_Indigenous_duplicate&#39; to data source ## `data/cape_peninsula/veg/Vegetation_Indigenous_duplicate.shp&#39; using driver `ESRI Shapefile&#39; ## Writing 1325 features with 5 fields and geometry type Polygon. Note that I added , append = FALSE because in my case it I want it to overwrite an existing file by the same name, and this command suppresses the warning it would usually give. file.exists(&quot;data/cape_peninsula/veg/Vegetation_Indigenous_duplicate.shp&quot;) ## [1] TRUE Confirms that the file exists, so it has written a file out successfully. Note that the function recognised that I wanted to write out an ESRI shapefile from the .shp file extension I provided. You can set the file type using the driver = setting in st_write(). Try st_drivers() for the list of file types supported. 7.3 Basic plotting As with other data types in R (and perhaps even more so with spatial data), you can really go to town with plotting. I’m only going to show you enough to be able to interrogate your data. Making it look pretty is a week-long course or more in its own right. Check out the “Making maps with R” chapter in Lovelace et al’s online book Geocomputation with R for a good start. You could also check out library(tmap) for plotting thematic maps or library(mapview) for interactive maps. The easiest way to plot datasets in R is often a bad thing to do when working with spatial datasets! plot(veg) Fortunately, in this case the dataset isn’t too big, but often you’ll either be overwhelmed with plots or your computer will crash… Why 5 plots and not one? This is because sf wants to plot the properties of each attribute in the attribute table. Fortunately, there were only 5, but there could have been hundreds! You can select the one you want with indexing like so: plot(veg[3]) These are the National Vegetation Types for the City of Cape Town municipality. You’ll note that we’re using the base R graphics functions. I mentioned before that sf integrates well with the Tidyverse, so this could also be plotted like so: library(tidyverse) #calls ggplot2 and other Tidyverse packages together ggplot() + geom_sf(data=veg, aes(fill = `National_`)) That’s better for the legend, but now we’ve squashed the map. Let’s narrow in on the Cape Peninsula for convenience. 7.4 Cropping Here we’ll apply the function st_crop(). To use the function you need an object to crop, and an extent or bounding box to crop to. sf is clever, and you can set the extent by giving it another object who’s extent you’d like to match (check the bounding box given when we read in the data earlier). We don’t have a second object in this case, so we have to provide a “numeric vector with named elements xmin, ymin, xmax and ymax”, like so: #Make a vector with desired coordinates in metres according to TM Lo19 ext &lt;- c(-66642.18, -3809853.29, -44412.18, -3750723.29) ext ## [1] -66642.18 -3809853.29 -44412.18 -3750723.29 #Give the vector names names(ext) &lt;- c(&quot;xmin&quot;, &quot;ymin&quot;, &quot;xmax&quot;, &quot;ymax&quot;) ext ## xmin ymin xmax ymax ## -66642.18 -3809853.29 -44412.18 -3750723.29 Now we can feed that into st_crop veg &lt;- st_crop(veg, ext) #Note that I&#39;m overwriting the old data object &quot;veg&quot; ## Warning: attribute variables are assumed to be spatially constant throughout all geometries ggplot() + geom_sf(data=veg, aes(fill = `National_`)) Better? Note that in this case I gave it the coordinates in TM Lo19. These are not always easy to work out, so you may want to create an extent using Lat/Long and then project it to TM Lo19 before cropping. You can do this like so: myextent &lt;- st_sf(a = 1:2, geom = st_sfc(st_point(c(18,-33)), st_point(c(19,-34))), crs = 4326) myextent &lt;- st_transform(myextent, crs = &quot;+proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;) # And then run veg &lt;- st_crop(veg, myextent) But what about the silly splits like Peninsula Granite Fynbos - North/South and Cape Flats Dune Strandveld - West Coast/False Bay. Which ones do I mean? 7.5 Select and subset by attribute Let’s select them from the attribute table and plot them. #Make a vector of the veg types we want split_veg &lt;- c(&quot;Peninsula Granite Fynbos - North&quot;, &quot;Peninsula Granite Fynbos - South&quot;, &quot;Cape Flats Dune Strandveld - West Coast&quot;, &quot;Cape Flats Dune Strandveld - False Bay&quot;) #Use base R indexing to select attributes vegsub &lt;- veg[which(veg$National_ %in% split_veg),] #Plot ggplot() + geom_sf(data=vegsub, aes(fill = `National_`)) Or tidyverse… #Using tidyverse piping to filter and plot veg %&gt;% filter(National_ %in% split_veg) %&gt;% ggplot() + geom_sf(aes(fill = `National_`)) #The advantage being that you don&#39;t have to make the intermediate &quot;vegsub&quot; object Ok. What if we decided we don’t want them split? 7.6 Combine classes and dissolve by attribute We can just rename them in appropriate column in the attribute table… vegsub$National_ &lt;- str_replace_all(vegsub$National_, c(&quot;Peninsula Granite Fynbos - North&quot; = &quot;Peninsula Granite Fynbos&quot;, &quot;Peninsula Granite Fynbos - South&quot; = &quot;Peninsula Granite Fynbos&quot;, &quot;Cape Flats Dune Strandveld - West Coast&quot; = &quot;Cape Flats Dune Strandveld&quot;, &quot;Cape Flats Dune Strandveld - False Bay&quot; = &quot;Cape Flats Dune Strandveld&quot;)) ggplot() + geom_sf(data=vegsub, aes(fill = `National_`)) Nice, but from the polygon boundaries we see that there are a number of adjacent polygons (i.e. they have shared boundaries) that are of the same veg type. We can “dissolve” and plot it without the unwanted boundaries using summarize(): vegsub %&gt;% group_by(National_) %&gt;% summarize() %&gt;% ggplot() + geom_sf(aes(fill = National_)) Ok… I think we’ve flogged that horse as far as it’ll go for now. Let’s bring in another dataset. How about points? 7.7 Calling iNaturalist locality (point) data from R A very cool feature of iNaturalist is that the team at rOpenSci have built a great R package for interfacing with it directly, called rinat! Let’s get all the records we can for the King Protea (Protea cynaroides). library(rinat) #Call the data directly from iNat pc &lt;- get_inat_obs(taxon_name = &quot;Protea cynaroides&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #View the first few rows of data head(pc) ## scientific_name datetime description ## 1 Protea cynaroides 2024-02-17 15:19:32 +0200 ## 2 Protea cynaroides 2024-02-18 11:06:57 +0200 ## 3 Protea cynaroides 2024-02-18 13:29:11 +0200 ## 4 Protea cynaroides 2024-02-18 07:50:17 +0200 ## 5 Protea cynaroides 2024-02-17 11:08:16 +0200 ## 6 Protea cynaroides 2024-02-17 11:35:46 +0200 ## place_guess latitude longitude tag_list ## 1 Wynberg NU (2), Cape Town, 7824, South Africa -33.98363 18.41677 ## 2 Table Mountain (Nature Reserve), Cape Town, South Africa -33.96553 18.41059 ## 3 Table Mountain National Park, ZA-WC-CT, ZA-WC, ZA -33.96780 18.42637 ## 4 Table Mountain National Park, Cape Town, WC, ZA -34.00400 18.40527 ## 5 Table Mountain National Park, ZA-WC-CT, ZA-WC, ZA -34.06891 18.39379 ## 6 Silver Mine (Nature Reserve), Cape Town, South Africa -34.11639 18.44388 ## common_name url ## 1 King Protea https://www.inaturalist.org/observations/199907030 ## 2 King Protea https://www.inaturalist.org/observations/199880775 ## 3 King Protea https://www.inaturalist.org/observations/199706446 ## 4 King Protea https://www.inaturalist.org/observations/199658012 ## 5 King Protea https://www.inaturalist.org/observations/199597143 ## 6 King Protea https://www.inaturalist.org/observations/199583340 ## image_url user_login ## 1 https://static.inaturalist.org/photos/352604636/medium.jpeg sonjastock ## 2 https://inaturalist-open-data.s3.amazonaws.com/photos/352554820/medium.jpeg susanthescout ## 3 https://inaturalist-open-data.s3.amazonaws.com/photos/352228494/medium.jpeg dryfveer ## 4 https://inaturalist-open-data.s3.amazonaws.com/photos/352137332/medium.jpg cajacobs ## 5 https://inaturalist-open-data.s3.amazonaws.com/photos/352016448/medium.jpeg sashie889 ## 6 https://inaturalist-open-data.s3.amazonaws.com/photos/351990638/medium.jpeg dryfveer ## id species_guess iconic_taxon_name taxon_id num_identification_agreements ## 1 199907030 Giant Protea Plantae 132848 2 ## 2 199880775 Giant Protea Plantae 132848 3 ## 3 199706446 King Protea Plantae 132848 1 ## 4 199658012 King Protea Plantae 132848 2 ## 5 199597143 Giant Protea Plantae 132848 2 ## 6 199583340 King Protea Plantae 132848 0 ## num_identification_disagreements observed_on_string observed_on ## 1 0 2024-02-17 15:19:32 2024-02-17 ## 2 0 2024-02-18 11:06:57 2024-02-18 ## 3 0 2024-02-18 13:29:11 2024-02-18 ## 4 0 2024-02-18 07:50:17+02:00 2024-02-18 ## 5 0 2024-02-17 11:08:16 2024-02-17 ## 6 0 2024-02-17 11:35:46 2024-02-17 ## time_observed_at time_zone positional_accuracy public_positional_accuracy geoprivacy ## 1 2024-02-17 13:19:32 UTC Pretoria NA NA &lt;NA&gt; ## 2 2024-02-18 09:06:57 UTC Pretoria 4 4 &lt;NA&gt; ## 3 2024-02-18 11:29:11 UTC Pretoria 12 12 &lt;NA&gt; ## 4 2024-02-18 05:50:17 UTC Pretoria 4 4 &lt;NA&gt; ## 5 2024-02-17 09:08:16 UTC Pretoria 8 8 &lt;NA&gt; ## 6 2024-02-17 09:35:46 UTC Pretoria 4 4 &lt;NA&gt; ## taxon_geoprivacy coordinates_obscured positioning_method positioning_device user_id ## 1 open false gps gps 2589781 ## 2 open false gps gps 1495441 ## 3 open false 2173153 ## 4 open false 7637674 ## 5 open false 5301909 ## 6 open false 2173153 ## user_name created_at updated_at quality_grade license sound_url ## 1 Sonja Stock 2024-02-20 16:41:14 UTC 2024-02-20 20:56:47 UTC research NA ## 2 Susan Gammon 2024-02-20 09:25:06 UTC 2024-02-20 11:39:37 UTC research CC-BY-NC NA ## 3 Santie Gouws 2024-02-18 19:22:46 UTC 2024-02-18 19:23:23 UTC research CC-BY-NC NA ## 4 2024-02-18 10:26:50 UTC 2024-02-19 10:31:13 UTC research CC-BY-NC NA ## 5 2024-02-17 20:25:20 UTC 2024-02-19 10:46:22 UTC research CC-BY-NC NA ## 6 Santie Gouws 2024-02-17 18:19:29 UTC 2024-02-17 18:21:24 UTC needs_id CC-BY-NC NA ## oauth_application_id captive_cultivated ## 1 2 false ## 2 2 false ## 3 2 false ## 4 3 false ## 5 2 false ## 6 2 false #Filter returned observations by a range of column attribute criteria pc &lt;- pc %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) class(pc) ## [1] &quot;data.frame&quot; Ok, so this is a dataframe with lat/long data, but it isn’t registered as an object with spatial attributes (i.e. geometries). 7.8 Converting a dataframe into a spatial object To make it an object of class(sf) we use the function st_as_sf(). #Make the dataframe a spatial object of class = &quot;sf&quot; pc &lt;- st_as_sf(pc, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) Note that I had to define the CRS!!! I defined it to be Geographic WGS84, using the EPSG code in this case. Recall the difference between defining and CRS and projecting of transforming the CRS as discussed in yesterday’s notes… #What class is it? class(pc) ## [1] &quot;sf&quot; &quot;data.frame&quot; #Note the new &quot;geometry&quot; column names(pc) ## [1] &quot;scientific_name&quot; &quot;datetime&quot; ## [3] &quot;description&quot; &quot;place_guess&quot; ## [5] &quot;tag_list&quot; &quot;common_name&quot; ## [7] &quot;url&quot; &quot;image_url&quot; ## [9] &quot;user_login&quot; &quot;id&quot; ## [11] &quot;species_guess&quot; &quot;iconic_taxon_name&quot; ## [13] &quot;taxon_id&quot; &quot;num_identification_agreements&quot; ## [15] &quot;num_identification_disagreements&quot; &quot;observed_on_string&quot; ## [17] &quot;observed_on&quot; &quot;time_observed_at&quot; ## [19] &quot;time_zone&quot; &quot;positional_accuracy&quot; ## [21] &quot;public_positional_accuracy&quot; &quot;geoprivacy&quot; ## [23] &quot;taxon_geoprivacy&quot; &quot;coordinates_obscured&quot; ## [25] &quot;positioning_method&quot; &quot;positioning_device&quot; ## [27] &quot;user_id&quot; &quot;user_name&quot; ## [29] &quot;created_at&quot; &quot;updated_at&quot; ## [31] &quot;quality_grade&quot; &quot;license&quot; ## [33] &quot;sound_url&quot; &quot;oauth_application_id&quot; ## [35] &quot;captive_cultivated&quot; &quot;geometry&quot; #Plot ggplot() + geom_sf(data=pc) Great! We got lots of points, but without a base layer its very difficult to tell where exactly these are? 7.9 Adding basemaps to plots There are lots of ways to make the basemap from data objects etc that we can plot our points over, but an easy way is to pull in tiles from Open Street Maps and plot our points on those. library(rosm) library(ggspatial) ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc) Note that there are quite a few base layer/tile options that can be set with type = \"\". Try rosm::osm.types() to see them all. This is better than nothing, but the scale of the map is too small to really see where the plants actually are. It would be much easier if we could look at the data interactively? 7.10 Interactive maps with leaflet and mapview We can generate interactive maps by calling the leaflet mapserver using wrapper functions in the leaflet R package written for this purpose. NOTE: If you can’t get leaflet to work it is probably a CRS problem. Your data need to be in Geographic or Web Mercator library(leaflet) library(htmltools) leaflet() %&gt;% # Add default OpenStreetMap map tiles addTiles(group = &quot;Default&quot;) %&gt;% # Add our points addCircleMarkers(data = pc, group = &quot;Protea cynaroides&quot;, radius = 3, color = &quot;green&quot;) Much better! Strange, but even though we filtered our iNaturalist records for captive_cultivated == \"false\" we still have a number of observations that appear to be in people’s gardens. Let this serve as a warning to be wary of all data! Always do “common-sense-checks” on your data and the outputs of your analyses!!! One way to do common sense checks with interactive plots is to add popup labels that allow you to inspect the data. See here to do this with library(leaflet), but here’s an example with library(mapview). library(mapview) library(leafpop) mapview(pc, popup = popupTable(pc, zcol = c(&quot;user_login&quot;, &quot;captive_cultivated&quot;, &quot;url&quot;))) Nice, but we can’t click on the URL, we have to copy and paste it. Fortunately, with a little html formatting we can make them live links. lpc &lt;- pc %&gt;% mutate(click_url = paste(&quot;&lt;b&gt;&lt;a href=&#39;&quot;, url, &quot;&#39;&gt;Link to iNat observation&lt;/a&gt;&lt;/b&gt;&quot;)) mapview(pc, popup = popupTable(lpc, zcol = c(&quot;user_login&quot;, &quot;captive_cultivated&quot;, &quot;click_url&quot;))) 7.11 Reprojecting One way to drastically reduce the number of cultivated records is to overlay the localities (points) with the remaining extent of the vegetation types (i.e. anything that is not in natural vegtation is likely to be cultivated). Let’s try that… #Get the remnants layer vegr &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous_Remnants.shp&quot;) ## Reading layer `Vegetation_Indigenous_Remnants&#39; from data source ## `/Users/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous_Remnants.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 3428 features and 7 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63951.23 ymin: -3803532 xmax: 420.7595 ymax: -3705506 ## Projected CRS: WGS_1984_Transverse_Mercator hmm &lt;- st_intersection(pc, vegr) ## Error in geos_op2_geom(&quot;intersection&quot;, x, y, ...): st_crs(x) == st_crs(y) is not TRUE Oops! The Coordinate Reference Systems are different! We will need to reproject one of the two datasets… Let’s see what CRS are currently set: st_crs(pc) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] So the points are Geographic, with no projected CRS CONVERSION. st_crs(vegr) ## Coordinate Reference System: ## User input: WGS_1984_Transverse_Mercator ## wkt: ## PROJCRS[&quot;WGS_1984_Transverse_Mercator&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;Hartebeesthoek94&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6148]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;unnamed&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,19, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,1, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] The remnants of vegetation types are in Transverse Mercator Lo19, just like the dataset of the historical extent of the veg types we were working with earlier. In this case, either CRS is fine for our purposes, but let’s stick with Transverse Mercator Lo19, because it’ll be useful later. For this we need to project the points like so: pc &lt;- st_transform(pc, st_crs(vegr)) Note that I fed it the CRS from vegr. This guarantees that they’ll be the same, even if we misidentified what the actual CRS is… 7.12 Intersecting points and polygons …and now we can try to intersect the points and polygons again… First lets see how many rows and columns the point data before the intersection: #call the dimensions of pc dim(pc) ## [1] 661 36 And after the intersection? pc &lt;- st_intersection(pc, vegr) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries dim(pc) ## [1] 648 43 Less rows, but more columns! Two things have happened: The attribute data from the polygons in vegr intersected by the points in pc have been added to the attribute table in pc! All points that do not intersect the polygons in vegr were dropped (i.e. those that were recorded outside the remaining extent of natural vegetation). Let’s have a look ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc) Yup, the localities in suburbia are gone… The map is a bit bland though. How about we use our “new information” about which vegetation types the observations occur in to colour or label the points on the map? 7.13 Colour or label points First, let’s add colour: library(wesanderson) pal &lt;- wes_palette(&quot;Darjeeling1&quot;, 7, type = &quot;continuous&quot;) ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc, aes(col = National_)) + scale_colour_manual(values = pal) Looks like almost all of them are in Peninsula Sandstone Fynbos… pc %&gt;% group_by(National_) %&gt;% summarise(n()) ## Simple feature collection with 7 features and 2 fields ## Geometry type: GEOMETRY ## Dimension: XY ## Bounding box: xmin: -62094.02 ymin: -3797540 xmax: -48638.23 ymax: -3755805 ## Projected CRS: WGS_1984_Transverse_Mercator ## # A tibble: 7 × 3 ## National_ `n()` geometry ## &lt;chr&gt; &lt;int&gt; &lt;GEOMETRY [m]&gt; ## 1 Cape Flats Sand Fynbos 2 MULTIPOINT ((-53325.87 -3769939), (-53306.33 -3769936)) ## 2 Hangklip Sand Fynbos 8 MULTIPOINT ((-52805.82 -3780199), (-52780.97 -3780196)… ## 3 Peninsula Granite Fynbos - North 1 POINT (-55987.76 -3757205) ## 4 Peninsula Granite Fynbos - South 2 MULTIPOINT ((-53290.55 -3763665), (-49759.88 -3775369)) ## 5 Peninsula Sandstone Fynbos 632 MULTIPOINT ((-62094.02 -3767667), (-62038.23 -3767597)… ## 6 Peninsula Shale Renosterveld 1 POINT (-55866.33 -3755805) ## 7 Southern Afrotemperate Forest 2 MULTIPOINT ((-53668.51 -3762128), (-53645.86 -3762155)) Yup! Note the numbers in column n(). But I can’t see where the Hangklip Sand Fynbos record is, so let’s label that one with text using geom_sf_label(). hsf &lt;- pc %&gt;% filter(National_ == &quot;Hangklip Sand Fynbos&quot;) #find the locality ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc, aes(col = National_)) + scale_colour_manual(values = pal) + geom_sf_label(data=hsf, aes(label = &quot;Here&quot;)) Aha! Note that you can specify that the label = setting points to a column in your dataset with names if you have lots of labels to add. 7.14 Buffering One issue here may be that all localities should be in Peninsula Sandstone Fynbos, but the vegetation type boundaries are wrong. After all, the transition or ecotone between two vegetation types is usually diffuse rather than a clear boundary, not to mention that the data may have been digitized at a very small scale, compromizing precision and accuracy. One way to check this is to buffer the points using st_buffer to see if they are within some distance (say 250m) of the boundary with Peninsula Sandstone Fynbos. #Find the localities that are not in Peninsula Sandstone Fynbos and add a 250m buffer npsf &lt;- pc %&gt;% filter(National_ != &quot;Peninsula Sandstone Fynbos&quot;) %&gt;% st_buffer(dist = 250) #NOTE that st_buffer() makes them polygons, because they now have area! npsf$geometry[1] #The first geometry in npsf ## Geometry set for 1 feature ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -53918.51 ymin: -3762378 xmax: -53418.51 ymax: -3761878 ## Projected CRS: WGS_1984_Transverse_Mercator ## POLYGON ((-53418.51 -3762128, -53418.86 -376214... #Get the number of unique iNaturalist record numbers length(unique(npsf$id)) ## [1] 16 #Intersect new polygons with veg remnants and filter for those that overlap Peninsula Sandstone Fynbos only npsf &lt;- st_intersection(npsf, vegr) %&gt;% filter(National_.1 == &quot;Peninsula Sandstone Fynbos&quot;) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries #Get the number of unique iNaturalist record numbers that overlap PSF length(unique(npsf$id)) ## [1] 11 So a fair proportion of the records are suspiciously close to Peninsula Sandstone Fynbos… 7.15 Within distance and intersect Perhaps a more interesting use of buffering is to see if a species’ locality is within a certain distance of a particular habitat etc. For example, we could ask if a species is associated with riparian zones by buffering either the localities (points) or rivers (lines) and then doing an intersection. But of course there are many ways to skin a cat, and it turns out buffering and intersecting may not be the most efficient here. If we don’t want to pull the attribute data from one dataset to the other we can just use st_intersects() to see if they overlap at all. We can even take it one step further, because sf has the function st_is_within_distance(), which is similar to applying st_buffer() and st_intersects() in one go. Here we’ll use Brabejum stellatifolium (a riparian tree in the Proteaceae) as our focal species and the watercourse layer from the City of Cape Town. #Get the watercourse data water &lt;- st_read(&quot;data/cape_peninsula/Open_Watercourses.geojson&quot;) ## Reading layer `Open_Watercourses&#39; from data source ## `/Users/jasper/GIT/spatial-r/data/cape_peninsula/Open_Watercourses.geojson&#39; ## using driver `GeoJSON&#39; ## Simple feature collection with 10848 features and 11 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: 18.31249 ymin: -34.28774 xmax: 18.99045 ymax: -33.47256 ## Geodetic CRS: WGS 84 #Check it&#39;s CRS st_crs(water) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] #Call the data directly from iNat bs &lt;- get_inat_obs(taxon_name = &quot;Brabejum stellatifolium&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #Filter returned observations by a range of attribute criteria bs &lt;- bs %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) #See how many records we got nrow(bs) ## [1] 306 #Make the dataframe a spatial object of class = &quot;sf&quot; bs &lt;- st_as_sf(bs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) #Note that I had to define the CRS (as Geographic WGS84)!!! Let’s see what we’ve got… #Crop the water courses to the extent of the locality data water &lt;- st_crop(water, bs) ## Warning: attribute variables are assumed to be spatially constant throughout all geometries #Plot ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data = water, colour = &quot;blue&quot;) + geom_sf(data=bs) Hard to tell, but they could all be on rivers? Let’s try st_intersects() without any buffering first to see if they overlap at all. st_intersects(bs, water) %&gt;% unlist() ## integer(0) Oops! We forgot to project our data! bs &lt;- st_transform(bs, st_crs(vegr)) water &lt;- st_transform(water, st_crs(vegr)) st_intersects(bs, water) %&gt;% unlist() ## integer(0) So none of them intersect, but this is not surprising, because lines and points in GIS do not have area, so they can’t really intersect unless you buffer them… Let’s try st_is_within_distance() and set it for 20 metres. Note that I add unlist() %&gt;% unique() so that it gives me a vector of the unique features (i.e. once each) that are within 20m, because the function returns a list and will return the same feature (line/river) multiple times - once for every point (tree) it is within 20m of. st_is_within_distance(bs, water, 20) %&gt;% unlist() %&gt;% unique() ## [1] 346 165 322 290 347 179 101 280 327 328 284 285 592 332 617 507 330 264 351 349 885 294 333 ## [24] 224 281 282 615 19 So it’s given us the list of lines (rivers) within 20m of our points, but that doesn’t tell us how many (or what proportion) of our points are within 20m of a river. Let’s apply the function again, swapping the layers around: st_is_within_distance(water, bs, 20) %&gt;% unlist() %&gt;% unique() ## [1] 293 47 278 294 304 5 23 214 235 244 145 91 219 252 14 229 230 48 54 70 97 113 117 ## [24] 127 141 165 172 173 199 200 271 272 83 86 151 161 164 183 218 221 286 106 144 270 254 1 ## [47] 3 18 22 41 42 44 50 77 78 137 138 139 152 159 194 215 216 245 160 217 146 212 126 ## [70] 95 107 211 213 225 282 So only 75 of the trees are within 20m of the rivers. What about 100m? st_is_within_distance(water, bs, 100) %&gt;% unlist() %&gt;% unique() ## [1] 72 73 98 142 37 233 293 47 278 294 304 5 6 33 63 155 167 188 257 285 23 214 ## [23] 235 244 149 25 156 17 19 20 21 24 27 29 35 36 38 51 59 62 67 85 90 91 ## [45] 99 104 105 123 145 163 197 219 220 236 238 239 240 248 252 256 265 269 32 295 247 273 ## [67] 296 26 14 237 266 267 268 96 229 230 259 196 177 277 222 8 9 15 34 40 48 52 ## [89] 53 54 55 57 58 69 70 80 83 84 89 97 108 113 114 115 117 118 119 120 121 122 ## [111] 127 133 134 141 158 164 165 168 170 171 172 173 178 183 186 199 200 206 207 208 209 210 ## [133] 227 232 242 253 260 262 263 264 271 272 286 288 299 12 31 86 102 140 150 151 161 175 ## [155] 176 185 187 201 202 203 204 205 218 221 234 116 75 106 107 112 144 270 254 3 11 109 ## [177] 110 111 1 2 18 22 41 42 43 44 50 77 78 82 88 92 103 131 137 138 139 152 ## [199] 159 160 194 215 216 245 255 300 217 146 154 212 297 126 39 94 95 93 125 128 129 132 ## [221] 211 213 225 282 231 225 It’s at this point that it’s worth thinking about the scale, precision and accuracy of both the species localities and the watercourse data before drawing any strong conclusions!!! "],["terra.html", "8 Raster GIS operations in R with terra 8.1 Reading in data 8.2 Cropping 8.3 Aggregating / Resampling 8.4 Basic plotting 8.5 Disaggregating 8.6 Raster maths! 8.7 Focal and terrain calculations 8.8 Raster stacks 8.9 Extracting raster to vector 8.10 Rasterizing 8.11 Visualizing multiple datasets on one map 8.12 Cloud Optimized GeoTiffs (COGs)!!! 8.13 Obtaining satellite data from APIs", " 8 Raster GIS operations in R with terra 8.1 Reading in data Ok, now to look at handling rasters. As with sf, the terra package has one function -rast()- that can read in just about any raster file format, which it assigns it’s own class SpatRaster. Let’s get started and read in the digital elevation model (DEM) for the City of Cape Town. library(terra) ## terra 1.7.65 ## ## Attaching package: &#39;terra&#39; ## The following object is masked from &#39;package:knitr&#39;: ## ## spin ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract dem &lt;- rast(&quot;data/cape_peninsula/CoCT_10m.tif&quot;) class(dem) ## [1] &quot;SpatRaster&quot; ## attr(,&quot;package&quot;) ## [1] &quot;terra&quot; dem #Typing the name of a &quot;SpatRaster&quot; class data object gives you the details ## class : SpatRaster ## dimensions : 9902, 6518, 1 (nrow, ncol, nlyr) ## resolution : 10, 10 (x, y) ## extent : -64180, 1000, -3804020, -3705000 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## source : CoCT_10m.tif ## name : 10m_BA ## min value : -35 ## max value : 1590 The coord. ref. field shows GCS_WGS_1984, which is Geographic Coordinates, but perhaps there is a projected CRS too? The extent appears to be in metres, with the eastings being a mix of positive and negative numbers, from which we can deduce that the coordinate reference system may be Transverse Mercator centred on Lo19, as for the other datasets we obtained from the City of Cape Town. Best to make sure! If you just want to know the CRS from a SpatRaster, you just call crs() like so: crs(dem) ## [1] &quot;PROJCRS[\\&quot;GCS_WGS_1984\\&quot;,\\n BASEGEOGCRS[\\&quot;WGS 84\\&quot;,\\n DATUM[\\&quot;World Geodetic System 1984\\&quot;,\\n ELLIPSOID[\\&quot;WGS 84\\&quot;,6378137,298.25722356049,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1]]],\\n PRIMEM[\\&quot;Greenwich\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n ID[\\&quot;EPSG\\&quot;,4326]],\\n CONVERSION[\\&quot;Transverse Mercator\\&quot;,\\n METHOD[\\&quot;Transverse Mercator\\&quot;,\\n ID[\\&quot;EPSG\\&quot;,9807]],\\n PARAMETER[\\&quot;Latitude of natural origin\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8801]],\\n PARAMETER[\\&quot;Longitude of natural origin\\&quot;,19,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8802]],\\n PARAMETER[\\&quot;Scale factor at natural origin\\&quot;,1,\\n SCALEUNIT[\\&quot;unity\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8805]],\\n PARAMETER[\\&quot;False easting\\&quot;,0,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8806]],\\n PARAMETER[\\&quot;False northing\\&quot;,0,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8807]]],\\n CS[Cartesian,2],\\n AXIS[\\&quot;easting\\&quot;,east,\\n ORDER[1],\\n LENGTHUNIT[\\&quot;metre\\&quot;,1,\\n ID[\\&quot;EPSG\\&quot;,9001]]],\\n AXIS[\\&quot;northing\\&quot;,north,\\n ORDER[2],\\n LENGTHUNIT[\\&quot;metre\\&quot;,1,\\n ID[\\&quot;EPSG\\&quot;,9001]]]]&quot; Messy, but somewhere in there it says “Longitude of natural origin 19” and “Transverse Mercator”… Similar to st_crs(), you can define a projection using the syntax: crs(your_raster) &lt;- \"your_crs\", where the new CRS can be in WKT, and EPSG code, or a PROJ string. For reprojecting, you use the function project(). We’ll look at it later. 8.2 Cropping Ok, before we try to anything with this dataset, let’s think about how big it is… One of the outputs of calling dem was the row reading dimensions : 9902, 6518, 1 (nrow, ncol, nlyr). Given that we are talking about 10m pixels, this information tells us that the extent of the region is roughly 100km by 65km and that there are ~65 million pixels! No wonder the original file was ~130MB (I reduced the one I shared with you slightly). While R can handle this, it does become slow when dealing with very large files. There are many ways to improve the efficiency of handling big rasters in R (see this slightly dated post for details if you’re interested), but for the purposes of this tutorial we’re going to take the easy option and just crop it to a smaller extent, like so: dem &lt;- crop(dem, ext(c(-66642.18, -44412.18, -3809853.29, -3750723.29))) Note that the crop() function requires us to pass it an object of class SpatExtent. Just like st_crop() from sf, crop() can derive the extent from another data object. One silly difference, is that if you pass it the coordinates of the extent manually (as above), you first need to pass it to the ext() function, and they need to follow the order xmin, xmax, ymin, ymax (as opposed to xmin, ymin, xmax, ymax as you do for st_crop()). Keep your eye out for these little differences, because they will trip you up… Ok, so how big is our dataset now? dem ## class : SpatRaster ## dimensions : 5330, 1977, 1 (nrow, ncol, nlyr) ## resolution : 10, 10 (x, y) ## extent : -64180, -44410, -3804020, -3750720 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## source(s) : memory ## varname : CoCT_10m ## name : 10m_BA ## min value : -15 ## max value : 1084 …still &gt;10 million pixels… 8.3 Aggregating / Resampling Do we need 10m data? If your analysis doesn’t need such fine resolution data, you can resample the raster to a larger pixel size, like 30m. The aggregate() function does this very efficiently, like so: dem30 &lt;- aggregate(dem, fact = 3, fun = mean) ## |---------|---------|---------|---------| ========================================= Here I’ve told it to aggregate by a factor of 3 (i.e. bin 9 neighbouring pixels (3x3) into one) and to assign the bigger pixel the mean of the 9 original pixels. This obviously results in some data loss, but that can be acceptable, depending on the purpose of your analysis. Note that you can pass just about any function to fun =, like min(), max() or even your own function. dem30 ## class : SpatRaster ## dimensions : 1777, 659, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## source(s) : memory ## name : 10m_BA ## min value : -15.000 ## max value : 1083.556 Ok, so we’ve reduced the size of the raster by a factor of 9 and only have a little over 1 million pixels to deal with. Much more reasonable! Now let’s have a look at what we’re dealing with. 8.4 Basic plotting Now that we’ve reduced the size of the dataset, we can try the base plotting function: plot(dem30) Or with the Tidyverse… Note that ggplot() doesn’t accept rasters, so we need to give it a dataframe with x and y columns for the coordinates, and a column containing the values to plot. This is easily done by coercing the raster into a dataframe, like so: #call tidyverse libraries and plot library(tidyverse) dem30 %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = `10m_BA`)) # Note that I had to know that the column name for the elevation data is &quot;10m_BA&quot;... and that you need to use ` ` around a variable name when feeding it to a function if it starts with a digit. Ok, how different does our 30m raster look to the 10m version? dem %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = `10m_BA`)) Not noticeably different at this scale! 8.5 Disaggregating One way to explore the degree of data loss is to disagg() our 30m DEM back to 10m and then compare it to the original. dem10 &lt;- disagg(dem30, fact = 3, method = &quot;bilinear&quot;) Note that I’ve tried to use bilinear interpolation to give it a fair chance of getting nearer the original values. You can google this on your own, but it essentially smooths the data by averaging across neighbouring pixels. Now, how can I compare my two 10m rasters? 8.6 Raster maths! The raster and terra packages make this easy, because you can do maths with rasters, treating them as variables in an equation. This means we can explore the data loss by calculating the difference between the original and disaggregated DEMS. Note that when aggregating you often lose some of the cells along the edges, and that you can’t do raster maths on rasters with different extents… We can fix this by cropping the larger raster with the smaller first. dem10 &lt;- crop(dem10, dem) diff &lt;- dem - dem10 #maths with rasters! And plot the result! diff %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = `10m_BA`)) If you look really closely, you’ll see the outline of the cliffs of Table Mountain, where you’d expect the data loss to be worst. The colour ramp tells us that the worst distortion was up to 100m, or about 10% of the elevation range in this dataset, but don’t be fooled by the extremes! Let’s have a look at all the values as a histogram. diff %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_histogram(aes(`10m_BA`)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Looks like most values are within 10 or so metres of their original values, so the data loss really wasn’t that bad! 8.7 Focal and terrain calculations In addition to maths with multiple rasters, you can do all kinds of calculations within a raster using focal(). This essentially applies a moving window, calculating values for a neighbourhood of cells as it goes, using whatever function you supply (mean, max, your own, etc). The function terrain() is a special case of focal(), optimized for calculating slope, aspect, topographic position index (TPI), topographic roughness index (TRI), roughness, or flow direction. Here I’ll calculate the slope and aspect so that we can pass them to the function shade() to make a pretty hillshade layer. aspect &lt;- terrain(dem30, &quot;aspect&quot;, unit = &quot;radians&quot;) slope &lt;- terrain(dem30, &quot;slope&quot;, unit = &quot;radians&quot;) hillshade &lt;- shade(slope, aspect) plot(hillshade) Probably prettier with Tidyverse: hillshade %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = hillshade)) + #note that the hillshade column name in this case is &quot;hillshade&quot; scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) Nice ne? 8.8 Raster stacks Another nice thing about rasters is that if you have multiple rasters “on the same grid” (i.e. with the same pixel size, extent and CRS) then you can stack them and work with them as a single object. library(raster) users will be familiar with stack(), but in terra you just use the base function c(), like so: dstack &lt;- c(dem30, slope, aspect, hillshade) dstack ## class : SpatRaster ## dimensions : 1777, 659, 4 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## source(s) : memory ## names : 10m_BA, slope, aspect, hillshade ## min values : -15.000, 0.000000, 0.000000, -0.4906481 ## max values : 1083.556, 1.370826, 6.283185, 0.9999974 As you can see the “dimensions” now report 4 layers, and there are 4 names. Some of the names don’t look all that informative though, so let’s rename them. names(dstack) &lt;- c(&quot;elevation&quot;, &quot;slope&quot;, &quot;aspect&quot;, &quot;shade&quot;) 8.9 Extracting raster to vector Ok, enough fooling around. More often than not, we just want to extract data from rasters for further analyses (e.g. climate layers, etc), so let’s cover that base here. Extract to points First, let’s get some points for two species in the Proteaceae, Protea cynaroides and Leucadendron laureolum… library(rinat) library(sf) #Call data for two species directly from iNat pc &lt;- get_inat_obs(taxon_name = &quot;Protea cynaroides&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) ll &lt;- get_inat_obs(taxon_name = &quot;Leucadendron laureolum&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #Combine the records into one dataframe pc &lt;- rbind(pc,ll) #Filter returned observations by a range of attribute criteria pc &lt;- pc %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) #Make the dataframe a spatial object of class = &quot;sf&quot; pc &lt;- st_as_sf(pc, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) #Set to the same projection as the elevation data pc &lt;- st_transform(pc, crs(dem30)) Now let’s extract the data to the points. NOTE!!! terra doesn’t play nicely with sf objects at this stage, so you need to coerce them into terra’s own vector format using vect(). dat &lt;- extract(dem30, vect(pc)) # note vect() head(dat) ## ID 10m_BA ## 1 1 1024.5556 ## 2 2 1052.1111 ## 3 3 528.6667 ## 4 4 530.8889 ## 5 5 1014.4444 ## 6 6 1062.4444 Nice, but not all that handy on it’s own. Let’s add the elevation column to our points layer, so we can match it with the species names and plot. pc$dem &lt;- dat$`10m_BA` pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem)) A clear separation in the preferred elevation range between the two species. Ok, that’s handy, but what if we have data lots of rasters? We don’t want to have to do that for every raster! This is where raster stacks come into their own! #extract from stack dat &lt;- extract(dstack, vect(pc)) #bind columns to points to match the names edat &lt;- cbind(as.data.frame(pc), dat) #select columns we want and tidy data into long format edat &lt;- edat %&gt;% dplyr::select(scientific_name, elevation, slope, aspect, shade) %&gt;% pivot_longer(c(elevation, slope, aspect, shade)) #panel boxplot of the variables extracted edat %&gt;% ggplot() + geom_boxplot(aes(scientific_name, value)) + facet_wrap(~name, scales = &quot;free&quot;) Something I should have mentioned is that if you would like each point to sample a larger region you can add a buffer = argument to the extract() function, and a function (fun =) to summarize the neighbourhood of pixels sampled, like so: pc$dem30 &lt;- extract(dem30, vect(pc), buffer = 200, fun = mean)$`10m_BA` #Note the sneaky use of $ to access the column I want pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem30)) Extract to polygons Now let’s try that with our vegetation polygons. #Get historical vegetation layer veg &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous.shp&quot;) ## Reading layer `Vegetation_Indigenous&#39; from data source ## `/Users/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1325 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63972.95 ymin: -3803535 xmax: 430.8125 ymax: -3705149 ## Projected CRS: WGS_1984_Transverse_Mercator #Crop to same extent as DEM veg &lt;- st_crop(veg, ext(dem30)) #Note that I just fed it the extent of the DEM ## Warning: attribute variables are assumed to be spatially constant throughout all geometries #Best to dissolve polygons first - otherwise you get repeat outputs for each polygon within each veg type vegsum &lt;- veg %&gt;% group_by(National_) %&gt;% summarize() #Do extraction - note the summary function vegdem &lt;- extract(dem30, vect(vegsum), fun = mean, na.rm = T) ## Warning: [extract] transforming vector data to the CRS of the raster #Combine the names and vector extracted means into a dataframe vegdem &lt;- cbind(vegdem, vegsum$National_) #Rename the columns to something meaningful names(vegdem) &lt;- c(&quot;ID&quot;, &quot;Mean elevation (m)&quot;, &quot;Vegetation type&quot;) #Plot vegdem %&gt;% ggplot() + geom_col(aes(y = `Mean elevation (m)`, x = `Vegetation type`)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) Ok, I did a lot of things there…, but you get it right? Note that I applied a function to the extract() to summarize the output, because each polygon usually returns multiple raster cell values. You can choose (or code up) your own function. Here’s a different approach… 8.10 Rasterizing Rasterizing essentially means turning a vector layer into a raster. To rasterize, you need an existing raster grid to rasterize to, like dem30 in this case. #Make the vegetation type a factor vegsum$National_ &lt;- as.factor(vegsum$National_) #Rasterize vegras &lt;- rasterize(vect(vegsum), dem30, field = &quot;National_&quot;) #Plot vegras %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = National_)) I’m sure this plot is a surprise to those who worked with raster. Usually rasters want to work with numbers. terra can work with (and rasterize) data of class “factor”, opening up all kinds of opportunities. 8.11 Visualizing multiple datasets on one map What about if we want to plot multiple datasets on one map? This is easy, if you can feed each dataset into a separate ggplot function. Here’s the veg types with contours and the iNaturalist records we retrieved earlier. ggplot() + geom_raster(data = as.data.frame(vegras, xy = TRUE), aes(x = x, y = y, fill = National_)) + geom_contour(data = as.data.frame(dem30, xy = TRUE), aes(x = x, y = y, z = `10m_BA`), breaks = seq(0, 1100, 100), colour = &quot;black&quot;) + geom_sf(data=pc, colour = &quot;white&quot;, size = 0.5) For more inspiration on mapping with R, check out https://slingsby-maps.myshopify.com/. I’ve been generating the majority of the basemap (terrain colour, hillshade, contours, streams, etc) for these in R for the past few years. 8.12 Cloud Optimized GeoTiffs (COGs)!!! I thought I’d add this as a bonus section, reinforcing the value of standardized open metadata and file formats from the Data Management module. First, let’s open a connection to our COG, which is stored in the cloud. To do this, we need to pass a URL to the file’s online location to terra. cog.url &lt;- &quot;/vsicurl/https://mnemosyne.somisana.ac.za/osgeo/saeon_rgb/grootbos.tif&quot; grootbos &lt;- rast(cog.url) grootbos ## class : SpatRaster ## dimensions : 100024, 121627, 3 (nrow, ncol, nlyr) ## resolution : 0.08, 0.08 (x, y) ## extent : 35640.41, 45370.57, -3828176, -3820175 (xmin, xmax, ymin, ymax) ## coord. ref. : LO19 ## source : grootbos.tif ## colors RGB : 1, 2, 3 ## names : grootbos_1, grootbos_2, grootbos_3 This has given us the metadata about the file, but has not read it into R’s memory. The file is ~1.8GB so it would do bad things if we tried to read the whole thing in… Now let’s retrieve a subset of the file. To do this we need to make a vector polygon for our region of interest (ROI), like so: roi &lt;- vect(data.frame(lon = c(19.433975, 19.436451), lat = c(-34.522733, -34.520735)), crs = &quot;epsg:4326&quot;) And transform it to the same projection as the COG: roi &lt;- terra::project(roi, crs(grootbos)) And then extract our ROI roi_ras &lt;- crop(grootbos, roi) roi_ras ## class : SpatRaster ## dimensions : 2758, 2853, 3 (nrow, ncol, nlyr) ## resolution : 0.08, 0.08 (x, y) ## extent : 39845.61, 40073.85, -3821732, -3821512 (xmin, xmax, ymin, ymax) ## coord. ref. : LO19 ## source(s) : memory ## colors RGB : 1, 2, 3 ## varname : grootbos ## names : grootbos_1, grootbos_2, grootbos_3 ## min values : 28, 47, 56 ## max values : 255, 255, 255 Now we have a raster with 3 layers in memory. There are Red Green and Blue, so we should be able to plot them, like so: plotRGB(roi_ras) This somewhat arbitrary looking site is where we did some fieldwork in the Grootbos Private Nature Reserve with the 2022 class… 8.13 Obtaining satellite data from APIs There are also R packages like MODISTools that allow you to query the online databases. MODISTools interfaces with the ‘MODIS Land Products Subsets’ Web Services to download various products. In this case we’ll be downloading the “MOD13Q1” product, which is the Vegetation Indices product for the Terra satellite, generated every 16 days at 250 meter (m) spatial resolution. The algorithm chooses the best available pixel value from all (daily) the acquisitions from the 16 day period, minimizing clouds, low view angle, and selecting the highest NDVI/EVI value. WARNING! This code can take a while to run! Hence, I have wrapped it in an if() statement that tells the code not to run if the file already exists. if(!file.exists(&quot;data/MODISdat_batch_30Jan2023.csv&quot;)) # if the file does not exist, then run... otherwise do nothing... { library(MODISTools) sites &lt;- data.frame(site_name = c(&quot;grassy field&quot;, &quot;invasion&quot;, &quot;renosterveld&quot;, &quot;sand&quot;, &quot;sandstone&quot;, &quot;limestone&quot;), lat = c(-34.375052, -34.386014, -34.374259, -34.3961, -34.3748, -34.4309), lon = c(20.531749, 20.534986, 20.504233, 20.5494, 20.5428, 20.5666)) ### Here&#39;s some code if you want to use an existing layer of points instead of entering them manually # sites &lt;- st_read(&quot;/home/jasper/GIT/BIO3018F/prac/Potberg_prac_sites.kml&quot;) # sites &lt;- data.frame(site_name = sites$Name, lat = st_coordinates(sites)[,2], lon = st_coordinates(sites)[,1]) dat &lt;- mt_batch_subset(df = sites, product = &quot;MOD13Q1&quot;, band = &quot;250m_16_days_NDVI&quot;, internal = TRUE, start = &quot;2000-01-01&quot;, end = &quot;2023-01-30&quot;) write_csv(dat, &quot;data/MODISdat_batch_30Jan2023.csv&quot;) } Plot all time series read_csv(&quot;data/MODISdat_batch_30Jan2023.csv&quot;) %&gt;% ggplot(aes(x = calendar_date, y = value*0.0001)) + geom_line() + # geom_point() + facet_wrap(.~ site) + ylab(&quot;NDVI&quot;) + ylim(0.2, 0.9) There are many more complex spatial and remote sensing analyses you can do by interaction with the cloud from R. Here are some links to a few: Cloud-based processing of satellite image collections with rstac and gdalcubes Run Google Earth Engine from R with rgee Obtain spatial data from ESRI REST APIs in R or see package arcpullr There are many more!!! "],["old-raster-gis-operations-in-r-with-raster.html", "9 (Old!) Raster GIS operations in R with raster 9.1 Reading in data 9.2 Cropping 9.3 Aggregating / Resampling 9.4 Basic plotting 9.5 Disaggregating 9.6 Raster maths! 9.7 Focal and terrain calculations 9.8 Raster stacks 9.9 Extracting raster to vector 9.10 Rasterizing", " 9 (Old!) Raster GIS operations in R with raster WARNING!!! READ THIS!!! This section focuses on library(raster) which is being phased out and will not be maintained in the long term. This is because it relies on other packages that are no longer maintained, because their creators have retired. I used raster in the first iteration of these notes so I’ve kept this section for posterity and for those who rely on it, but will remove it in the next year or so. I strongly recommend you ignore this section and move on to the next, which focuses on the new replacement library(terra)! I have kept to the same set of examples in that section, and many of the function names are the same or similar, so shifting to terra should be easy. 9.1 Reading in data Ok, now to look at handling rasters. As with sf, the raster package has one function -raster()- that can read in just about any raster file format. Let’s get started and read in the digital elevation model (DEM) for the City of Cape Town. library(raster) dem &lt;- raster(&quot;data/cape_peninsula/CoCT_10m.tif&quot;) class(dem) ## [1] &quot;RasterLayer&quot; ## attr(,&quot;package&quot;) ## [1] &quot;raster&quot; dem #Typing the name of a &quot;raster&quot; class data object gives you the details ## class : RasterLayer ## dimensions : 9902, 6518, 64541236 (nrow, ncol, ncell) ## resolution : 10, 10 (x, y) ## extent : -64180, 1000, -3804020, -3705000 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## source : CoCT_10m.tif ## names : X10m_BA ## values : -35, 1590 (min, max) The crs field shows a proj4string, from which we can deduce that the coordinate reference system is Transverse Mercator Lo19. If you just want to know the CRS from a raster, you just call the proj4string like so: proj4string(dem) ## [1] &quot;+proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs&quot; Similar to st_crs(), you can define a projection using the syntax: proj4string(your_raster) &lt;- \"your_proj4string\". For reprojecting, you use the function projectRaster(). We’ll look at it later. 9.2 Cropping Ok, before we try to anything with this dataset, let’s think about how big it is… One of the outputs of calling dem was the row reading dimensions : 9902, 6518, 64541236 (nrow, ncol, ncell). Given that we are talking about 10m pixels, this information tells us that the extent of the region is roughly 100km by 65km and that there are ~65 million pixels! No wonder the file is ~130MB. While R can handle this, it will be slow! There are many ways to improve the efficiency of handling big rasters in R (see this post for details if you’re interested), but for the purposes of this tutorial we’re going to take the easy option and just crop it to a smaller extent, like so: dem &lt;- crop(dem, extent(c(-66642.18, -44412.18, -3809853.29, -3750723.29))) Note that the crop() function requires us to pass it an object of class extent. Just like st_crop() from sf, crop() can derive the extent from another data object. One silly difference, is that if you pass it the coordinates of the extent manually (as above), you first need to pass it to the extent() function, and they need to follow the order xmin, xmax, ymin, ymax (as opposed to xmin, ymin, xmax, ymax as you do for st_crop()). Keep your eye out for these little differences, because they will trip you up… Ok, so how big is our dataset now? dem ## class : RasterLayer ## dimensions : 5330, 1977, 10537410 (nrow, ncol, ncell) ## resolution : 10, 10 (x, y) ## extent : -64180, -44410, -3804020, -3750720 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## source : memory ## names : X10m_BA ## values : -15, 1084 (min, max) …still &gt;10 million pixels… 9.3 Aggregating / Resampling Do we need 10m data? If your analysis doesn’t need such fine resolution data, you can resample the raster to a larger pixel size, like 30m. The aggregate() function in the raster package does this very efficiently, like so: dem30 &lt;- aggregate(dem, fact = 3, fun = mean) Here I’ve told it to aggregate by a factor of 3 (i.e. bin 9 neighbouring pixels (3x3) into one) and to assign the bigger pixel the mean of the 9 original pixels. This obviously results in some data loss, but that can be acceptable, depending on the purpose of your analysis. Note that you can pass just about any function to fun =, like min(), max() or even your own function. dem30 ## class : RasterLayer ## dimensions : 1777, 659, 1171043 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## source : memory ## names : X10m_BA ## values : -15, 1083.556 (min, max) Ok, so we’ve reduced the size of the raster by a factor of 9 and only have a little over 1 million pixels to deal with. Much more reasonable! Now let’s have a look at what we’re dealing with. 9.4 Basic plotting Now that we’ve reduced the size of the dataset, we can try the base plotting function: plot(dem30) Or with the Tidyverse… Note that ggplot() doesn’t accept rasters, so we need to give it a dataframe with x and y columns for the coordinates, and a column containing the values to plot. This is easily done, firstly by converting the raster to a vector layer of points, and then by coorcing that into a dataframe, like so: #convert to points dem30df &lt;- rasterToPoints(dem30, spatial = TRUE) class(dem30df) #note that this is a class from library(sp) ## [1] &quot;SpatialPointsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; #coerce to a dataframe dem30df &lt;- data.frame(dem30df) #have a look at the column names names(dem30df) # &quot;X10m_BA&quot; is the elevation data... ## [1] &quot;X10m_BA&quot; &quot;x&quot; &quot;y&quot; &quot;optional&quot; #call tidyverse libraries and plot library(tidyverse) dem30df %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = X10m_BA)) Ok, how different does our 30m raster look to the 10m version? demdf &lt;- data.frame(rasterToPoints(dem, spatial = TRUE)) demdf %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = X10m_BA)) Not noticeably different at this scale! 9.5 Disaggregating One way to explore the degree of data loss is to disaggregate() our 30m DEM back to 10m and then compare it to the original. dem10 &lt;- disaggregate(dem30, fact = 3, method = &quot;bilinear&quot;) Note that I’ve tried to use bilinear interpolation to give it a fair chance of getting nearer the original values. You can google this on your own, but it essentially smooths the data by averaging across neighbouring pixels. Now, how can I compare my two 10m rasters? 9.6 Raster maths! The raster package makes this easy, because you can do maths with rasters, treating them as variables in an equation. This means we can explore the data loss by calculating the difference between the original and disaggregated DEMS, like so. diff &lt;- dem - dem10 ## Warning in dem - dem10: Raster objects have different extents. Result for their intersection is ## returned Note that the error is because we lost some of the 10m cells along the edges when we aggregated… We can ignore this in this example. diffdf &lt;- data.frame(rasterToPoints(diff, spatial = TRUE)) names(diffdf) #note that the elevation column is now calles &quot;layer&quot;, which we need to feed to ggplot() ## [1] &quot;layer&quot; &quot;x&quot; &quot;y&quot; &quot;optional&quot; diffdf %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = layer)) If you look really closely, you’ll see the outline of the cliffs of Table Mountain, where you’d expect the data loss to be worst. The colour ramp tells us that the worst distortion was up to 100m, or about 10% of the elevation range in this dataset, but don’t be fooled by the extremes! Let’s have a look at all the values as a histogram. diffdf %&gt;% ggplot() + geom_histogram(aes(layer)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Looks like most values are within 10 or so metres of their original values, so the data loss really wasn’t that bad! 9.7 Focal and terrain calculations In addition to maths with multiple rasters, you can do all kinds of calculations within a raster using focal(). This essentially applies a moving window, calculating values for a neighbourhood of cells as it goes, using whatever function you supply (mean, max, your own, etc). The function terrain() is a special case of focal(), optimized for calculating slope, aspect, topographic position index (TPI), topographic roughness index (TRI), roughness, or flow direction. Here I’ll calculate the slope and aspect so that we can pass them to the function hillShade() to make a pretty hillshade layer. aspect &lt;- terrain(dem30, &quot;aspect&quot;) slope &lt;- terrain(dem30, &quot;slope&quot;) hillshade &lt;- hillShade(slope, aspect) plot(hillshade) Probably prettier with Tidyverse: hsdf &lt;- data.frame(rasterToPoints(hillshade, spatial = TRUE)) names(hsdf) ## [1] &quot;layer&quot; &quot;x&quot; &quot;y&quot; &quot;optional&quot; hsdf %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = layer)) + scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) Nice ne? 9.8 Raster stacks Another nice thing about rasters is that if you have multiple rasters “on the same grid” (i.e. with the same pixel size, extent and CRS) then you can stack them and work with them as a single rasterstack object. dstack &lt;- stack(dem30, slope, aspect, hillshade) dstack ## class : RasterStack ## dimensions : 1777, 659, 1171043, 4 (nrow, ncol, ncell, nlayers) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## names : X10m_BA, slope, aspect, layer ## min values : -15.0000000, 0.0000000, 0.0000000, -0.4906481 ## max values : 1083.5555556, 1.3708258, 6.2831853, 0.9999974 As you can see the “dimensions” now report 4 layers, and there are 4 names. Some of the names don’t look all that informative though, so let’s rename them. names(dstack) &lt;- c(&quot;elevation&quot;, &quot;slope&quot;, &quot;aspect&quot;, &quot;shade&quot;) 9.9 Extracting raster to vector Ok, enough fooling around. More often than not, we just want to extract data from rasters for further analyses (e.g. climate layers, etc), so let’s cover that base here. Extract to points First, let’s get some points for two species in the Proteaceae, Protea cynaroides and Leucadendron laureolum… library(rinat) library(sf) #Call data for two species directly from iNat pc &lt;- get_inat_obs(taxon_name = &quot;Protea cynaroides&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) ll &lt;- get_inat_obs(taxon_name = &quot;Leucadendron laureolum&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #Combine the records into one dataframe pc &lt;- rbind(pc,ll) #Filter returned observations by a range of attribute criteria pc &lt;- pc %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) #Make the dataframe a spatial object of class = &quot;sf&quot; pc &lt;- st_as_sf(pc, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) Now let’s extract the data to the points. dat &lt;- extract(dem30, pc) ## Warning in .local(x, y, ...): Transforming SpatialPoints to the crs of the Raster Note how raster conveniently handled my error in not making sure the CRS are the same for both layers! In general, you should avoid relying on automated fixes like this… head(dat) ## [1] 1024.5556 1052.1111 528.6667 530.8889 1014.4444 1062.4444 Hmm… ok… It just returned the vector of numbers… Not all that handy on it’s own. Let’s add it to out points layer, so we can match it with the species names and plot. pc$dem30 &lt;- dat pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem30)) (Hmm… do you think those Leucadendron laureolum outliers that must be near Maclear’s Beacon could actually be Leucadendron strobilinum?) Ok, that’s handy, but what if we have data lots of rasters? We don’t want to have to do that for every raster! This is where rasterstacks come into their own! #extract from stack dat &lt;- extract(dstack, pc) ## Warning in .local(x, y, ...): Transforming SpatialPoints to the crs of the Raster #bind columns to points to match thenames edat &lt;- cbind(as.data.frame(pc), dat) #select columns we want and tidy data into long format edat &lt;- edat %&gt;% as_tibble() %&gt;% dplyr::select(scientific_name, elevation, slope, aspect, shade) %&gt;% pivot_longer(c(elevation, slope, aspect, shade)) #panel boxplot of the variables extracted edat %&gt;% ggplot() + geom_boxplot(aes(scientific_name, value)) + facet_wrap(~name, scales = &quot;free&quot;) Something I should have mentioned is that if you would like each point to sample a larger region you can add a buffer = argument to the extract() function, and a function (fun =) to summarize the neighbourhood of pixels sampled, like so: pc$dem30 &lt;- extract(dem30, pc, buffer = 200, fun = mean) ## Warning in .local(x, y, ...): Transforming SpatialPoints to the crs of the Raster pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem30)) Extract to polygons Now let’s try that with our vegetation polygons. #Get historical vegetation layer veg &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous.shp&quot;) ## Reading layer `Vegetation_Indigenous&#39; from data source ## `/Users/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1325 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63972.95 ymin: -3803535 xmax: 430.8125 ymax: -3705149 ## Projected CRS: WGS_1984_Transverse_Mercator #Crop to same extent as DEM veg &lt;- st_crop(veg, extent(dem30)) #Note that I just fed it the extent of the DEM ## Warning: attribute variables are assumed to be spatially constant throughout all geometries #Best to dissolve polygons first - otherwise you get repeat outputs for each polygone within each veg type vegsum &lt;- veg %&gt;% group_by(National_) %&gt;% summarize() #Do extraction - gives a vector - note the summary function vegdem &lt;- extract(dem30, vegsum, fun = mean, na.rm = T) #Combine the names and vector extracted means into a dataframe vegdem &lt;- data.frame(veg = vegsum$National_, elevation = vegdem) #Plot vegdem %&gt;% ggplot() + geom_col(aes(y = elevation, x = veg)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) ###Add example where all cells are extracted and summarized afterwards... #hmm &lt;- do.call(rbind,lapply(vegdem,data.frame))# bind_rows(vegdem) #hmm &lt;- data.frame(veg = rownames(hmm), elevation = hmm) Ok, I did a lot of things there…, but you get it right? Note that I applied a function to the extract() to summarize the output, because each polygon usualy returns multiple raster cell values. You can choose (or code up) your own function. Here’s a different approach… 9.10 Rasterizing Rasterizing essentially means turning a vector layer into a raster. To rasterize, you need an existing raster grid to rasterize to, like dem30 in this case. #Make the vegetation type name a factor* vegsum$National_ &lt;- as.factor(vegsum$National_) #Rasterize vegras &lt;- rasterize(vegsum, dem30, field = &quot;National_&quot;) #Plot vrs &lt;- data.frame(rasterToPoints(vegras, spatial = TRUE)) vrs %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = layer)) + scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) Note that rasters want numbers. This is why I had to convert the “National_” column from class “character” to “factor”. You can think of a factor in R as essentially a vector of numbers with an associated translation table that links the numbers to the names. Here’s the names: levels(vegsum$National_) ## [1] &quot;Beach - FalseBay&quot; &quot;Cape Estuarine Salt Marshes&quot; ## [3] &quot;Cape Flats Dune Strandveld - False Bay&quot; &quot;Cape Flats Dune Strandveld - West Coast&quot; ## [5] &quot;Cape Flats Sand Fynbos&quot; &quot;Cape Lowland Freshwater Wetlands&quot; ## [7] &quot;Hangklip Sand Fynbos&quot; &quot;Peninsula Granite Fynbos - North&quot; ## [9] &quot;Peninsula Granite Fynbos - South&quot; &quot;Peninsula Sandstone Fynbos&quot; ## [11] &quot;Peninsula Shale Fynbos&quot; &quot;Peninsula Shale Renosterveld&quot; ## [13] &quot;RECLAIMED&quot; &quot;Southern Afrotemperate Forest&quot; 9.10.0.0.0.0.0.0.0.0.0.0.1 We could even add contours… ggplot() + geom_raster(data = hsdf, aes(x = x, y = y, fill = layer, alpha = 0.5)) + scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) + geom_contour(data = dem30df, aes(x = x, y = y, z = X10m_BA), breaks = seq(0, 1100, 100), colour = &quot;black&quot;) + theme(legend.position = &quot;none&quot;) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
