[["index.html", "A Minimal Introduction to GIS (in R) 1 Overview 1.1 General 1.2 Using this resource 1.3 Module and Project details", " A Minimal Introduction to GIS (in R) Jasper Slingsby 2022-04-14 1 Overview This is a minimal introduction to GIS and handling spatial data in R compiled for the Biological Sciences BSc(Honours) class at the University of Cape Town. 1.1 General The goal is to give you a very brief introduction to Geographic Information Systems (GIS) in general and some familiarity with handling spatial data in R. GIS is a field of research that many people dedicate their entire lives to, yet we only have a week, so this really is a minimalist introduction. I’ll focus on giving you a broad overview and some idea of how to teach yourself (using R). The core outcomes I hope you’ll come away with: Some familiarity with GIS and what it can help you achieve Some familiarity with GIS jargon and technical terms Highlight some of the common problems and pitfalls when using GIS Some familiarity with handling spatial data in R Some hints and resources to help you teach yourself R Some idea of how to help yourself or find help when you inevitably come unstuck… These course notes borrow or paraphrase extensively from Adam Wilson’s GEO 511 Spatial Data Science course, Manny Gimond’s Intro to GIS &amp; Spatial Analysis and the 2020 series of GIS Lecture Lunches by Thomas Slingsby and Nicholas Lindenberg from UCT Library’s GIS Support Unit. Other very valuable resources include: Lovelace et al’s online book Geocomputation with R Ryan Garnett’s cheatsheet for library(sf) All code, images, etc can be found here. I have only used images etc that were made available online under a non-restrictive license (Creative Commons, etc) and have attributed my sources. Content without attribution is my own and shared under the license below. If there is any content you find concerning with regard to licensing, or that you find offensive, please contact me. Any feedback, positive or negative, is welcome! This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. 1.2 Using this resource I’ve included quite a bit of demonstration R code in Chapter 7. To tun the code you will need: to have working R and RStudio installations - this tutorial may help if needed to install the following R packages (you can copy and run the code): required: install.packages(c(\"tidyverse\", \"sp\", \"rgdal\", \"raster\", \"sf\", \"lwgeom\", \"terra\", \"stars\", \"exactextractr\")) optional, but handy for some visualizations: install.packages(c(\"cowplot\", \"hrbrthemes\", \"knitr\", \"leaflet\", \"htmltools\", \"rosm\", \"ggspatial\", \"rnaturalearth\", \"mapview\", \"tmap\")) to download the datasets discussed in section 7.1 1.3 Module and Project details Lectures Lectures will be held in the mornings between 10:00 to 12:00 Monday to Thursday in BIOLT1. Thursday will be presentation day. Projects Afternoons are self-study time where you will incrementally develop your own individual GIS project in R and RMarkdown. The project will count 70% of your mark for the module and will be due on Monday the 25th April. You will need to submit a .Rmd file and stitched HTML notebook. You’re welcome to do this in a Git Repository, now that we’ve completed the Data Management module. The focus of the assignment is essentially setting up a GIS workflow and will be assessed on whether you’ve absorbed the content of the lectures. The topic and datasets used will be up to you. Pro tip: Use this as an opportunity to get a kick start on your Honours projects. If your Honours project doesn’t require GIS (which is unlikely) then either help a buddy or let your curiosity roam wild! This is a teaching exercise, so it doesn’t even have to be based on biological data, but it would help you to explore some of the data sources suggested below. The project objectives, broken down as daily goals to help you pace yourselves: Day 1: Define a question that requires GIS, the kinds of data you’ll need to address the question, and describe in words what you think you would need to do with the data to get there. Find and describe the datasets you’ll need for your analysis (type, source, how created, etc). Day 2: Describe the GIS workflow you think you’ll need to perform your analysis (in words and/or a figure). Reconsider and refine your datasets. Day 3: Translate your workflow into the R functions you think you’ll need to use and begin coding and running the GIS workflow in R. Day 4: Keep working on the code. Develop and present your 1 slide lightning talk - examples from last year here. Until the 25th April: Iterate over the previous steps until done! I will be available 2-3PM on Tuesday and Wednesday afternoons as a “help desk” to assist you refine your projects and help troubleshoot issues. Lightning talks: you’ll do online lightning presentations (30% of your mark for the module) on your GIS projects (1 slide, 2 minutes presentation, 1 minute questions) on Thursday afternoon (2PM, 14th April). Don’t worry, you won’t lose marks if your projects are not yet complete! We just want to know what you’re doing your project on, what datasets you’re using, and what you plan to do with them. Please add your slide to the Google slide deck and read the instructions there. Some sources of local data to help you get started. Feel free to look for others! If you find good ones, let me know and I’ll add them. SANBI’s “Biodiversity GIS” - https://bgis.sanbi.org/ SANBI’s Botanical Database of South Africa (BODATSA) - http://newposa.sanbi.org/ SAEON’s Data Catalogue - http://catalogue.saeon.ac.za/ City of Cape Town’s Open Data Portal - https://odp.capetown.gov.za/ iNaturalist - https://www.inaturalist.org/ (accessible from R - see section 7.7) Google Datasets Search - https://datasetsearch.research.google.com/ Make sure to check the data use policies and make sure you have permission use the datasets!!! "],["intro.html", "2 Why care about GIS?", " 2 Why care about GIS? How lost would you be without Google Maps? Figure 2.1: Screenshot of Google Maps for Cape Town. If we search “Geographic Information Systems OR GIS” in Web of Science, it is clear that the number of papers using GIS has exploded over time! Figure 2.2: The number of papers on Web of Science when searching Geographic Information Systems OR GIS. GIS is especially important in environment and life sciences! Figure 2.3: The number of papers on Web of Science by theme when searching Geographic Information Systems OR GIS. We use it for things like mapping ecosystems and biomes. Figure 2.4: The historical extent of the biomes of South Africa from Mucina, Rutherford, and Others (2006). Or the loss of ecosystems and biomes. Figure 2.5: The remaining extent of the biomes of South Africa from Skowno, Jewitt, and Slingsby (2021). Although the outcomes of GIS analyses are not always maps, e.g. this table from Skowno, Jewitt, and Slingsby (2021). But it can also be interactive! Figure 2.6: The Global Forest Watch app. References "],["gis-basics.html", "3 GIS basics 3.1 What is GIS? 3.2 How do we do GIS? 3.3 How to get help?", " 3 GIS basics Here we cover the basics of GIS… 3.1 What is GIS? A Geographic Information System is a framework used to create, manage, visualize and analyze data in a spatial context. Most datasets can be assigned a spatial location, either on the earth’s surface or within some constrained area like a sports field, a vegetation survey plot, or even a drawer in your kitchen. While almost any dataset has a spatial context, and can thus be represented in a GIS, the real question is whether it needs to be analyzed in a GIS environment? The answer to this question depends on the purpose of the analysis. Typically, one would use GIS if you were using the spatial information associated with the data to: access data elements (e.g. select data by spatial location or coverage), perform analytical operations (e.g. overlay and merge two datasets to produce a new dataset), or render visualizations (e.g. generate a map). 3.1.1 An example workflow Identifying the list of South Africa’s threatened ecosystems is largely based on analyses done in GIS, using the map of the historical extent of South Africa’s vegetation types and the National Land Cover data (which, incidentally, were largely created using GIS…). Figure 3.1: South Africa’s list of threatened ecosystems depends heavily on GIS… Here’s a simplified breakdown of the steps in the GIS workflow… Figure 3.2: A simplified workflow for assessing threat to South Africa’s ecosystems, highlighting GIS operations (numbered and in italics). There are many many operations and functions within each of the three major operation types (access, analyze, map). There are also many support functions and operations that are common across the operation types. We’ll delve into these later. 3.2 How do we do GIS? There are a large number of software packages for doing GIS. Some are free (often open source), while others require you to pay for a license (i.e. are proprietary). They also vary in what they do (or the combination of things they do). For example, some of the best known desktop GIS applications, which is what your average GIS user is interested in, include: ArcGIS - a proprietary product that works on Windows only (or online). UCT has a site license, which provides a set number of user licenses for UCT postgrads and staff (see section 3.3 for details). Note that ArcGIS is an ESRI product and comes with (or can be upgraded to include) a suite of other (commercial) software components in addition to the desktop application QGIS (“Quantum GIS”) - free and open source software (FOSS) for Windows, Mac or Linux Google Earth - free (for now), but not open source, for Windows, Mac, Linux, or online Figure 3.3: Screenshot of the QGIS Desktop 3.14 graphical user interface (GUI). Desktop GIS applications are typically graphical user interfaces (GUIs) that call on various other geospatial libraries to do the actual data processing (think of these as sets of functions or tools). Some examples of FOSS geospatial libraries are GDAL and GEOS, which are designed to deal with different data models and/or file types. Another one you’ll encounter quite a bit id PROJ, which deals with coordinate reference systems. We can also do GIS by calling the geospatial libraries and other GIS software types directly with various programming languages. While we can use many (or just about any) coding language, there are a few for which the functions and syntax are better developed for the user, including: R Python JavaScript A major advantage of using these general purpose programming languages for your GIS work is that it allows you take advantage of their functions for statistical analyses and presentation etc all in one environment and/or work flow. This also makes it much easier to make your research reproducible. Note that the geospatial libraries and the other GIS softwares that we call are coded in a variety of other languages, such as Java, C/C++, C# or SQL, but these languages are typically less user friendly and/or more difficult to learn. Other “GIS software types” include: spatial databases such as PostGIS, which is free and open source software (FOSS) for Windows, Mac or Linux and great for storing and querying large and complex datasets web mapping such as MapServer or GeoServer I’m sure there are others I haven’t thought of… 3.3 How to get help? UCT has a GIS Support Unit to assist UCT staff and postgraduate researchers with their GIS and spatial data needs. Their primary goal is to help users develop their GIS skills in order to perform sound data capture, geospatial analysis and map production. They can help you with: Troubleshooting, Project Planning, Analysis, Cartographic Design and Data Handling. GIS Training Applying for an ESRI (ArcMAP) Software license. Note that they predominantly work with ESRI and QGIS and don’t provide support for R, etc. That said, many GIS work flow issues are common across platforms, and the support unit really know what they’re doing. They’re also a good source of data if you’re struggling to find what you need, but please do your homework before asking them for data! Lastly… I am not a help desk!!! The goal for this module is to teach you how to help yourself. I’m available to help in the afternoon office hours during this module, but unfortunately, while I would love to, I do not have the time to help you all with your GIS or R issues for the rest of your careers… You can find the answer to any issue you are encountering in an online forum like GIS Stack Exchange. I typically just type my questions (or copy and past error messages) directly into Google. The trick is working out what to ask, and sometimes you need to reword your question a couple of times to find the answer you need. If you’re really getting nowhere, you can even post your question on a forum, although it is unlikely that you’ll ever need to do this… "],["gis-data-models-and-file-formats.html", "4 GIS data models and file formats 4.1 Data models 4.2 Attribute data 4.3 File formats", " 4 GIS data models and file formats 4.1 Data models GIS data typically come in two data model types vector or raster. 4.1.1 Vector data The three basic vector data types are points, lines (also sometimes referred to as polylines or linestrings) and polygons. While they are treated as different data types, you can also consider them to be a nested hierarchy. For example, to make a line you need two or more points, while a polygon requires three or more lines. Figure 4.1: The hierarchical construction of vector data types. From this we can observe the different properties of the data types: a point is a location in space defined by a set of coordinates based on a coordinate reference system (more about these later) a line is two or more points with straight lines connecting them, where each line has a length a polygon is a set of points connected by lines that form a closed shape, which has an area Note that these “data types” are also commonly called feature classes, geometric primitives or geometries. Later we’ll see that you get more complicated “types,” but these are generally combinations of the above: multipoint, multilinestring, multipolygon, geometry collection, etc and are largely just different data classes designed to help with handling data than unique geometries. Vector data models are obviously the best way to represent points and lines. Polygons are usually the best way to represent discrete (categorical) data, especially where they may have complex boundaries. For example: Figure 4.2: Vector (polygon) representation of discrete data; the vegetation types of the Cape Peninsula. Vector data models are less good for representing continuous data (e.g. elevation, see surface temperature, etc). See further down. 4.1.2 Raster data Raster data are essentially data stored in a regular grid of pixels (or cells). Digital images like jpeg or png files are essentially rasters without spatial information. The value of each pixel is a number representing a measured value (e.g. continuous data such as sea surface temperature) or a category (e.g. discrete data such as land cover class). All pixels have a value, even if the value is “No Data.” Rasters are particularly useful for representing continuous data. Figure 4.3: Raster representation of continuous data; a digital elevation model of the Cape Peninsula. Rasters are great for continuous data. If this was a vector plot of the raw data, each pixel would have to be its own polygon and the legend would have a separate entry for each unique value, &gt;60 000 entries!!! That said, you can quite effectively represent continuous values visually with a vector data model if you bin the continuous data (from the raster) into classes, such as one can do with a filled contour plot (see below). This is not ideal for analyses though, as the binning results in data loss. You’ll find that you often need to convert data between vector and raster models for various reasons, and that this usually means some tough decisions need to be made about what is acceptable data loss. We’ll cover that later. Figure 4.4: Vector representation of continuous data; a filled contour plot of a digital elevation model of the Cape Peninsula using 100m contours. Conversely, rasters are usually not that good at representing categorical data. Note that most raster file formats (and GIS software) can only store numeric data, so this plot misleadingly represents the vegetation types as continuous data. You can label and represent categorical data in rasters in R, but this is usually more effort than its worth and is almost always less effective than using a vector format… A common exception is land use and land cover (LULC) maps, where remotely sensed satellite imagery (raster data) are classified into predefined classes (e.g. agriculture, rock, grassland, etc) based on various criteria or algorithms. Even then, these are difficult to interpret visually with static maps and are best visualized as interactive maps so you can make sense of them by zooming in and panning around. Figure 4.5: Raster representation the discrete data; the vegetation types of the Cape Peninsula. 4.2 Attribute data Attributes are what we know about the objects represented in a layer in addition to their geometry - i.e. each spatial object usually has additional information associated with it. These data are usually stored in an associated Attribute Table. Here are the first few entries of the attribute table for our Cape Peninsula vegetation layer: AREA_HCTR PRMT_MTR veg type Subtype Community geometry 66 6.774255 1596.83494 Beach - FalseBay BEACH Need to Find Out POLYGON ((-46636.54 -380320… 67 14.151168 3886.68578 Beach - FalseBay BEACH Need to Find Out POLYGON ((-47220.45 -380302… 68 8.575597 2154.00714 Beach - FalseBay BEACH Need to Find Out POLYGON ((-48967.57 -380253… 69 0.000001 23.25575 Beach - FalseBay BEACH Need to Find Out POLYGON ((-49355.61 -380223… 70 5.333203 3589.09436 Beach - FalseBay BEACH Need to Find Out POLYGON ((-50008.26 -380132… 71 24.448116 7378.70451 Beach - FalseBay BEACH Need to Find Out POLYGON ((-52927.7 -3800156… Note that vector data generally have attribute tables, but they are rare for raster layers, because most raster file formats can store just one attribute per cell (e.g. elevation) and can’t have associated attribute tables. A handy feature of most GIS systems is that they can treat attribute tables like relational database table structures. Additional information can be joined onto your spatial data by joining two tables with a common key field, as one does when joining two tables of non-spatial data. In GIS, this is called an “Attribute Join,” because you have joined the tables by attribute and haven’t used spatial information (also sometimes called a “non-spatial join”). We’ll learn about “spatial joins” later… 4.3 File formats Linked to data models, and attributes, is file formats. Generally, there are separate file formats for vector vs raster data. Usually, we even have separate files for the different types of vectors (points, lines, polygons, etc), but this is changing as new “database” formats evolve. There is a huge variety of GIS file formats, which have proliferated as different software packages have developed their own set of “native” formats. Each of these have different properties in terms of the data they store, whether they can include attribute data, file size and compression, and of course how they actually store (and retrieve) the data. Many of these, like the ESRI formats, are proprietary (i.e. not open source). If you’ve done any GIS before, you’ll be familiar with ESRI shapefiles, which usually include a group of 3 or more files with the same name, but a different file extension. Each file stores different information. The most common ones are: .shp = the main feature geometry .shx = an index file, used for searching etc .dbf = stores the attribute information .prj = stores the coordinate reference system etc = there are many other optional files that may be present depending on the data stored Shapefiles are by far the most common format for vector data. For raster data, the most common format is probably GeoTIFF (.tif) or ASCII (.asc). You can view the lists of most of the file types supported by R (or at least the types supported by the GDAL geospatial library) by running the code rgdal::ogrDrivers() for vector drivers, which gives this output: name long_name write copy isVector AmigoCloud AmigoCloud TRUE FALSE TRUE ARCGEN Arc/Info Generate FALSE FALSE TRUE AVCBin Arc/Info Binary Coverage FALSE FALSE TRUE AVCE00 Arc/Info E00 (ASCII) Coverage FALSE FALSE TRUE BAG Bathymetry Attributed Grid TRUE TRUE TRUE CAD AutoCAD Driver FALSE FALSE TRUE Carto Carto TRUE FALSE TRUE Cloudant Cloudant / CouchDB TRUE FALSE TRUE CouchDB CouchDB / GeoCouch TRUE FALSE TRUE CSV Comma Separated Value (.csv) TRUE FALSE TRUE CSW OGC CSW (Catalog Service for the Web) FALSE FALSE TRUE DGN Microstation DGN TRUE FALSE TRUE DXF AutoCAD DXF TRUE FALSE TRUE EDIGEO French EDIGEO exchange format FALSE FALSE TRUE EEDA Earth Engine Data API FALSE FALSE TRUE Elasticsearch Elastic Search TRUE FALSE TRUE ESRI Shapefile ESRI Shapefile TRUE FALSE TRUE ESRIC Esri Compact Cache FALSE FALSE TRUE ESRIJSON ESRIJSON FALSE FALSE TRUE FITS Flexible Image Transport System TRUE FALSE TRUE FlatGeobuf FlatGeobuf TRUE FALSE TRUE Geoconcept Geoconcept TRUE FALSE TRUE GeoJSON GeoJSON TRUE FALSE TRUE GeoJSONSeq GeoJSON Sequence TRUE FALSE TRUE Geomedia Geomedia .mdb FALSE FALSE TRUE GeoRSS GeoRSS TRUE FALSE TRUE GML Geography Markup Language (GML) TRUE FALSE TRUE GMLAS Geography Markup Language (GML) driven by application schemas FALSE TRUE TRUE GPKG GeoPackage TRUE TRUE TRUE GPSBabel GPSBabel TRUE FALSE TRUE GPSTrackMaker GPSTrackMaker TRUE FALSE TRUE GPX GPX TRUE FALSE TRUE HTTP HTTP Fetching Wrapper FALSE FALSE TRUE Idrisi Idrisi Vector (.vct) FALSE FALSE TRUE Interlis 1 Interlis 1 TRUE FALSE TRUE Interlis 2 Interlis 2 TRUE FALSE TRUE JML OpenJUMP JML TRUE FALSE TRUE JP2OpenJPEG JPEG-2000 driver based on OpenJPEG library FALSE TRUE TRUE KML Keyhole Markup Language (KML) TRUE FALSE TRUE LIBKML Keyhole Markup Language (LIBKML) TRUE FALSE TRUE LVBAG Kadaster LV BAG Extract 2.0 FALSE FALSE TRUE MapInfo File MapInfo File TRUE FALSE TRUE MapML MapML TRUE FALSE TRUE MBTiles MBTiles TRUE TRUE TRUE Memory Memory TRUE FALSE TRUE MSSQLSpatial Microsoft SQL Server Spatial Database TRUE FALSE TRUE MVT Mapbox Vector Tiles TRUE FALSE TRUE MySQL MySQL TRUE FALSE TRUE NAS NAS - ALKIS FALSE FALSE TRUE netCDF Network Common Data Format TRUE TRUE TRUE NGW NextGIS Web TRUE TRUE TRUE OAPIF OGC API - Features FALSE FALSE TRUE ODBC FALSE FALSE TRUE ODS Open Document/ LibreOffice / OpenOffice Spreadsheet TRUE FALSE TRUE OGCAPI OGCAPI FALSE FALSE TRUE OGR_GMT GMT ASCII Vectors (.gmt) TRUE FALSE TRUE OGR_OGDI OGDI Vectors (VPF, VMAP, DCW) FALSE FALSE TRUE OGR_PDS Planetary Data Systems TABLE FALSE FALSE TRUE OGR_SDTS SDTS FALSE FALSE TRUE OGR_VRT VRT - Virtual Datasource FALSE FALSE TRUE OpenFileGDB ESRI FileGDB FALSE FALSE TRUE OSM OpenStreetMap XML and PBF FALSE FALSE TRUE PCIDSK PCIDSK Database File TRUE FALSE TRUE PDF Geospatial PDF TRUE TRUE TRUE PDS4 NASA Planetary Data System 4 TRUE TRUE TRUE PGDUMP PostgreSQL SQL dump TRUE FALSE TRUE PGeo ESRI Personal GeoDatabase FALSE FALSE TRUE PLSCENES Planet Labs Scenes API FALSE FALSE TRUE PostgreSQL PostgreSQL/PostGIS TRUE FALSE TRUE REC EPIInfo .REC FALSE FALSE TRUE S57 IHO S-57 (ENC) TRUE FALSE TRUE Selafin Selafin TRUE FALSE TRUE SOSI Norwegian SOSI Standard FALSE FALSE TRUE SQLite SQLite / Spatialite TRUE FALSE TRUE SVG Scalable Vector Graphics FALSE FALSE TRUE SXF Storage and eXchange Format FALSE FALSE TRUE TIGER U.S. Census TIGER/Line TRUE FALSE TRUE TopoJSON TopoJSON FALSE FALSE TRUE UK .NTF UK .NTF FALSE FALSE TRUE VDV VDV-451/VDV-452/INTREST Data Format TRUE FALSE TRUE VFK Czech Cadastral Exchange Data Format FALSE FALSE TRUE VICAR MIPL VICAR file TRUE TRUE TRUE Walk FALSE FALSE TRUE WAsP WAsP .map format TRUE FALSE TRUE WFS OGC WFS (Web Feature Service) FALSE FALSE TRUE XLS MS Excel format FALSE FALSE TRUE XLSX MS Office Open XML spreadsheet TRUE FALSE TRUE Or rgdal::gdalDrivers() for raster drivers: name long_name create copy isRaster AAIGrid Arc/Info ASCII Grid FALSE TRUE TRUE ACE2 ACE2 FALSE FALSE TRUE ADRG ARC Digitized Raster Graphics TRUE FALSE TRUE AIG Arc/Info Binary Grid FALSE FALSE TRUE AirSAR AirSAR Polarimetric Image FALSE FALSE TRUE ARG Azavea Raster Grid format FALSE TRUE TRUE BAG Bathymetry Attributed Grid TRUE TRUE TRUE BIGGIF Graphics Interchange Format (.gif) FALSE FALSE TRUE BLX Magellan topo (.blx) FALSE TRUE TRUE BMP MS Windows Device Independent Bitmap TRUE FALSE TRUE BSB Maptech BSB Nautical Charts FALSE FALSE TRUE BT VTP .bt (Binary Terrain) 1.3 Format TRUE FALSE TRUE BYN Natural Resources Canada’s Geoid TRUE FALSE TRUE CAD AutoCAD Driver FALSE FALSE TRUE CALS CALS (Type 1) FALSE TRUE TRUE CEOS CEOS Image FALSE FALSE TRUE COASP DRDC COASP SAR Processor Raster FALSE FALSE TRUE COG Cloud optimized GeoTIFF generator FALSE TRUE TRUE COSAR COSAR Annotated Binary Matrix (TerraSAR-X) FALSE FALSE TRUE CPG Convair PolGASP FALSE FALSE TRUE CTable2 CTable2 Datum Grid Shift TRUE FALSE TRUE CTG USGS LULC Composite Theme Grid FALSE FALSE TRUE DAAS Airbus DS Intelligence Data As A Service driver FALSE FALSE TRUE DERIVED Derived datasets using VRT pixel functions FALSE FALSE TRUE DIMAP SPOT DIMAP FALSE FALSE TRUE DIPEx DIPEx FALSE FALSE TRUE DOQ1 USGS DOQ (Old Style) FALSE FALSE TRUE DOQ2 USGS DOQ (New Style) FALSE FALSE TRUE DTED DTED Elevation Raster FALSE TRUE TRUE ECRGTOC ECRG TOC format FALSE FALSE TRUE EEDAI Earth Engine Data API Image FALSE FALSE TRUE EHdr ESRI .hdr Labelled TRUE TRUE TRUE EIR Erdas Imagine Raw FALSE FALSE TRUE ELAS ELAS TRUE FALSE TRUE ENVI ENVI .hdr Labelled TRUE FALSE TRUE ERS ERMapper .ers Labelled TRUE FALSE TRUE ESAT Envisat Image Format FALSE FALSE TRUE ESRIC Esri Compact Cache FALSE FALSE TRUE FAST EOSAT FAST Format FALSE FALSE TRUE FIT FIT Image FALSE TRUE TRUE FITS Flexible Image Transport System TRUE FALSE TRUE FujiBAS Fuji BAS Scanner Image FALSE FALSE TRUE GenBin Generic Binary (.hdr Labelled) FALSE FALSE TRUE GFF Ground-based SAR Applications Testbed File Format (.gff) FALSE FALSE TRUE GIF Graphics Interchange Format (.gif) FALSE TRUE TRUE GMT GMT NetCDF Grid Format FALSE TRUE TRUE GPKG GeoPackage TRUE TRUE TRUE GRASSASCIIGrid GRASS ASCII Grid FALSE FALSE TRUE GRIB GRIdded Binary (.grb, .grb2) FALSE TRUE TRUE GS7BG Golden Software 7 Binary Grid (.grd) TRUE TRUE TRUE GSAG Golden Software ASCII Grid (.grd) FALSE TRUE TRUE GSBG Golden Software Binary Grid (.grd) TRUE TRUE TRUE GSC GSC Geogrid FALSE FALSE TRUE GTiff GeoTIFF TRUE TRUE TRUE GTX NOAA Vertical Datum .GTX TRUE FALSE TRUE GXF GeoSoft Grid Exchange Format FALSE FALSE TRUE HDF4 Hierarchical Data Format Release 4 FALSE FALSE TRUE HDF4Image HDF4 Dataset TRUE FALSE TRUE HDF5 Hierarchical Data Format Release 5 FALSE FALSE TRUE HDF5Image HDF5 Dataset FALSE FALSE TRUE HEIF ISO/IEC 23008-12:2017 High Efficiency Image File Format FALSE FALSE TRUE HF2 HF2/HFZ heightfield raster FALSE TRUE TRUE HFA Erdas Imagine Images (.img) TRUE TRUE TRUE HTTP HTTP Fetching Wrapper FALSE FALSE TRUE IDA Image Data and Analysis TRUE FALSE TRUE ILWIS ILWIS Raster Map TRUE TRUE TRUE INGR Intergraph Raster TRUE TRUE TRUE IRIS IRIS data (.PPI, .CAPPi etc) FALSE FALSE TRUE ISCE ISCE raster TRUE FALSE TRUE ISG International Service for the Geoid FALSE FALSE TRUE ISIS2 USGS Astrogeology ISIS cube (Version 2) TRUE FALSE TRUE ISIS3 USGS Astrogeology ISIS cube (Version 3) TRUE TRUE TRUE JAXAPALSAR JAXA PALSAR Product Reader (Level 1.1/1.5) FALSE FALSE TRUE JDEM Japanese DEM (.mem) FALSE FALSE TRUE JP2OpenJPEG JPEG-2000 driver based on OpenJPEG library FALSE TRUE TRUE JPEG JPEG JFIF FALSE TRUE TRUE JPEGLS JPEGLS FALSE TRUE TRUE KMLSUPEROVERLAY Kml Super Overlay FALSE TRUE TRUE KRO KOLOR Raw TRUE FALSE TRUE L1B NOAA Polar Orbiter Level 1b Data Set FALSE FALSE TRUE LAN Erdas .LAN/.GIS TRUE FALSE TRUE LCP FARSITE v.4 Landscape File (.lcp) FALSE TRUE TRUE Leveller Leveller heightfield TRUE FALSE TRUE LOSLAS NADCON .los/.las Datum Grid Shift FALSE FALSE TRUE MAP OziExplorer .MAP FALSE FALSE TRUE MBTiles MBTiles TRUE TRUE TRUE MEM In Memory Raster TRUE FALSE TRUE MFF Vexcel MFF Raster TRUE TRUE TRUE MFF2 Vexcel MFF2 (HKV) Raster TRUE TRUE TRUE MRF Meta Raster Format TRUE TRUE TRUE MSGN EUMETSAT Archive native (.nat) FALSE FALSE TRUE NDF NLAPS Data Format FALSE FALSE TRUE netCDF Network Common Data Format TRUE TRUE TRUE NGSGEOID NOAA NGS Geoid Height Grids FALSE FALSE TRUE NGW NextGIS Web TRUE TRUE TRUE NITF National Imagery Transmission Format TRUE TRUE TRUE NTv2 NTv2 Datum Grid Shift TRUE FALSE TRUE NWT_GRC Northwood Classified Grid Format .grc/.tab FALSE FALSE TRUE NWT_GRD Northwood Numeric Grid Format .grd/.tab TRUE TRUE TRUE OGCAPI OGCAPI FALSE FALSE TRUE OZI OziExplorer Image File FALSE FALSE TRUE PAux PCI .aux Labelled TRUE FALSE TRUE PCIDSK PCIDSK Database File TRUE FALSE TRUE PCRaster PCRaster Raster File TRUE TRUE TRUE PDF Geospatial PDF TRUE TRUE TRUE PDS NASA Planetary Data System FALSE FALSE TRUE PDS4 NASA Planetary Data System 4 TRUE TRUE TRUE PLMOSAIC Planet Labs Mosaics API FALSE FALSE TRUE PLSCENES Planet Labs Scenes API FALSE FALSE TRUE PNG Portable Network Graphics FALSE TRUE TRUE PNM Portable Pixmap Format (netpbm) TRUE FALSE TRUE PostGISRaster PostGIS Raster driver FALSE TRUE TRUE PRF Racurs PHOTOMOD PRF FALSE FALSE TRUE R R Object Data Store FALSE TRUE TRUE Rasterlite Rasterlite FALSE TRUE TRUE RDA DigitalGlobe Raster Data Access driver FALSE FALSE TRUE RIK Swedish Grid RIK (.rik) FALSE FALSE TRUE RMF Raster Matrix Format TRUE FALSE TRUE ROI_PAC ROI_PAC raster TRUE FALSE TRUE RPFTOC Raster Product Format TOC format FALSE FALSE TRUE RRASTER R Raster TRUE TRUE TRUE RS2 RadarSat 2 XML Product FALSE FALSE TRUE RST Idrisi Raster A.1 TRUE TRUE TRUE SAFE Sentinel-1 SAR SAFE Product FALSE FALSE TRUE SAGA SAGA GIS Binary Grid (.sdat, .sg-grd-z) TRUE TRUE TRUE SAR_CEOS CEOS SAR Image FALSE FALSE TRUE SDTS SDTS Raster FALSE FALSE TRUE SENTINEL2 Sentinel 2 FALSE FALSE TRUE SGI SGI Image File Format 1.0 TRUE FALSE TRUE SIGDEM Scaled Integer Gridded DEM .sigdem FALSE TRUE TRUE SNODAS Snow Data Assimilation System FALSE FALSE TRUE SRP Standard Raster Product (ASRP/USRP) FALSE FALSE TRUE SRTMHGT SRTMHGT File Format FALSE TRUE TRUE STACIT Spatio-Temporal Asset Catalog Items FALSE FALSE TRUE STACTA Spatio-Temporal Asset Catalog Tiled Assets FALSE FALSE TRUE Terragen Terragen heightfield TRUE FALSE TRUE TGA TGA/TARGA Image File Format FALSE FALSE TRUE TIL EarthWatch .TIL FALSE FALSE TRUE TSX TerraSAR-X Product FALSE FALSE TRUE USGSDEM USGS Optional ASCII DEM (and CDED) FALSE TRUE TRUE VICAR MIPL VICAR file TRUE TRUE TRUE VRT Virtual Raster TRUE TRUE TRUE WCS OGC Web Coverage Service FALSE FALSE TRUE WEBP WEBP FALSE TRUE TRUE WMS OGC Web Map Service FALSE TRUE TRUE WMTS OGC Web Map Tile Service FALSE TRUE TRUE XPM X11 PixMap Format FALSE TRUE TRUE XYZ ASCII Gridded XYZ FALSE TRUE TRUE Zarr Zarr TRUE FALSE TRUE ZMap ZMap Plus Grid FALSE TRUE TRUE Lots!!! But note that there are others that are not supported in R. Perhaps the most common unsupported ones you’ll encounter are the ESRI geodatabases (.gdb and .mdb), which are designed for ArcGIS and are super efficient (in ArcGIS), but ESRI haven’t released the drivers, so they don’t work (or at least not properly) for most other GIS software… Note that there has been a big push to develop a standardized set of open source, efficient and interoperable file formats. Some examples to watch: GeoPackage - SQLite database containers for storing vector, raster and attribute data in a compact and transferable format. GeoJSON - a geographic version of JSON (JavaScript Object Notation) for vector data, very commonly used for web apps etc. Cloud-optimized GeoTIFF - as the name suggests; a GeoTIFF-based format for optimally hosting and allowing querying and downloading of raster data on the cloud… Simple Features - an open, efficient and interoperable standard for vector data. "],["some-important-concepts-and-pitfalls.html", "5 Some important concepts and pitfalls 5.1 Scale 5.2 Coordinate Reference Systems (CRS)", " 5 Some important concepts and pitfalls 5.1 Scale All maps have a scale. Scale is the ratio between the size of the representation of an object and its size in reality. E.g. objects on a 1:50,000 scale map are drawn at 1/50,000 their size, so 1cm on the map represents a distance of 500m in reality (i.e. \\(1*50,000 = 50,000cm = 500m\\)). Figure 5.1: The Cape Peninsula (3418AB &amp; AD) from South Africa’s 1:50,000 topographic map series. The printed scale for these maps is 1:50,000, but what is it on your screen? GIS is usually scaleless (or at least flexible in scale); we can “zoom in” as much as we want to, and perform operations at just about any scale we want to, but should we? There are lots of issues we need to consider! Representation (i.e. mapping)… There are 2 issues here: Firstly, a 1mm thick line on a 1:50,000 scale map would be 50m wide in reality. Conversely, a 5m wide road would be 1/10mm on the map. Would the map be readable? Sometimes we break the rules of scale to make maps readable. Bear this in mind! Secondly, scale and our desired representation affect how we capture data. For example, a road is typically best represented as a line at 1:5,000 scale or smaller (note that scale is a ratio, so “small scale” = large extent or area!). At 1:1,000 scale a 5m wide road would be 5mm across on the map, so one might capture it as a polygon to represent its area. You should always consider the purpose for which (and scale at which) the data were captured before using them for a new application! This affects both the appropriateness of the data type and the accuracy and precision of the data… Figure 5.2: Zoomed in on Chapman’s peak on the the Cape Peninsula 1:50,000 topographic map (3418AB &amp; AD). At this scale the road (in red) is probably better mapped as an area than a line? Accuracy of location versus scale of data capture. We should always check the scale at which the data were captured to make sure it is accurate enough for the scale of the analysis we are doing. For example, the various vegetation units in the National Vegetation Map of South Africa were mapped at a range of scales, some as small as 1:250,000. At this scale 1mm = 250m, so a minor digitization error is a huge difference on the ground! If you need your analysis to be accurate to &lt;10m then you’d probably need data mapped at a scale larger than 1:10,000. Precision - Can mean two things: The unit or number of decimal places to which the attribute has been measured (and can be stored) The spread of repeat measurements (typically in field data collection). A big spread means the measurements weren’t very precise… A quick aside on the difference between accuracy and precision! Figure 5.3: The difference between accuracy and precision, where the true value is the origin (0,0). A last word on Scale… For vector data, we typically refer to scale when describing a data set. For raster data, we typically refer to pixel resolution (or sample interval). For example, a 30m digital elevation model is made up of pixels 30m across. For remotely sensed imagery (i.e. from drone, plane or satellite), one often uses the term “ground sample distance.” BEWARE!!! If you convert between raster and vector data formats (e.g. by “rasterizing” a vector layer, or “binning” a raster into polygons), it will affect all three of precision, accuracy and representation, so you need to give careful thought to whether what you are doing is appropriate for the analysis you are doing! 5.2 Coordinate Reference Systems (CRS) Coordinate Reference Systems (CRS) provide a framework for defining real-world locations. There are many different CRSs, with different properties. They can be a minefield, and I don’t have the time to cover them in any detail. I provide some of the basics here, and list some of the golden rules (mostly from the GIS Support Unit) below. 5.2.1 Geographic (or “unprojected”) Coordinate Systems The most common coordinate system is latitude/longitude, also known as geographic, lat/long or sometimes WGS84. There are many ways to record geographic coordinates: Degrees, Minutes &amp; Seconds: S33°26’46”,E18°10’23’’ Degrees and decimal minutes: S33°26.7666667’,E18°10.3833333’ (out to get you!) Decimal Degrees: -33.4461111,18.17305556 Most GIS prefer decimal degrees… The problem with doing analyses using geographic CRS is that lat/long coordinates are actually angular measurements on a 3D sphere (Geodesic) and degrees differ in their actual ground distance depending on where you are on the planet. They also differ in the N-S vs W-E plane! Figure 5.4: Map highlighting that a degree is larger at the equator than at the poles. Image source: https://annakrystalli.me/intro-r-gis/gis.html This means that Euclidean measurement calculations are not appropriate for calculating areas and distances. 5.2.2 Projected Coordinate Systems To perform linear measurements from a 3D shape using Euclidean methods, you need to squash that shape into a 2D plane. This squashing is called a projection… Figure 5.5: How many different ways could you flatten a naartjie peel? There are 4 properties that get distorted, you can pick which one gets preserved the best by a projection type: Shape - you want a Conformal projection Area - Equal-Area Distance - Equidistant Direction - Azimuthal (see here for a nice description and illustrations) Some “general purpose” projections, like Transverse Mercator (TM), try to compromise and minimize distortion in all properties, but can’t preserve any perfectly. Their distortions also tend to get worse the larger the spatial extent being analyzed. Projections get tuned to best fit an area through the use of projection parameters. For example, Transverse Mercator, which is used by the 1:50,000 map series and by Municipalities like Cape Town, uses a narrow projection window of 2 degree-wide bands. As a result, our map series projection parameters are set by moving the central meridian (or tangent) line of longitude every odd degree across the country. Cape Town is close to 19°E, so our version is colloquially called ‘Lo19.’ For Durban you’d use ‘Lo31.’ Universal Transverse Mercator (UTM) is similar, but uses 6 degree-wide bands so that it can be applied across larger extents. 5.2.3 Projection codes The type of CRS is usually (but not always) stored in the metadata of your file (or dataset, if it is comprised of multiple files like an ESRI shapefile). There are various formats for this, the most commonly used in R being known as EPSG, PROJ4 or WKT codes or strings (be warned, there are many more…). To assign a CRS or reproject your data in R you need to know the appropriate code in the format required by the function. Fortunately, there is a huge online library of the codes at https://spatialreference.org/. I also provide some suggested projections, based on the properties you’d like to preserve, and their codes here. Note that for UTM and TM you may need to adjust the PROJ4 strings for your area - read the comments. Why do projections matter? Figure 5.6: Africa, visualized with different coordinate reference systems. All four maps are different, even if the differences may be subtle! 5.2.4 “On the fly” vs manual projection Note that some GIS tools can perform “on the fly” (re)projection of data. For example, by default ArcGIS sets the CRS for a project from the first dataset imported. When you want to visualize the data, it will reproject all other datasets to the set CRS so that it can visualize it properly. Similarly, ArcGIS and other software can project data in a geographic CRS to a projected CRS on the fly when asked to perform Euclidean measurement calculations. On the fly projection can clearly be very useful, but it can also be misleading if you don’t know what its doing. My take is that you should always check the default settings for the software you’re using, and check the set CRS(s) and individual dataset CRS(s) to make sure you are working in a suitable CRS for the operations you want to perform… 5.2.5 The golden rules… If things don’t line up, its probably a CRS issue. You need to know what CRS your dataset is in. This is essential, because you need to define your projections to be able to compare datasets. If they are not the same, you will need to reproject one to align with the other*. If your datasets are not in the same CRS, most GIS software will give you warning or error messages, but not always! Note that not all file formats store the CRS “metadata,” so check and store it yourself if needed! You need to make sure you use a CRS that best preserves the properties you are interested in (area, distance, direction, shape). More on this below in section 5.2.2. If your areas and distances are stupidly small (0.001 etc) your data are probably in Geographic (i.e. degrees and not a unit of distance like metres). Always interrogate and “common-sense-check” your results!!! *Defining Projection is not the same as reprojecting! Think in terms of languages, “I have text in Japanese and want it in English.” Defining is saying what it IS (“This text is in Japanese” - defining Japanese as English gives you garbage). Reprojecting is what you want it to be (i.e. translate Japanese to English). Two other issues to look out for: The official South African CRS is waiting to get you. If you see Gauss Conform run screaming, its a left handed CRS based on Southings and Westings (i.e. completely inverted…). Simple, yes? Datums… These are essentially models of the shape of the surface of the planet. Most South African datasets (since 1999 at least) use the Hartebeesthoek 94 datum, which is our local “bespoke” solution. It’s pretty much the same as the WGS84 datum (a good global datum), and the difference is negligible for most ecological analyses. Our former local “bespoke” datum (the Cape or Clarke1880 datum), which was often used for data before 1999, is out to get you. These datasets will never line up perfectly with modern data sets when reprojected in a normal GIS and will usually be a couple of tens of metres off… Some other important pitfalls to avoid are best covered in this chapter in Manny Gimond’s Intro to GIS and Spatial Analysis. "],["r-as-a-gis.html", "6 R as a GIS 6.1 Overview 6.2 Some key R packages", " 6 R as a GIS 6.1 Overview Points, lines, polygons and rasters - R can handle them all and more! R is a free software environment for statistical computing and graphics, but its abilities have been extended into many realms through the ~18,000 (!) contributed extension packages (also called libraries). The list of packages can be bewildering, but fortunately some great folks have taken the time to sift through and make some sense of them for different focal topics and created Task Views. For GIS there are two Task Views of interest: Spatial - maintained by Roger Bivand and Jakub Nowosad, and SpatioTemporal - maintained by Edzer Pebesma and Roger Bivand They overlap somewhat, but the latter specifically focuses on data where both location and time of observation are registered, and relevant for the analysis of the data. Each has an overview page listing packages and highlighting their respective strengths, weaknesses etc., e.g. Figure 6.1: Screenshot of the “Spatial” Task View at https://cran.r-project.org/ The Spatial Task View focuses on “Analysis of Spatial Data,” with sections on: Classes for spatial data and metadata Reading and writing spatial data Handling spatial data Visualizing spatial data Analyzing spatial data Task Views also allow easy download and installation of all packages in a Task View using library(ctv) (which you can install with install.packages(\"ctv\")). In this case the code you’d need to install the Task View is ctv::install.views(\"Spatial\", coreOnly = TRUE). But beware! If you leave out the coreOnly = TRUE it can take a while to download and install!!! It may take a while even then… You don’t need the whole Task View for my tutorials, so don’t bother downloading it if you’re just working through these. Figure 6.2: Screenshot of the Task View landing page at https://cran.r-project.org/ 6.2 Some key R packages We don’t have time to go through all packages or provide a full history, but here are some notes in brief. 6.2.1 For vector data (although some of these packages can handle rasters too) The two leading packages were sp and rgdal. While these are still active and useful, they are being superseded by a newer package sf, which is a modern implementation and standardization of parts of sp. It is highly recommended that you use sf over these older packages as they will not be maintained in the long term, largely because they rely on other packages that are no longer maintained because their creators have retired. sf stands for “Simple Features for R,” in compliance with the OGC Simple Feature standard. It is highly efficient, and comes with the advantage that it uses Tidyverse principles and coding styles, e.g. allowing use of the pipe operator (%&gt;%) and the direct application of library(dplyr) data manipulation and library(ggplot2) visualization functions. I will use sf for the most part in the demonstration material. Unfortunately, not all operations are available in sf yet and I may still have to use sp at times, especially when performing operations using both vector and raster data. Here’s a quick list of the functions available in sf: library(sf) methods(class = &#39;sf&#39;) ## [1] [ [[&lt;- ## [3] $&lt;- aggregate ## [5] anti_join arrange ## [7] as.data.frame cbind ## [9] coerce dbDataType ## [11] dbWriteTable distinct ## [13] dplyr_reconstruct extent ## [15] extract filter ## [17] full_join gather ## [19] group_by group_split ## [21] identify initialize ## [23] inner_join left_join ## [25] mask merge ## [27] mutate nest ## [29] pivot_longer plot ## [31] print raster ## [33] rasterize rbind ## [35] rename right_join ## [37] rowwise sample_frac ## [39] sample_n select ## [41] semi_join separate_rows ## [43] separate show ## [45] slice slotsFromS3 ## [47] spread st_agr ## [49] st_agr&lt;- st_area ## [51] st_as_s2 st_as_sf ## [53] st_as_sfc st_bbox ## [55] st_boundary st_buffer ## [57] st_cast st_centroid ## [59] st_collection_extract st_convex_hull ## [61] st_coordinates st_crop ## [63] st_crs st_crs&lt;- ## [65] st_difference st_drop_geometry ## [67] st_filter st_geometry ## [69] st_geometry&lt;- st_inscribed_circle ## [71] st_interpolate_aw st_intersection ## [73] st_intersects st_is_valid ## [75] st_is st_join ## [77] st_line_merge st_m_range ## [79] st_make_valid st_nearest_points ## [81] st_node st_normalize ## [83] st_point_on_surface st_polygonize ## [85] st_precision st_reverse ## [87] st_sample st_segmentize ## [89] st_set_precision st_shift_longitude ## [91] st_simplify st_snap ## [93] st_sym_difference st_transform ## [95] st_triangulate st_union ## [97] st_voronoi st_wrap_dateline ## [99] st_write st_z_range ## [101] st_zm summarise ## [103] transform transmute ## [105] ungroup unite ## [107] unnest ## see &#39;?methods&#39; for accessing help and source code This doesn’t tell you how to use them though. To get help with a function in R just type “?” followed by the function name, e.g. ?st_read, and it’ll take you to the help page. Of course, you don’t want to have to read every help page to find the function you want! Fortunately, here’s a “cheat sheet” that allows you to find the function you want relatively quickly (once you’re familiar with the syntax etc): Figure 6.3: An R cheat sheet for library(sf) by Ryan Garnett (page 1). Figure 6.4: An R cheat sheet for library(sf) by Ryan Garnett (page 2). 6.2.2 For raster data By far the best package has been raster, maintained by Robert Hijmans (of WorldClim fame), and can do just about anything with rasters and interfaces with sp very nicely. Unfortunately, both raster and sp are being phased out as explained above. raster is currently being superseded by a new package called terra, also being developed by Hijmans. “terra is very similar to the raster package; but terra is simpler, better, and faster” - Roger Bivand I first developed this module using raster, because terra was still largely in development, so I have kept the demonstrations of raster for those who depend on them for now, but highly recommend you start using terra, because I’ll likely remove the raster material next year. It’s worth noting that raster and terra can handle vector data directly too, and that sometimes their integration with sf is a little clunky, but it does seem to get better all the time. Unfortunately, there’s no cheat sheet for terra or raster, but there’s a lot of documentation and tutorials here. terra is also able to handle spatiotemporal arrays (raster and vector data cubes). Think of these as time-series of GIS data, like satellite archives, etc. Other powerful packages to watch in the spatiotemporal space are stars and gdalcubes. "],["rdemo.html", "7 Vector GIS operations in R 7.1 Case study and demo datasets 7.2 Reading and writing 7.3 Basic plotting 7.4 Cropping 7.5 Select and subset by attribute 7.6 Combine classes and dissolve by attribute 7.7 Calling iNaturalist locality (point) data from R 7.8 Converting a dataframe into a spatial object 7.9 Adding basemaps to plots 7.10 Interactive maps with leaflet 7.11 Reprojecting 7.12 Intersecting points and polygons 7.13 Colour or label points 7.14 Buffering 7.15 Within distance and intersect", " 7 Vector GIS operations in R 7.1 Case study and demo datasets Ok, for demonstrating some of the many GIS operations R can perform we will be using data from one of my favourite study areas, the Cape Peninsula. The datasets we will use, some of their properties and where to source them are tabled below. You can also download them as one .zip file here, because the City of Cape Town updates the files from time to time and small changes in format, naming etc break the code to come. Please do not use the version from the .zip file in any real analyses etc, because I can make no guarantees about their accuracy etc. It is best to use the latest version from the links in the table. Name Data source Data model Geometry type File format Localities iNaturalist Vector Point Data frame from API Watercourses City of Cape Town Vector Line ESRI shapefile Vegetation Types (historical) City of Cape Town Vector Polygon ESRI shapefile Vegetation Types (remnants) City of Cape Town Vector Polygon ESRI shapefile Elevation City of Cape Town Raster Raster GeoTIFF If you’d like to follow along and run the analyses that follow, please follow the links and download the datasets. The elevation dataset is ~13MB as a .zip file, but will expand to ~130MB. There’s no need to download the iNaturalist data as we’ll download it directly from R. For installing R and the required packages see section 1.2. 7.2 Reading and writing sf has a one-size-fits-all approach in that most functions can be applied to most different data types (point, line, polygon, etc) or, in the case of reading and writing, file formats. To read data the function you want is st_read(). You’ll note that most of the sf functions begin with “st_” - this stands for “spatial and temporal” and is the same in some other GIS like PostGIS. Let’s try to read in some data with st_read(): NOTE: if you’re trying any of the read/write code at home, you’ll need to set the file path to where you put the data and want the outputs on your local machine. You can also use ?setwd to simplify this. If you are on Windows, make sure to change the backslashes “\" to either double backslashes or forward slashes”/“. library(sf) veg &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous.shp&quot;) ## Reading layer `Vegetation_Indigenous&#39; from data source ## `/home/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1325 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63972.95 ymin: -3803535 xmax: 430.8125 ymax: -3705149 ## Projected CRS: WGS_1984_Transverse_Mercator This has successfully read in the data and given us a summary of some of its properties. Note the “Projected CRS” WGS_1984_Transverse_Mercator, so it is Transverse Mercator (TM), using the WGS84 datum, but it hasn’t told us what line of longitude it’s centred on, which is an essential feature of any TM projection. The first thing you should do when interrogating any spatial data is to check the coordinate reference system (CRS). In sf, you do this with the function st_crs, like so: st_crs(veg) ## Coordinate Reference System: ## User input: WGS_1984_Transverse_Mercator ## wkt: ## PROJCRS[&quot;WGS_1984_Transverse_Mercator&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;Hartebeesthoek94&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6148]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;unnamed&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,19, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,1, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] This shows us the CRS as a WKT string and looks very complicated… Essentially there are three components to it: BASEGEOGCRS - the geographic (or unprojected) CRS CONVERSION - the projection, which includes a lot of information, but essentially tells us it’s Transverse Mercator, and the \"Longitude of natural origin\",19 indicates that it is centred on the 19 degree line of longitude (i.e. we’re dealing with Transverse Mercator Lo19) CS - the cartesian axes, showing that we’re dealing with axes oriented to North and East and units of metres TM Lo19 is a good projection for most calculations at this scale (and on this line of longitude). If you’re using Transverse Mercator, always make sure it is set for your closest “odd” line of longitude (i.e. Lo19, Lo21, Lo23)! More on working with coordinate reference systems in see section 7.11. Let’s have a closer look at the data: class(veg) ## [1] &quot;sf&quot; &quot;data.frame&quot; It is an object of two different “classes,” a data.frame, which is an R object class you should be familiar with, and class sf, which is the native class for the sf library. The nice thing about being both classes is it means you can apply the functions built for either class, such as head, a commonly used function for looking at the first few rows of a dataframe. head(veg) ## Simple feature collection with 6 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -51324.95 ymin: -3732075 xmax: -35653.98 ymax: -3718136 ## Projected CRS: WGS_1984_Transverse_Mercator ## AREA_HCTR PRMT_MTR National_ ## 1 1807.5183616 24763.8073 Atlantis Sand Fynbos ## 2 2.1437754 609.5892 Atlantis Sand Fynbos ## 3 0.2134855 185.5566 Atlantis Sand Fynbos ## 4 2.8602421 652.1671 Atlantis Sand Fynbos ## 5 0.5468058 336.8006 Atlantis Sand Fynbos ## 6 0.4172046 259.7772 Atlantis Sand Fynbos ## Subtype Community ## 1 on marine-derived acid sands Need to Find Out ## 2 on marine-derived acid sands Need to Find Out ## 3 on marine-derived acid sands Need to Find Out ## 4 on marine-derived acid sands Need to Find Out ## 5 on marine-derived acid sands Need to Find Out ## 6 on marine-derived acid sands Need to Find Out ## geometry ## 1 POLYGON ((-48203.88 -372294... ## 2 POLYGON ((-36676.72 -371974... ## 3 POLYGON ((-35891.46 -371837... ## 4 POLYGON ((-35750.07 -371847... ## 5 POLYGON ((-35823.89 -371817... ## 6 POLYGON ((-35929.18 -371824... Note there are 5 attribute columns (the attribute table as you would see in most GIS software) and a 6th geometry column. All sf objects have a geometry column. This is where it stores the geometry - i.e. the point, line, polygon etc - associated with each row of attribute data. To write data with sf you use st_write(), like so: st_write(veg, &quot;data/cape_peninsula/veg/Vegetation_Indigenous_duplicate.shp&quot;, append = FALSE) ## Writing layer `Vegetation_Indigenous_duplicate&#39; to data source ## `data/cape_peninsula/veg/Vegetation_Indigenous_duplicate.shp&#39; using driver `ESRI Shapefile&#39; ## Writing 1325 features with 5 fields and geometry type Polygon. Note that I added , append = FALSE because in my case it I want it to overwrite an existing file by the same name, and this command suppresses the warning it would usually give. file.exists(&quot;data/cape_peninsula/veg/Vegetation_Indigenous_duplicate.shp&quot;) ## [1] TRUE Confirms that the file exists, so it has written a file out successfully. Note that the function recognised that I wanted to write out an ESRI shapefile from the .shp file extension I provided. You can set the file type using the driver = setting in st_write(). Try st_drivers() for the list of file types supported. 7.3 Basic plotting As with other data types in R (and perhaps even more so with spatial data), you can really go to town with plotting. I’m only going to show you enough to be able to interrogate your data. Making it look pretty is a week-long course or more in its own right. Check out the “Making maps with R” chapter in Lovelace et al’s online book Geocomputation with R for a good start. The easiest way to plot datasets in R is often a bad thing to do when working with spatial datasets! plot(veg) Fortunately, in this case the dataset isn’t too big, but often you’ll either be overwhelmed with plots or your computer will crash… Why 5 plots and not one? This is because sf wants to plot the properties of each attribute in the attribute table. Fortunately, there were only 5, but there could have been hundreds! You can select the one you want with indexing like so: plot(veg[3]) These are the National Vegetation Types for the City of Cape Town municipality. You’ll note that we’re using the base R graphics functions. I mentioned before that sf integrates well with the Tidyverse, so this could also be plotted like so: library(tidyverse) #calls ggplot2 and other Tidyverse packages together ggplot() + geom_sf(data=veg, aes(fill = `National_`)) That’s better for the legend, but now we’ve squashed the map. Let’s narrow in on the Cape Peninsula for convenience. 7.4 Cropping Here we’ll apply the function st_crop(). To use the function you need an object to crop, and an extent or bounding box to crop to. sf is clever, and you can set the extent by giving it another object who’s extent you’d like to match (check the bounding box given when we read in the data earlier). We don’t have a second object in this case, so we have to provide a “numeric vector with named elements xmin, ymin, xmax and ymax,” like so: #Make a vector with desired coordinates in metres according to TM Lo19 ext &lt;- c(-66642.18, -3809853.29, -44412.18, -3750723.29) ext ## [1] -66642.18 -3809853.29 -44412.18 -3750723.29 #Give the vector names names(ext) &lt;- c(&quot;xmin&quot;, &quot;ymin&quot;, &quot;xmax&quot;, &quot;ymax&quot;) ext ## xmin ymin xmax ymax ## -66642.18 -3809853.29 -44412.18 -3750723.29 Now we can feed that into st_crop veg &lt;- st_crop(veg, ext) #Note that I&#39;m overwriting the old data object &quot;veg&quot; ## Warning: attribute variables are assumed to be spatially ## constant throughout all geometries ggplot() + geom_sf(data=veg, aes(fill = `National_`)) Better? But what about the silly splits like Peninsula Granite Fynbos - North/South and Cape Flats Dune Strandveld - West Coast/False Bay. Which ones do I mean? 7.5 Select and subset by attribute Let’s select them from the attribute table and plot them. #Make a vector of the veg types we want split_veg &lt;- c(&quot;Peninsula Granite Fynbos - North&quot;, &quot;Peninsula Granite Fynbos - South&quot;, &quot;Cape Flats Dune Strandveld - West Coast&quot;, &quot;Cape Flats Dune Strandveld - False Bay&quot;) #Use base R indexing to select attributes vegsub &lt;- veg[which(veg$National_ %in% split_veg),] #Plot ggplot() + geom_sf(data=vegsub, aes(fill = `National_`)) Or tidyverse… #Using tidyverse piping to filter and plot veg %&gt;% filter(National_ %in% split_veg) %&gt;% ggplot() + geom_sf(aes(fill = `National_`)) #The advantage being that you don&#39;t have to make the intermediate &quot;vegsub&quot; object Ok. What if we decided we don’t want them split? 7.6 Combine classes and dissolve by attribute We can just rename them in appropriate column in the attribute table… vegsub$National_ &lt;- str_replace_all(vegsub$National_, c(&quot;Peninsula Granite Fynbos - North&quot; = &quot;Peninsula Granite Fynbos&quot;, &quot;Peninsula Granite Fynbos - South&quot; = &quot;Peninsula Granite Fynbos&quot;, &quot;Cape Flats Dune Strandveld - West Coast&quot; = &quot;Cape Flats Dune Strandveld&quot;, &quot;Cape Flats Dune Strandveld - False Bay&quot; = &quot;Cape Flats Dune Strandveld&quot;)) ggplot() + geom_sf(data=vegsub, aes(fill = `National_`)) Nice, but from the polygon boundaries we see that there are a number of adjacent polygons (i.e. they have shared boundaries) that are of the same veg type. We can “dissolve” and plot it without the unwanted boundaries using summarize(): vegsub %&gt;% group_by(National_) %&gt;% summarize() %&gt;% ggplot() + geom_sf(aes(fill = National_)) Ok… I think we’ve flogged that horse as far as it’ll go for now. Let’s bring in another dataset. How about points? 7.7 Calling iNaturalist locality (point) data from R A very cool feature of iNaturalist is that the team at rOpenSci have built a great R package for interfacing with it directly, called rinat! Let’s get all the records we can for the King Protea (Protea cynaroides). library(rinat) #Call the data directly from iNat pc &lt;- get_inat_obs(taxon_name = &quot;Protea cynaroides&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #View the first few rows of data head(pc) ## scientific_name datetime description ## 1 Protea cynaroides 2022-04-13 11:48:00 +0200 ## 2 Protea cynaroides 2022-04-06 14:31:09 +0200 ## 3 Protea cynaroides 2022-04-12 11:11:32 UTC ## 4 Protea cynaroides 2022-04-12 10:14:03 +0200 ## 5 Protea cynaroides 2022-04-12 10:11:18 +0200 ## 6 Protea cynaroides 2022-04-12 10:04:47 +0200 ## place_guess ## 1 Western Cape, ZA ## 2 Table Mountain National Park, ZA-WC-CT, ZA-WC, ZA ## 3 Table Mountain National Park, ZA-WC-CT, ZA-WC, ZA ## 4 Table Mountain National Park, ZA-WC-CT, ZA-WC, ZA ## 5 Table Mountain National Park, ZA-WC-CT, ZA-WC, ZA ## 6 Table Mountain National Park, ZA-WC-CT, ZA-WC, ZA ## latitude longitude tag_list common_name ## 1 -34.26310 18.45771 King Protea ## 2 -34.07419 18.39739 King Protea ## 3 -34.11413 18.43995 King Protea ## 4 -34.11744 18.43949 King Protea ## 5 -34.11781 18.44010 King Protea ## 6 -34.11845 18.44151 King Protea ## url ## 1 https://www.inaturalist.org/observations/111391781 ## 2 https://www.inaturalist.org/observations/111257518 ## 3 https://www.inaturalist.org/observations/111189309 ## 4 https://www.inaturalist.org/observations/111189125 ## 5 https://www.inaturalist.org/observations/111189092 ## 6 https://www.inaturalist.org/observations/111189018 ## image_url ## 1 https://inaturalist-open-data.s3.amazonaws.com/photos/188041176/medium.jpeg ## 2 https://inaturalist-open-data.s3.amazonaws.com/photos/187794625/medium.jpg ## 3 https://inaturalist-open-data.s3.amazonaws.com/photos/187673956/medium.jpeg ## 4 https://inaturalist-open-data.s3.amazonaws.com/photos/187673602/medium.jpeg ## 5 https://inaturalist-open-data.s3.amazonaws.com/photos/187673580/medium.jpeg ## 6 https://inaturalist-open-data.s3.amazonaws.com/photos/187673480/medium.jpeg ## user_login id species_guess iconic_taxon_name ## 1 justinhawthorne 111391781 Giant Protea Plantae ## 2 as_andile 111257518 King Protea Plantae ## 3 susanthescout 111189309 King Protea Plantae ## 4 susanthescout 111189125 King Protea Plantae ## 5 susanthescout 111189092 King Protea Plantae ## 6 susanthescout 111189018 King Protea Plantae ## taxon_id num_identification_agreements ## 1 132848 0 ## 2 132848 4 ## 3 132848 3 ## 4 132848 3 ## 5 132848 3 ## 6 132848 3 ## num_identification_disagreements ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 ## observed_on_string observed_on ## 1 2022-04-13 11:48:00 2022-04-13 ## 2 Wed Apr 06 2022 14:31:09 GMT+0200 (GMT+2) 2022-04-06 ## 3 2022-04-12 11:11:32 2022-04-12 ## 4 2022-04-12 10:14:03 2022-04-12 ## 5 2022-04-12 10:11:18 2022-04-12 ## 6 2022-04-12 10:04:47 2022-04-12 ## time_observed_at time_zone positional_accuracy ## 1 2022-04-13 09:48:00 UTC Pretoria 21 ## 2 2022-04-06 12:31:09 UTC Pretoria 8 ## 3 2022-04-12 11:11:32 UTC UTC 1 ## 4 2022-04-12 08:14:03 UTC Pretoria 2 ## 5 2022-04-12 08:11:18 UTC Pretoria 2 ## 6 2022-04-12 08:04:47 UTC Pretoria 2 ## public_positional_accuracy geoprivacy taxon_geoprivacy ## 1 28846 obscured open ## 2 8 open ## 3 1 open ## 4 2 open ## 5 2 open ## 6 2 open ## coordinates_obscured positioning_method ## 1 true gps ## 2 false ## 3 false ## 4 false ## 5 false ## 6 false ## positioning_device user_id created_at ## 1 gps 2040203 2022-04-14 08:01:25 UTC ## 2 2665692 2022-04-12 23:44:44 UTC ## 3 1495441 2022-04-12 12:22:58 UTC ## 4 1495441 2022-04-12 12:19:22 UTC ## 5 1495441 2022-04-12 12:19:03 UTC ## 6 1495441 2022-04-12 12:17:51 UTC ## updated_at quality_grade license sound_url ## 1 2022-04-14 08:01:48 UTC needs_id CC-BY-NC NA ## 2 2022-04-13 18:50:22 UTC research CC-BY-NC NA ## 3 2022-04-13 21:43:06 UTC research CC-BY-NC NA ## 4 2022-04-13 08:59:51 UTC research CC-BY-NC NA ## 5 2022-04-13 06:19:56 UTC research CC-BY-NC NA ## 6 2022-04-13 06:20:23 UTC research CC-BY-NC NA ## oauth_application_id captive_cultivated ## 1 2 false ## 2 3 false ## 3 2 false ## 4 2 false ## 5 2 false ## 6 2 false #Filter returned observations by a range of column attribute criteria pc &lt;- pc %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) class(pc) ## [1] &quot;data.frame&quot; Ok, so this is a dataframe with lat/long data, but it isn’t registered as an object with spatial attributes (i.e. geometries). 7.8 Converting a dataframe into a spatial object To make it an object of class(sf) we use the function st_as_sf(). #Make the dataframe a spatial object of class = &quot;sf&quot; pc &lt;- st_as_sf(pc, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) Note that I had to define the CRS!!! I defined it to be Geographic WGS84, using the EPSG code in this case. #What class is it? class(pc) ## [1] &quot;sf&quot; &quot;data.frame&quot; #Note the new &quot;geometry&quot; column names(pc) ## [1] &quot;scientific_name&quot; ## [2] &quot;datetime&quot; ## [3] &quot;description&quot; ## [4] &quot;place_guess&quot; ## [5] &quot;tag_list&quot; ## [6] &quot;common_name&quot; ## [7] &quot;url&quot; ## [8] &quot;image_url&quot; ## [9] &quot;user_login&quot; ## [10] &quot;id&quot; ## [11] &quot;species_guess&quot; ## [12] &quot;iconic_taxon_name&quot; ## [13] &quot;taxon_id&quot; ## [14] &quot;num_identification_agreements&quot; ## [15] &quot;num_identification_disagreements&quot; ## [16] &quot;observed_on_string&quot; ## [17] &quot;observed_on&quot; ## [18] &quot;time_observed_at&quot; ## [19] &quot;time_zone&quot; ## [20] &quot;positional_accuracy&quot; ## [21] &quot;public_positional_accuracy&quot; ## [22] &quot;geoprivacy&quot; ## [23] &quot;taxon_geoprivacy&quot; ## [24] &quot;coordinates_obscured&quot; ## [25] &quot;positioning_method&quot; ## [26] &quot;positioning_device&quot; ## [27] &quot;user_id&quot; ## [28] &quot;created_at&quot; ## [29] &quot;updated_at&quot; ## [30] &quot;quality_grade&quot; ## [31] &quot;license&quot; ## [32] &quot;sound_url&quot; ## [33] &quot;oauth_application_id&quot; ## [34] &quot;captive_cultivated&quot; ## [35] &quot;geometry&quot; #Plot ggplot() + geom_sf(data=pc) Great! We got lots of points, but without a base layer its very difficult to tell where exactly these are? 7.9 Adding basemaps to plots There are lots of ways to make the basemap from data objects etc that we can plot our points over, but an easy way is to pull in tiles from Open Street Maps and plot our points on those. library(rosm) library(ggspatial) ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc) Note that there are quite a few base layer/tile options that can be set with type = \"\". Try rosm::osm.types() to see them all. This is better than nothing, but the scale of the map is too small to really see where the plants actually are. It would be much easier if we could look at the data interactively? 7.10 Interactive maps with leaflet We can generate interactive maps by calling the leaflet mapserver using wrapper functions in the leaflet R package written for this purpose. NOTE: If you can’t get leaflet to work it is probably a CRS problem. Your data need to be in Geographic or Web Mercator library(leaflet) library(htmltools) leaflet() %&gt;% # Add default OpenStreetMap map tiles addTiles(group = &quot;Default&quot;) %&gt;% # Add our points addCircleMarkers(data = pc, group = &quot;Protea cynaroides&quot;, radius = 3, color = &quot;green&quot;) Much better! Strange, but even though we filtered our iNaturalist records for captive_cultivated == \"false\" we still have a number of observations that appear to be in people’s gardens. Let this serve as a warning to be wary of all data! Always do “common-sense-checks” on your data and the outputs of your analyses!!! 7.11 Reprojecting One way to drastically reduce the number of cultivated records is to overlay the localities (points) with the remaining extent of the vegetation types (i.e. anything that is not in natural vegtation is likely to be cultivated). Let’s try that… #Get the remnants layer vegr &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous_Remnants.shp&quot;) ## Reading layer `Vegetation_Indigenous_Remnants&#39; from data source `/home/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous_Remnants.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 3428 features and 7 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63951.23 ymin: -3803532 xmax: 420.7595 ymax: -3705506 ## Projected CRS: WGS_1984_Transverse_Mercator hmm &lt;- st_intersection(pc, vegr) ## Error in geos_op2_geom(&quot;intersection&quot;, x, y, ...): st_crs(x) == st_crs(y) is not TRUE Oops! The Coordinate Reference Systems are different! We will need to reproject one of the two datasets… Let’s see what CRS are currently set: st_crs(pc) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] So the points are Geographic, with no projected CRS CONVERSION. st_crs(vegr) ## Coordinate Reference System: ## User input: WGS_1984_Transverse_Mercator ## wkt: ## PROJCRS[&quot;WGS_1984_Transverse_Mercator&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;Hartebeesthoek94&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6148]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;unnamed&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,19, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,1, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]] The remnants of vegetation types are in Transverse Mercator Lo19, just like the dataset of the historical extent of the veg types we were working with earlier. In this case, either CRS is fine for our purposes, but let’s stick with Transverse Mercator Lo19, because it’ll be useful later. For this we need to reproject the veg layer like so: pc &lt;- st_transform(pc, st_crs(vegr)) Note that I fed it the CRS from vegr. This guarantees that they’ll be the same, even if we misidentified what the actual CRS is… 7.12 Intersecting points and polygons …and now we can try to intersect the points and polygons again… First lets see how many rows and columns the point data before the intersection: #call the dimensions of pc dim(pc) ## [1] 639 35 And after the intersection? pc &lt;- st_intersection(pc, vegr) ## Warning: attribute variables are assumed to be spatially ## constant throughout all geometries dim(pc) ## [1] 611 42 Less rows, but more columns! Two things have happened: The attribute data from the polygons in vegr intersected by the points in pc have been added to the attribute table in pc! All points that do not intersect the polygons in vegr were dropped (i.e. those that were recorded outside the remaining extent of natural vegetation). Let’s have a look ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc) Yup, the localities in suburbia are gone… The map is a bit bland though. How about we use our “new information” about which vegetation types the observations occur in to colour or label the points on the map? 7.13 Colour or label points First, let’s add colour: library(wesanderson) pal &lt;- wes_palette(&quot;Darjeeling1&quot;, 7, type = &quot;continuous&quot;) ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc, aes(col = National_)) + scale_colour_manual(values = pal) Looks like almost all of them are in Peninsula Sandstone Fynbos… pc %&gt;% group_by(National_) %&gt;% summarise(n()) ## Simple feature collection with 5 features and 2 fields ## Geometry type: GEOMETRY ## Dimension: XY ## Bounding box: xmin: -61567.21 ymin: -3797115 xmax: -48812.79 ymax: -3755803 ## Projected CRS: WGS_1984_Transverse_Mercator ## # A tibble: 5 × 3 ## National_ `n()` geometry ## &lt;chr&gt; &lt;int&gt; &lt;GEOMETRY [m]&gt; ## 1 Hangklip Sand Fynbos 2 MULTIPOINT ((-52770.86 -… ## 2 Peninsula Granite Fynbos … 6 MULTIPOINT ((-56255.7 -3… ## 3 Peninsula Sandstone Fynbos 600 MULTIPOINT ((-61567.21 -… ## 4 Peninsula Shale Renosterv… 1 POINT (-55866.09 -375580… ## 5 Southern Afrotemperate Fo… 2 MULTIPOINT ((-53662.7 -3… Yup! Note the numbers in column n(). But I can’t see where the Hangklip Sand Fynbos record is, so let’s label that one with text using geom_sf_label(). hsf &lt;- pc %&gt;% filter(National_ == &quot;Hangklip Sand Fynbos&quot;) #find the locality ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data=pc, aes(col = National_)) + scale_colour_manual(values = pal) + geom_sf_label(data=hsf, aes(label = &quot;Here&quot;)) Aha! Note that you can specify that the label = setting points to a column in your dataset with names if you have lots of labels to add. 7.14 Buffering One issue here may be that all localities should be in Peninsula Sandstone Fynbos, but the vegetation type boundaries are wrong. After all, the transition or ecotone between two vegetation types is usually diffuse rather than a clear boundary, not to mention that the data may have been digitized at a very small scale, compromizing precision and accuracy. One way to check this is to buffer the points using st_buffer to see if they are within some distance (say 250m) of the boundary with Peninsula Sandstone Fynbos. #Find the localities that are not in Peninsula Sandstone Fynbos and add a 250m buffer npsf &lt;- pc %&gt;% filter(National_ != &quot;Peninsula Sandstone Fynbos&quot;) %&gt;% st_buffer(dist = 250) #NOTE that st_buffer() makes them polygons, because they now have area! npsf$geometry[1] #The first geometry in npsf ## Geometry set for 1 feature ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -53912.7 ymin: -3762372 xmax: -53412.7 ymax: -3761872 ## Projected CRS: WGS_1984_Transverse_Mercator ## POLYGON ((-53412.7 -3762122, -53413.05 -3762136... #Get the number of unique iNaturalist record numbers length(unique(npsf$id)) ## [1] 11 #Intersect new polygons with veg remnants and filter for those that overlap Peninsula Sandstone Fynbos only npsf &lt;- st_intersection(npsf, vegr) %&gt;% filter(National_.1 == &quot;Peninsula Sandstone Fynbos&quot;) ## Warning: attribute variables are assumed to be spatially ## constant throughout all geometries #Get the number of unique iNaturalist record numbers that overlap PSF length(unique(npsf$id)) ## [1] 6 So 6 of 11 records are suspiciously close to Peninsula Sandstone Fynbos… 7.15 Within distance and intersect Perhaps a more interesting use of buffering is to see if a species’ locality is within a certain distance of a particular habitat etc. For example, we could ask if a species is associated with riparian zones by buffering either the localities (points) or rivers (lines) and then doing an intersection. But of course there are many ways to skin a cat, and it turns out buffering and intersecting may not be the most efficient here. If we don’t want to pull the attribute data from one dataset to the other we can just use st_intersects() to see if they overlap at all. We can even take it one step further, because sf has the function st_is_within_distance(), which is similar to applying st_buffer() and st_intersects() in one go. Here we’ll use Brabejum stellatifolium (a riparian tree in the Proteaceae) as our focal species and the watercourse layer from the City of Cape Town. #Get the watercourse data water &lt;- st_read(&quot;data/cape_peninsula/Open_Watercourses.geojson&quot;) ## Reading layer `Open_Watercourses&#39; from data source ## `/home/jasper/GIT/spatial-r/data/cape_peninsula/Open_Watercourses.geojson&#39; ## using driver `GeoJSON&#39; ## Simple feature collection with 10848 features and 11 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: 18.31249 ymin: -34.28774 xmax: 18.99045 ymax: -33.47256 ## Geodetic CRS: WGS 84 #Check it&#39;s CRS st_crs(water) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] #Call the data directly from iNat bs &lt;- get_inat_obs(taxon_name = &quot;Brabejum stellatifolium&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #Filter returned observations by a range of attribute criteria bs &lt;- bs %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) #See how many records we got nrow(bs) ## [1] 173 #Make the dataframe a spatial object of class = &quot;sf&quot; bs &lt;- st_as_sf(bs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) #Note that I had to define the CRS (as Geographic WGS84)!!! Let’s see what we’ve got… #Crop the water courses to the extent of the locality data water &lt;- st_crop(water, bs) ## Warning: attribute variables are assumed to be spatially ## constant throughout all geometries #Plot ggplot() + annotation_map_tile(type = &quot;osm&quot;, progress = &quot;none&quot;) + geom_sf(data = water, colour = &quot;blue&quot;) + geom_sf(data=bs) Hard to tell, but they could all be on rivers? Let’s try st_intersects() without any buffering first to see if they overlap at all. st_intersects(bs, water) %&gt;% unlist() ## integer(0) Oops! We forgot to project our data! bs &lt;- st_transform(bs, st_crs(vegr)) water &lt;- st_transform(water, st_crs(vegr)) st_intersects(bs, water) %&gt;% unlist() ## integer(0) So none of them intersect, but this is not surprising, because lines and points in GIS do not have area, so they can’t really intersect unless you buffer them… Let’s try st_is_within_distance() and set it for 20 metres. Note that I add unlist() %&gt;% unique() so that it gives me a vector of the unique features (i.e. once each) that are within 20m, because the function returns a list and will return the same feature (line/river) multiple times - once for every point (tree) it is within 20m of. st_is_within_distance(bs, water, 20) %&gt;% unlist() %&gt;% unique() ## [1] 327 346 328 330 332 264 351 347 349 885 179 294 285 333 ## [15] 101 224 280 281 282 615 19 So it’s given us the list of lines (rivers) within 20m of our points, but that doesn’t tell us how many (or what proportion) of our points are within 20m of a river. Let’s apply the function again, swapping the layers around: st_is_within_distance(water, bs, 20) %&gt;% unlist() %&gt;% unique() ## [1] 160 145 161 171 84 104 113 18 121 98 99 1 14 38 ## [15] 69 70 138 139 24 34 37 54 88 90 153 17 137 123 ## [29] 10 11 12 25 32 64 85 86 114 33 87 19 82 81 ## [43] 83 94 149 So only ~45 of the trees are within 20m of the rivers. What about 50m? st_is_within_distance(water, bs, 100) %&gt;% unlist() %&gt;% unique() ## [1] 15 102 160 145 161 171 28 40 58 126 152 84 104 ## [14] 113 22 29 18 36 67 89 105 107 108 109 117 121 ## [27] 125 133 136 162 116 140 163 106 134 135 98 99 66 ## [40] 48 144 91 1 6 7 14 31 37 38 42 44 45 ## [53] 49 54 69 70 76 77 78 79 80 96 101 111 122 ## [66] 128 130 131 132 138 139 153 155 166 13 23 24 34 ## [79] 47 56 57 71 72 73 74 75 88 90 103 17 137 ## [92] 123 5 10 11 12 25 32 33 64 85 86 114 124 ## [105] 167 87 19 27 82 164 2 3 81 83 94 149 100 So about 120… It’s at this point that it’s worth thinking about the scale, precision and accuracy of both the species localities and the watercourse data before drawing any strong conclusions!!! "],["raster-gis-operations-in-r-with-terra.html", "8 Raster GIS operations in R with terra 8.1 Reading in data 8.2 Cropping 8.3 Aggregating / Resampling 8.4 Basic plotting 8.5 Disaggregating 8.6 Raster maths! 8.7 Focal and terrain calculations 8.8 Raster stacks 8.9 Extracting raster to vector 8.10 Rasterizing 8.11 Visualizing multiple datasets on one map 8.12 Cloud Optimized GeoTiffs (COGs)!!!", " 8 Raster GIS operations in R with terra 8.1 Reading in data Ok, now to look at handling rasters. As with sf, the terra package has one function -rast()- that can read in just about any raster file format, which it assigns it’s own class SpatRaster. Let’s get started and read in the digital elevation model (DEM) for the City of Cape Town. library(terra) ## terra 1.5.21 ## ## Attaching package: &#39;terra&#39; ## The following object is masked from &#39;package:rgdal&#39;: ## ## project ## The following object is masked from &#39;package:knitr&#39;: ## ## spin ## The following object is masked from &#39;package:dplyr&#39;: ## ## src ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## The following object is masked from &#39;package:ggplot2&#39;: ## ## arrow dem &lt;- rast(&quot;/home/jasper/Documents/Datasets/CoCT/10m_Grid_GeoTiff/10m_BA.tif&quot;) class(dem) ## [1] &quot;SpatRaster&quot; ## attr(,&quot;package&quot;) ## [1] &quot;terra&quot; dem #Typing the name of a &quot;SpatRaster&quot; class data object gives you the details ## class : SpatRaster ## dimensions : 9902, 6518, 1 (nrow, ncol, nlyr) ## resolution : 10, 10 (x, y) ## extent : -64180, 1000, -3804020, -3705000 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## source : 10m_BA.tif ## name : 10m_BA The coord. ref. field shows GCS_WGS_1984, which is Geographic Coordinates, but perhaps there is a projected CRS too? from which we can deduce that the coordinate reference system is Transverse Mercator Lo19. If you just want to know the CRS from a SpatRaster, you just call crs() like so: crs(dem) ## [1] &quot;PROJCRS[\\&quot;GCS_WGS_1984\\&quot;,\\n BASEGEOGCRS[\\&quot;WGS 84\\&quot;,\\n DATUM[\\&quot;World Geodetic System 1984\\&quot;,\\n ELLIPSOID[\\&quot;WGS 84\\&quot;,6378137,298.25722356049,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1]]],\\n PRIMEM[\\&quot;Greenwich\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433]],\\n ID[\\&quot;EPSG\\&quot;,4326]],\\n CONVERSION[\\&quot;unnamed\\&quot;,\\n METHOD[\\&quot;Transverse Mercator\\&quot;,\\n ID[\\&quot;EPSG\\&quot;,9807]],\\n PARAMETER[\\&quot;Latitude of natural origin\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8801]],\\n PARAMETER[\\&quot;Longitude of natural origin\\&quot;,19,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8802]],\\n PARAMETER[\\&quot;Scale factor at natural origin\\&quot;,1,\\n SCALEUNIT[\\&quot;unity\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8805]],\\n PARAMETER[\\&quot;False easting\\&quot;,0,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8806]],\\n PARAMETER[\\&quot;False northing\\&quot;,0,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8807]]],\\n CS[Cartesian,2],\\n AXIS[\\&quot;easting\\&quot;,east,\\n ORDER[1],\\n LENGTHUNIT[\\&quot;metre\\&quot;,1,\\n ID[\\&quot;EPSG\\&quot;,9001]]],\\n AXIS[\\&quot;northing\\&quot;,north,\\n ORDER[2],\\n LENGTHUNIT[\\&quot;metre\\&quot;,1,\\n ID[\\&quot;EPSG\\&quot;,9001]]]]&quot; Similar to st_crs(), you can define a projection using the syntax: crs(your_raster) &lt;- \"your_crs\", where the new CRS can be in WKT, and EPSG code, or a PROJ string. For reprojecting, you use the function project(). We’ll look at it later. 8.2 Cropping Ok, before we try to anything with this dataset, let’s think about how big it is… One of the outputs of calling dem was the row reading dimensions : 9902, 6518, 1 (nrow, ncol, nlyr). Given that we are talking about 10m pixels, this information tells us that the extent of the region is roughly 100km by 65km and that there are ~65 million pixels! No wonder the file is ~130MB. While R can handle this, it does become slow when dealing with very large files. There are many ways to improve the efficiency of handling big rasters in R (see this slightly dated post for details if you’re interested), but for the purposes of this tutorial we’re going to take the easy option and just crop it to a smaller extent, like so: dem &lt;- crop(dem, ext(c(-66642.18, -44412.18, -3809853.29, -3750723.29))) Note that the crop() function requires us to pass it an object of class SpatExtent. Just like st_crop() from sf, crop() can derive the extent from another data object. One silly difference, is that if you pass it the coordinates of the extent manually (as above), you first need to pass it to the ext() function, and they need to follow the order xmin, xmax, ymin, ymax (as opposed to xmin, ymin, xmax, ymax as you do for st_crop()). Keep your eye out for these little differences, because they will trip you up… Ok, so how big is our dataset now? dem ## class : SpatRaster ## dimensions : 5330, 1977, 1 (nrow, ncol, nlyr) ## resolution : 10, 10 (x, y) ## extent : -64180, -44410, -3804020, -3750720 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## source : memory ## name : 10m_BA ## min value : -15 ## max value : 1084 …still &gt;10 million pixels… 8.3 Aggregating / Resampling Do we need 10m data? If your analysis doesn’t need such fine resolution data, you can resample the raster to a larger pixel size, like 30m. The aggregate() function in the raster package does this very efficiently, like so: dem30 &lt;- aggregate(dem, fact = 3, fun = mean) Here I’ve told it to aggregate by a factor of 3 (i.e. bin 9 neighbouring pixels (3x3) into one) and to assign the bigger pixel the mean of the 9 original pixels. This obviously results in some data loss, but that can be acceptable, depending on the purpose of your analysis. Note that you can pass just about any function to fun =, like min(), max() or even your own function. dem30 ## class : SpatRaster ## dimensions : 1777, 659, 1 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## source : memory ## name : 10m_BA ## min value : -15 ## max value : 1083.556 Ok, so we’ve reduced the size of the raster by a factor of 9 and only have a little over 1 million pixels to deal with. Much more reasonable! Now let’s have a look at what we’re dealing with. 8.4 Basic plotting Now that we’ve reduced the size of the dataset, we can try the base plotting function: plot(dem30) Or with the Tidyverse… Note that ggplot() doesn’t accept rasters, so we need to give it a dataframe with x and y columns for the coordinates, and a column containing the values to plot. This is easily done by coercing the raster into a dataframe, like so: #call tidyverse libraries and plot library(tidyverse) dem30 %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = `10m_BA`)) # Note that I had to know that the column name for the elevation data is &quot;10m_BA&quot;... and that you need to use ` ` around a variable name when feeding it to a function if it starts with a digit. Ok, how different does our 30m raster look to the 10m version? dem %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = `10m_BA`)) Not noticeably different at this scale! 8.5 Disaggregating One way to explore the degree of data loss is to disagg() our 30m DEM back to 10m and then compare it to the original. dem10 &lt;- disagg(dem30, fact = 3, method = &quot;bilinear&quot;) Note that I’ve tried to use bilinear interpolation to give it a fair chance of getting nearer the original values. You can google this on your own, but it essentially smooths the data by averaging across neighbouring pixels. Now, how can I compare my two 10m rasters? 8.6 Raster maths! The raster and terra packages make this easy, because you can do maths with rasters, treating them as variables in an equation. This means we can explore the data loss by calculating the difference between the original and disaggregated DEMS. Note that when aggregating you often lose some of the cells along the edges, and that you can’t do raster maths on rasters with different extents… We can fix this by cropping the larger raster with the smaller first. dem10 &lt;- crop(dem10, dem) diff &lt;- dem - dem10 #maths with rasters! And plot the result! diff %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = `10m_BA`)) If you look really closely, you’ll see the outline of the cliffs of Table Mountain, where you’d expect the data loss to be worst. The colour ramp tells us that the worst distortion was up to 100m, or about 10% of the elevation range in this dataset, but don’t be fooled by the extremes! Let’s have a look at all the values as a histogram. diff %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_histogram(aes(`10m_BA`)) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. Looks like most values are within 10 or so metres of their original values, so the data loss really wasn’t that bad! 8.7 Focal and terrain calculations In addition to maths with multiple rasters, you can do all kinds of calculations within a raster using focal(). This essentially applies a moving window, calculating values for a neighbourhood of cells as it goes, using whatever function you supply (mean, max, your own, etc). The function terrain() is a special case of focal(), optimized for calculating slope, aspect, topographic position index (TPI), topographic roughness index (TRI), roughness, or flow direction. Here I’ll calculate the slope and aspect so that we can pass them to the function shade() to make a pretty hillshade layer. aspect &lt;- terrain(dem30, &quot;aspect&quot;, unit = &quot;radians&quot;) slope &lt;- terrain(dem30, &quot;slope&quot;, unit = &quot;radians&quot;) hillshade &lt;- shade(slope, aspect) plot(hillshade) Probably prettier with Tidyverse: hillshade %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = lyr1)) + #note that the hillshade column name in this case is &quot;lyr1&quot; scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) Nice ne? 8.8 Raster stacks Another nice thing about rasters is that if you have multiple rasters “on the same grid” (i.e. with the same pixel size, extent and CRS) then you can stack them and work with them as a single object. library(raster) users will be familiar with stack(), but in terra you just use the base function c(), like so: dstack &lt;- c(dem30, slope, aspect, hillshade) dstack ## class : SpatRaster ## dimensions : 1777, 659, 4 (nrow, ncol, nlyr) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## coord. ref. : GCS_WGS_1984 ## sources : memory ## memory ## memory ## ... and 1 more source(s) ## names : 10m_BA, slope, aspect, lyr1 ## min values : -15.0000000, 0.0000000, 0.0000000, -0.4906481 ## max values : 1083.5555556, 1.3708258, 6.2831853, 0.9999974 As you can see the “dimensions” now report 4 layers, and there are 4 names. Some of the names don’t look all that informative though, so let’s rename them. names(dstack) &lt;- c(&quot;elevation&quot;, &quot;slope&quot;, &quot;aspect&quot;, &quot;shade&quot;) 8.9 Extracting raster to vector Ok, enough fooling around. More often than not, we just want to extract data from rasters for further analyses (e.g. climate layers, etc), so let’s cover that base here. Extract to points First, let’s get some points for two species in the Proteaceae, Protea cynaroides and Leucadendron laureolum… library(rinat) library(sf) #Call data for two species directly from iNat pc &lt;- get_inat_obs(taxon_name = &quot;Protea cynaroides&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) ll &lt;- get_inat_obs(taxon_name = &quot;Leucadendron laureolum&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #Combine the records into one dataframe pc &lt;- rbind(pc,ll) #Filter returned observations by a range of attribute criteria pc &lt;- pc %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) #Make the dataframe a spatial object of class = &quot;sf&quot; pc &lt;- st_as_sf(pc, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) #Set to the same projection as the elevation data pc &lt;- st_transform(pc, crs(dem30)) Now let’s extract the data to the points. NOTE!!! terra doesn’t play nicely with sf objects at this stage, so you need to coerce them into terra’s own vector format using vect(). dat &lt;- extract(dem30, vect(pc)) # note vect() head(dat) ## ID 10m_BA ## 1 1 453.2222 ## 2 2 463.3333 ## 3 3 358.5556 ## 4 4 341.2222 ## 5 5 314.6667 ## 6 6 444.8889 Nice, but not all that handy on it’s own. Let’s add the elevation column to our points layer, so we can match it with the species names and plot. pc$dem &lt;- dat$`10m_BA` pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem)) (Hmm… do you think those Leucadendron laureolum outliers that must be near Maclear’s Beacon could actually be Leucadendron strobilinum?) Ok, that’s handy, but what if we have data lots of rasters? We don’t want to have to do that for every raster! This is where raster stacks come into their own! #extract from stack dat &lt;- extract(dstack, vect(pc)) #bind columns to points to match the names edat &lt;- cbind(as.data.frame(pc), dat) #select columns we want and tidy data into long format edat &lt;- edat %&gt;% dplyr::select(scientific_name, elevation, slope, aspect, shade) %&gt;% pivot_longer(c(elevation, slope, aspect, shade)) #panel boxplot of the variables extracted edat %&gt;% ggplot() + geom_boxplot(aes(scientific_name, value)) + facet_wrap(~name, scales = &quot;free&quot;) Something I should have mentioned is that if you would like each point to sample a larger region you can add a buffer = argument to the extract() function, and a function (fun =) to summarize the neighbourhood of pixels sampled, like so: pc$dem30 &lt;- extract(dem30, vect(pc), buffer = 200, fun = mean)$X10m_BA #Note the sneaky use of $ to access the column I want pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem30)) Extract to polygons Now let’s try that with our vegetation polygons. #Get historical vegetation layer veg &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous.shp&quot;) ## Reading layer `Vegetation_Indigenous&#39; from data source ## `/home/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1325 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63972.95 ymin: -3803535 xmax: 430.8125 ymax: -3705149 ## Projected CRS: WGS_1984_Transverse_Mercator #Crop to same extent as DEM veg &lt;- st_crop(veg, ext(dem30)) #Note that I just fed it the extent of the DEM ## Warning: attribute variables are assumed to be spatially ## constant throughout all geometries #Best to dissolve polygons first - otherwise you get repeat outputs for each polygon within each veg type vegsum &lt;- veg %&gt;% group_by(National_) %&gt;% summarize() #Do extraction - note the summary function vegdem &lt;- extract(dem30, vect(vegsum), fun = mean, na.rm = T) #Combine the names and vector extracted means into a dataframe vegdem &lt;- cbind(vegdem, vegsum$National_) #Rename the columns to something meaningful names(vegdem) &lt;- c(&quot;ID&quot;, &quot;Mean elevation (m)&quot;, &quot;Vegetation type&quot;) #Plot vegdem %&gt;% ggplot() + geom_col(aes(y = `Mean elevation (m)`, x = `Vegetation type`)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) Ok, I did a lot of things there…, but you get it right? Note that I applied a function to the extract() to summarize the output, because each polygon usually returns multiple raster cell values. You can choose (or code up) your own function. Here’s a different approach… 8.10 Rasterizing Rasterizing essentially means turning a vector layer into a raster. To rasterize, you need an existing raster grid to rasterize to, like dem30 in this case. #Make the vegetation type a factor vegsum$National_ &lt;- as.factor(vegsum$National_) #Rasterize vegras &lt;- rasterize(vect(vegsum), dem30, field = &quot;National_&quot;) #Plot vegras %&gt;% as.data.frame(xy = TRUE) %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = National_)) I’m sure this plot is a surprise to those who worked with raster. Usually rasters want to work with numbers. terra can work with (and rasterize) data of class “factor,” opening up all kinds of opportunities. 8.11 Visualizing multiple datasets on one map What about if we want to plot multiple datasets on one map? This is easy, if you can feed each dataset into a separate ggplot function. Here’s the veg types with contours and the iNaturalist records we retrieved earlier. ggplot() + geom_raster(data = as.data.frame(vegras, xy = TRUE), aes(x = x, y = y, fill = National_)) + geom_contour(data = as.data.frame(dem30, xy = TRUE), aes(x = x, y = y, z = `10m_BA`), breaks = seq(0, 1100, 100), colour = &quot;black&quot;) + geom_sf(data=pc, colour = &quot;white&quot;, size = 0.5) For more inspiration on mapping with R, check out https://slingsby-maps.myshopify.com/. I’ve been generating the majority of the basemap (terrain colour, hillshade, contours, streams, etc) for these in R for the past few years. 8.12 Cloud Optimized GeoTiffs (COGs)!!! I thought I’d add this as a bonus section, reinforcing the value of standardized open metadata and file formats from the Data Management module. First, let’s open a connection to our COG, which is stored in the cloud. To do this, we need to pass a URL to the file’s online location to terra. cog.url &lt;- &quot;/vsicurl/https://storage.googleapis.com/grootbos-tiff/grootbos_cog.tif&quot; grootbos &lt;- rast(cog.url) grootbos ## class : SpatRaster ## dimensions : 100024, 121627, 3 (nrow, ncol, nlyr) ## resolution : 0.08, 0.08 (x, y) ## extent : 35640.41, 45370.57, -3828176, -3820175 (xmin, xmax, ymin, ymax) ## coord. ref. : LO19 ## source : grootbos_cog.tif ## colors RGB : 1, 2, 3 ## names : grootbos_cog_1, grootbos_cog_2, grootbos_cog_3 This has given us the metadata about the file, but has not read it into R’s memory. The file is ~1.8GB so it would do bad things if we tried to read the whole thing in… Now let’s retrieve a subset of the file. To do this we need to make a vector polygon for our region of interest, like so: roi &lt;- vect(data.frame(lon = c(19.433975, 19.436451), lat = c(-34.522733, -34.520735)), crs = &quot;epsg:4326&quot;) And transform it to the same projection as the COG: roi &lt;- project(roi, crs(grootbos)) And then extract our ROI roi_ras &lt;- crop(grootbos, roi) roi_ras ## class : SpatRaster ## dimensions : 2758, 2853, 3 (nrow, ncol, nlyr) ## resolution : 0.08, 0.08 (x, y) ## extent : 39845.61, 40073.85, -3821732, -3821512 (xmin, xmax, ymin, ymax) ## coord. ref. : LO19 ## source : spat_4sNq2j4MElbiml2_18837.tif ## colors RGB : 1, 2, 3 ## names : grootbos_cog_1, grootbos_cog_2, grootbos_cog_3 ## min values : 26, 48, 56 ## max values : 255, 255, 255 Now we have a raster with 3 layers in memory. There are Red Green and Blue, so we should be able to plot them, like so: plotRGB(roi_ras) Familiar? "],["raster-gis-operations-in-r.html", "9 Raster GIS operations in R 9.1 Reading in data 9.2 Cropping 9.3 Aggregating / Resampling 9.4 Basic plotting 9.5 Disaggregating 9.6 Raster maths! 9.7 Focal and terrain calculations 9.8 Raster stacks 9.9 Extracting raster to vector 9.10 Rasterizing", " 9 Raster GIS operations in R WARNING!!! READ THIS!!! This section focuses on library(raster) which is being phased out and will not be maintained in the long term. This is because it relies on other packages that are no longer maintained, because their creators have retired. I used raster in the first iteration of these notes so I’ve kept this section for posterity and for those who rely on it, but will remove it in the next year or so. I strongly recommend you ignore this section and move on to the next, which focuses on the new replacement library(terra)! I have kept to the same set of examples in that section, and many of the function names are the same or similar, so shifting to terra should be easy. 9.1 Reading in data Ok, now to look at handling rasters. As with sf, the raster package has one function -raster()- that can read in just about any raster file format. Let’s get started and read in the digital elevation model (DEM) for the City of Cape Town. library(raster) dem &lt;- raster(&quot;/home/jasper/Documents/Datasets/CoCT/10m_Grid_GeoTiff/10m_BA.tif&quot;) class(dem) ## [1] &quot;RasterLayer&quot; ## attr(,&quot;package&quot;) ## [1] &quot;raster&quot; dem #Typing the name of a &quot;raster&quot; class data object gives you the details ## class : RasterLayer ## dimensions : 9902, 6518, 64541236 (nrow, ncol, ncell) ## resolution : 10, 10 (x, y) ## extent : -64180, 1000, -3804020, -3705000 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## source : 10m_BA.tif ## names : X10m_BA ## values : -32768, 32767 (min, max) The crs field shows a proj4string, from which we can deduce that the coordinate reference system is Transverse Mercator Lo19. If you just want to know the CRS from a raster, you just call the proj4string like so: proj4string(dem) ## [1] &quot;+proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs&quot; Similar to st_crs(), you can define a projection using the syntax: proj4string(your_raster) &lt;- \"your_proj4string\". For reprojecting, you use the function projectRaster(). We’ll look at it later. 9.2 Cropping Ok, before we try to anything with this dataset, let’s think about how big it is… One of the outputs of calling dem was the row reading dimensions : 9902, 6518, 64541236 (nrow, ncol, ncell). Given that we are talking about 10m pixels, this information tells us that the extent of the region is roughly 100km by 65km and that there are ~65 million pixels! No wonder the file is ~130MB. While R can handle this, it will be slow! There are many ways to improve the efficiency of handling big rasters in R (see this post for details if you’re interested), but for the purposes of this tutorial we’re going to take the easy option and just crop it to a smaller extent, like so: dem &lt;- crop(dem, extent(c(-66642.18, -44412.18, -3809853.29, -3750723.29))) Note that the crop() function requires us to pass it an object of class extent. Just like st_crop() from sf, crop() can derive the extent from another data object. One silly difference, is that if you pass it the coordinates of the extent manually (as above), you first need to pass it to the extent() function, and they need to follow the order xmin, xmax, ymin, ymax (as opposed to xmin, ymin, xmax, ymax as you do for st_crop()). Keep your eye out for these little differences, because they will trip you up… Ok, so how big is our dataset now? dem ## class : RasterLayer ## dimensions : 5330, 1977, 10537410 (nrow, ncol, ncell) ## resolution : 10, 10 (x, y) ## extent : -64180, -44410, -3804020, -3750720 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## source : memory ## names : X10m_BA ## values : -15, 1084 (min, max) …still &gt;10 million pixels… 9.3 Aggregating / Resampling Do we need 10m data? If your analysis doesn’t need such fine resolution data, you can resample the raster to a larger pixel size, like 30m. The aggregate() function in the raster package does this very efficiently, like so: dem30 &lt;- aggregate(dem, fact = 3, fun = mean) Here I’ve told it to aggregate by a factor of 3 (i.e. bin 9 neighbouring pixels (3x3) into one) and to assign the bigger pixel the mean of the 9 original pixels. This obviously results in some data loss, but that can be acceptable, depending on the purpose of your analysis. Note that you can pass just about any function to fun =, like min(), max() or even your own function. dem30 ## class : RasterLayer ## dimensions : 1777, 659, 1171043 (nrow, ncol, ncell) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## source : memory ## names : X10m_BA ## values : -15, 1083.556 (min, max) Ok, so we’ve reduced the size of the raster by a factor of 9 and only have a little over 1 million pixels to deal with. Much more reasonable! Now let’s have a look at what we’re dealing with. 9.4 Basic plotting Now that we’ve reduced the size of the dataset, we can try the base plotting function: plot(dem30) Or with the Tidyverse… Note that ggplot() doesn’t accept rasters, so we need to give it a dataframe with x and y columns for the coordinates, and a column containing the values to plot. This is easily done, firstly by converting the raster to a vector layer of points, and then by coorcing that into a dataframe, like so: #convert to points dem30df &lt;- rasterToPoints(dem30, spatial = TRUE) class(dem30df) #note that this is a class from library(sp) ## [1] &quot;SpatialPointsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; #coerce to a dataframe dem30df &lt;- data.frame(dem30df) #have a look at the column names names(dem30df) # &quot;X10m_BA&quot; is the elevation data... ## [1] &quot;X10m_BA&quot; &quot;x&quot; &quot;y&quot; &quot;optional&quot; #call tidyverse libraries and plot library(tidyverse) dem30df %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = X10m_BA)) Ok, how different does our 30m raster look to the 10m version? demdf &lt;- data.frame(rasterToPoints(dem, spatial = TRUE)) demdf %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = X10m_BA)) Not noticeably different at this scale! 9.5 Disaggregating One way to explore the degree of data loss is to disaggregate() our 30m DEM back to 10m and then compare it to the original. dem10 &lt;- disaggregate(dem30, fact = 3, method = &quot;bilinear&quot;) Note that I’ve tried to use bilinear interpolation to give it a fair chance of getting nearer the original values. You can google this on your own, but it essentially smooths the data by averaging across neighbouring pixels. Now, how can I compare my two 10m rasters? 9.6 Raster maths! The raster package makes this easy, because you can do maths with rasters, treating them as variables in an equation. This means we can explore the data loss by calculating the difference between the original and disaggregated DEMS, like so. diff &lt;- dem - dem10 ## Warning in dem - dem10: Raster objects have different ## extents. Result for their intersection is returned Note that the error is because we lost some of the 10m cells along the edges when we aggregated… We can ignore this in this example. diffdf &lt;- data.frame(rasterToPoints(diff, spatial = TRUE)) names(diffdf) #note that the elevation column is now calles &quot;layer&quot;, which we need to feed to ggplot() ## [1] &quot;layer&quot; &quot;x&quot; &quot;y&quot; &quot;optional&quot; diffdf %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = layer)) If you look really closely, you’ll see the outline of the cliffs of Table Mountain, where you’d expect the data loss to be worst. The colour ramp tells us that the worst distortion was up to 100m, or about 10% of the elevation range in this dataset, but don’t be fooled by the extremes! Let’s have a look at all the values as a histogram. diffdf %&gt;% ggplot() + geom_histogram(aes(layer)) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. Looks like most values are within 10 or so metres of their original values, so the data loss really wasn’t that bad! 9.7 Focal and terrain calculations In addition to maths with multiple rasters, you can do all kinds of calculations within a raster using focal(). This essentially applies a moving window, calculating values for a neighbourhood of cells as it goes, using whatever function you supply (mean, max, your own, etc). The function terrain() is a special case of focal(), optimized for calculating slope, aspect, topographic position index (TPI), topographic roughness index (TRI), roughness, or flow direction. Here I’ll calculate the slope and aspect so that we can pass them to the function hillShade() to make a pretty hillshade layer. aspect &lt;- terrain(dem30, &quot;aspect&quot;) slope &lt;- terrain(dem30, &quot;slope&quot;) hillshade &lt;- hillShade(slope, aspect) plot(hillshade) Probably prettier with Tidyverse: hsdf &lt;- data.frame(rasterToPoints(hillshade, spatial = TRUE)) names(hsdf) ## [1] &quot;layer&quot; &quot;x&quot; &quot;y&quot; &quot;optional&quot; hsdf %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = layer)) + scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) Nice ne? 9.8 Raster stacks Another nice thing about rasters is that if you have multiple rasters “on the same grid” (i.e. with the same pixel size, extent and CRS) then you can stack them and work with them as a single rasterstack object. dstack &lt;- stack(dem30, slope, aspect, hillshade) dstack ## class : RasterStack ## dimensions : 1777, 659, 1171043, 4 (nrow, ncol, ncell, nlayers) ## resolution : 30, 30 (x, y) ## extent : -64180, -44410, -3804030, -3750720 (xmin, xmax, ymin, ymax) ## crs : +proj=tmerc +lat_0=0 +lon_0=19 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs ## names : X10m_BA, slope, aspect, layer ## min values : -15.0000000, 0.0000000, 0.0000000, -0.4906481 ## max values : 1083.5555556, 1.3708258, 6.2831853, 0.9999974 As you can see the “dimensions” now report 4 layers, and there are 4 names. Some of the names don’t look all that informative though, so let’s rename them. names(dstack) &lt;- c(&quot;elevation&quot;, &quot;slope&quot;, &quot;aspect&quot;, &quot;shade&quot;) 9.9 Extracting raster to vector Ok, enough fooling around. More often than not, we just want to extract data from rasters for further analyses (e.g. climate layers, etc), so let’s cover that base here. Extract to points First, let’s get some points for two species in the Proteaceae, Protea cynaroides and Leucadendron laureolum… library(rinat) library(sf) #Call data for two species directly from iNat pc &lt;- get_inat_obs(taxon_name = &quot;Protea cynaroides&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) ll &lt;- get_inat_obs(taxon_name = &quot;Leucadendron laureolum&quot;, bounds = c(-35, 18, -33.5, 18.5), maxresults = 1000) #Combine the records into one dataframe pc &lt;- rbind(pc,ll) #Filter returned observations by a range of attribute criteria pc &lt;- pc %&gt;% filter(positional_accuracy&lt;46 &amp; latitude&lt;0 &amp; !is.na(latitude) &amp; captive_cultivated == &quot;false&quot; &amp; quality_grade == &quot;research&quot;) #Make the dataframe a spatial object of class = &quot;sf&quot; pc &lt;- st_as_sf(pc, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) Now let’s extract the data to the points. dat &lt;- extract(dem30, pc) ## Warning in .local(x, y, ...): Transforming SpatialPoints to ## the crs of the Raster Note how raster conveniently handled my error in not making sure the CRS are the same for both layers! In general, you should avoid relying on automated fixes like this… head(dat) ## [1] 453.2222 463.3333 358.5556 341.2222 314.6667 444.8889 Hmm… ok… It just returned the vector of numbers… Not all that handy on it’s own. Let’s add it to out points layer, so we can match it with the species names and plot. pc$dem30 &lt;- dat pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem30)) (Hmm… do you think those Leucadendron laureolum outliers that must be near Maclear’s Beacon could actually be Leucadendron strobilinum?) Ok, that’s handy, but what if we have data lots of rasters? We don’t want to have to do that for every raster! This is where rasterstacks come into their own! #extract from stack dat &lt;- extract(dstack, pc) ## Warning in .local(x, y, ...): Transforming SpatialPoints to ## the crs of the Raster #bind columns to points to match thenames edat &lt;- cbind(as.data.frame(pc), dat) #select columns we want and tidy data into long format edat &lt;- edat %&gt;% as_tibble() %&gt;% dplyr::select(scientific_name, elevation, slope, aspect, shade) %&gt;% pivot_longer(c(elevation, slope, aspect, shade)) #panel boxplot of the variables extracted edat %&gt;% ggplot() + geom_boxplot(aes(scientific_name, value)) + facet_wrap(~name, scales = &quot;free&quot;) Something I should have mentioned is that if you would like each point to sample a larger region you can add a buffer = argument to the extract() function, and a function (fun =) to summarize the neighbourhood of pixels sampled, like so: pc$dem30 &lt;- extract(dem30, pc, buffer = 200, fun = mean) ## Warning in .local(x, y, ...): Transforming SpatialPoints to ## the crs of the Raster pc %&gt;% ggplot() + geom_boxplot(aes(scientific_name, dem30)) Extract to polygons Now let’s try that with our vegetation polygons. #Get historical vegetation layer veg &lt;- st_read(&quot;data/cape_peninsula/veg/Vegetation_Indigenous.shp&quot;) ## Reading layer `Vegetation_Indigenous&#39; from data source ## `/home/jasper/GIT/spatial-r/data/cape_peninsula/veg/Vegetation_Indigenous.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1325 features and 5 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -63972.95 ymin: -3803535 xmax: 430.8125 ymax: -3705149 ## Projected CRS: WGS_1984_Transverse_Mercator #Crop to same extent as DEM veg &lt;- st_crop(veg, extent(dem30)) #Note that I just fed it the extent of the DEM ## Warning: attribute variables are assumed to be spatially ## constant throughout all geometries #Best to dissolve polygons first - otherwise you get repeat outputs for each polygone within each veg type vegsum &lt;- veg %&gt;% group_by(National_) %&gt;% summarize() #Do extraction - gives a vector - note the summary function vegdem &lt;- extract(dem30, vegsum, fun = mean, na.rm = T) #Combine the names and vector extracted means into a dataframe vegdem &lt;- data.frame(veg = vegsum$National_, elevation = vegdem) #Plot vegdem %&gt;% ggplot() + geom_col(aes(y = elevation, x = veg)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) ###Add example where all cells are extracted and summarized afterwards... #hmm &lt;- do.call(rbind,lapply(vegdem,data.frame))# bind_rows(vegdem) #hmm &lt;- data.frame(veg = rownames(hmm), elevation = hmm) Ok, I did a lot of things there…, but you get it right? Note that I applied a function to the extract() to summarize the output, because each polygon usualy returns multiple raster cell values. You can choose (or code up) your own function. Here’s a different approach… 9.10 Rasterizing Rasterizing essentially means turning a vector layer into a raster. To rasterize, you need an existing raster grid to rasterize to, like dem30 in this case. #Make the vegetation type name a factor* vegsum$National_ &lt;- as.factor(vegsum$National_) #Rasterize vegras &lt;- rasterize(vegsum, dem30, field = &quot;National_&quot;) #Plot vrs &lt;- data.frame(rasterToPoints(vegras, spatial = TRUE)) vrs %&gt;% ggplot() + geom_raster(aes(x = x, y = y, fill = layer)) + scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) Note that rasters want numbers. This is why I had to convert the “National_” column from class “character” to “factor.” You can think of a factor in R as essentially a vector of numbers with an associated translation table that links the numbers to the names. Here’s the names: levels(vegsum$National_) ## [1] &quot;Beach - FalseBay&quot; ## [2] &quot;Cape Estuarine Salt Marshes&quot; ## [3] &quot;Cape Flats Dune Strandveld - False Bay&quot; ## [4] &quot;Cape Flats Dune Strandveld - West Coast&quot; ## [5] &quot;Cape Flats Sand Fynbos&quot; ## [6] &quot;Cape Lowland Freshwater Wetlands&quot; ## [7] &quot;Hangklip Sand Fynbos&quot; ## [8] &quot;Peninsula Granite Fynbos - North&quot; ## [9] &quot;Peninsula Granite Fynbos - South&quot; ## [10] &quot;Peninsula Sandstone Fynbos&quot; ## [11] &quot;Peninsula Shale Fynbos&quot; ## [12] &quot;Peninsula Shale Renosterveld&quot; ## [13] &quot;RECLAIMED&quot; ## [14] &quot;Southern Afrotemperate Forest&quot; 9.10.0.0.0.0.0.0.0.0.0.0.1 We could even add contours… ggplot() + geom_raster(data = hsdf, aes(x = x, y = y, fill = layer, alpha = 0.5)) + scale_fill_gradient(low = &quot;grey10&quot;, high = &quot;grey90&quot;) + geom_contour(data = dem30df, aes(x = x, y = y, z = X10m_BA), breaks = seq(0, 1100, 100), colour = &quot;black&quot;) + theme(legend.position = &quot;none&quot;) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
